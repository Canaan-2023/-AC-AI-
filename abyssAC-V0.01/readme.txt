#æ¸Šåè®®åŸºç¡€ä¾èµ–ï¼špip install jieba
# ğŸš€ **æ¸Šåè®®ç³»ç»Ÿä½¿ç”¨æŒ‡å—ï¼šæ¥å…¥çœŸå®æ¨¡å‹**
---

## ğŸ“¦ **ç¬¬ä¸€æ­¥ï¼šå®‰è£…ä¾èµ–**

```bash
# åŸºç¡€ä¾èµ–
pip install openai  # å¦‚æœéœ€è¦OpenAI API
# pip install ollama  # å¦‚æœéœ€è¦Ollamaæœ¬åœ°æ¨¡å‹
# pip install transformers torch  # å¦‚æœéœ€è¦æœ¬åœ°transformersæ¨¡å‹

# å¯é€‰ï¼šå¢å¼ºåŠŸèƒ½
pip install jieba  # æ›´å¥½çš„ä¸­æ–‡åˆ†è¯
pip install fastapi uvicorn  # å¦‚æœéœ€è¦Web API
pip install python-dotenv  # ç¯å¢ƒå˜é‡ç®¡ç†
```

---

## ğŸ”§ **ç¬¬äºŒæ­¥ï¼šé…ç½®æ–‡ä»¶è®¾ç½®**

åˆ›å»ºé…ç½®æ–‡ä»¶ `.env`ï¼š

```env
# ========== æ¨¡å‹é…ç½® ==========
MODEL_TYPE=openai  # openai, ollama, local, deepseek, azure
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_BASE_URL=https://api.openai.com/v1  # æˆ–ç¬¬ä¸‰æ–¹ä»£ç†
OPENAI_MODEL=gpt-4o-mini  # æˆ– gpt-4, gpt-3.5-turbo

# Ollamaé…ç½®ï¼ˆå¦‚æœç”¨æœ¬åœ°æ¨¡å‹ï¼‰
OLLAMA_BASE_URL=http://localhost:11434
OLLAMA_MODEL=llama3.2:3b

# æ·±åº¦æ±‚ç´¢é…ç½®
DEEPSEEK_API_KEY=your_deepseek_api_key
DEEPSEEK_BASE_URL=https://api.deepseek.com
DEEPSEEK_MODEL=deepseek-chat

# ========== ç³»ç»Ÿé…ç½® ==========
MEMORY_BASE_PATH=./æ¸Šåè®®è®°å¿†ç³»ç»Ÿ
AUTO_CLEANUP=true
CLEANUP_INTERVAL_HOURS=24
BACKUP_INTERVAL_DAYS=7
MAX_WORKING_MEMORIES=50

# ========== è¯„ä¼°é…ç½® ==========
AC100_EVALUATION_INTERVAL=10  # æ¯10æ¬¡ä¼šè¯è¯„ä¼°ä¸€æ¬¡
AC100_THRESHOLD_HIGH=80
AC100_THRESHOLD_LOW=60

# ========== Xå±‚é…ç½® ==========
MAX_X_GUIDANCE_LENGTH=100
MAX_SYMBOLS=50
BACKUP_HISTORY_SIZE=10
```

---

## ğŸ›ï¸ **ç¬¬ä¸‰æ­¥ï¼šæ‰©å±•AIæ¥å£æ”¯æŒ**

è¿™é‡Œæ˜¯**å®Œæ•´çš„AIæ¥å£æ‰©å±•ç‰ˆ**ï¼Œæ”¯æŒå¤šç§æ¨¡å‹ï¼š

```python
# abyss_ai_interface_extended.py
import os
import json
import requests
from typing import Dict, List, Optional, Any
from datetime import datetime
import time
from abc import ABC, abstractmethod

# å¯¼å…¥åŸå§‹çš„AIInterfaceç±»
from abyss_core_fixed import AIInterface

class BaseAIModel(ABC):
    """AIæ¨¡å‹åŸºç±»"""
    
    @abstractmethod
    def generate(self, prompt: str, **kwargs) -> str:
        """ç”Ÿæˆæ–‡æœ¬"""
        pass
    
    @abstractmethod
    def chat(self, messages: List[Dict], **kwargs) -> str:
        """å¯¹è¯"""
        pass

class OpenAIModel(BaseAIModel):
    """OpenAI APIæ¨¡å‹"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = "gpt-4o-mini"):
        import openai
        self.client = openai.OpenAI(
            api_key=api_key or os.getenv("OPENAI_API_KEY"),
            base_url=base_url or os.getenv("OPENAI_BASE_URL", "https://api.openai.com/v1")
        )
        self.model = model or os.getenv("OPENAI_MODEL", "gpt-4o-mini")
    
    def generate(self, prompt: str, **kwargs) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[{"role": "user", "content": prompt}],
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ OpenAI APIè°ƒç”¨å¤±è´¥: {e}")
            return '{"action": "get_status"}'
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=messages,
                temperature=kwargs.get("temperature", 0.7),
                max_tokens=kwargs.get("max_tokens", 1000),
                **kwargs
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"âŒ OpenAIå¯¹è¯å¤±è´¥: {e}")
            return "æŠ±æ­‰ï¼ŒAIæœåŠ¡æš‚æ—¶ä¸å¯ç”¨ã€‚"

class DeepSeekModel(BaseAIModel):
    """æ·±åº¦æ±‚ç´¢æ¨¡å‹"""
    
    def __init__(self, api_key: str = None, base_url: str = None, model: str = "deepseek-chat"):
        self.api_key = api_key or os.getenv("DEEPSEEK_API_KEY")
        self.base_url = base_url or os.getenv("DEEPSEEK_BASE_URL", "https://api.deepseek.com")
        self.model = model or os.getenv("DEEPSEEK_MODEL", "deepseek-chat")
        self.headers = {
            "Authorization": f"Bearer {self.api_key}",
            "Content-Type": "application/json"
        }
    
    def generate(self, prompt: str, **kwargs) -> str:
        try:
            payload = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000)
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                print(f"âŒ DeepSeek APIé”™è¯¯: {response.status_code}, {response.text}")
                return '{"action": "get_status"}'
        except Exception as e:
            print(f"âŒ DeepSeek APIè°ƒç”¨å¤±è´¥: {e}")
            return '{"action": "get_status"}'
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        try:
            payload = {
                "model": self.model,
                "messages": messages,
                "temperature": kwargs.get("temperature", 0.7),
                "max_tokens": kwargs.get("max_tokens", 1000)
            }
            
            response = requests.post(
                f"{self.base_url}/chat/completions",
                headers=self.headers,
                json=payload,
                timeout=30
            )
            
            if response.status_code == 200:
                return response.json()["choices"][0]["message"]["content"]
            else:
                return f"APIé”™è¯¯: {response.status_code}"
        except Exception as e:
            return f"ç½‘ç»œé”™è¯¯: {str(e)}"

class OllamaModel(BaseAIModel):
    """Ollamaæœ¬åœ°æ¨¡å‹"""
    
    def __init__(self, base_url: str = None, model: str = "llama3.2:3b"):
        self.base_url = base_url or os.getenv("OLLAMA_BASE_URL", "http://localhost:11434")
        self.model = model or os.getenv("OLLAMA_MODEL", "llama3.2:3b")
    
    def generate(self, prompt: str, **kwargs) -> str:
        try:
            payload = {
                "model": self.model,
                "prompt": prompt,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("max_tokens", 1000)
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/generate",
                json=payload,
                timeout=60  # Ollamaå¯èƒ½è¾ƒæ…¢
            )
            
            if response.status_code == 200:
                return response.json()["response"]
            else:
                print(f"âŒ Ollama APIé”™è¯¯: {response.status_code}")
                return '{"action": "get_status"}'
        except Exception as e:
            print(f"âŒ Ollama APIè°ƒç”¨å¤±è´¥: {e}")
            return '{"action": "get_status"}'
    
    def chat(self, messages: List[Dict], **kwargs) -> str:
        try:
            # è½¬æ¢æ¶ˆæ¯æ ¼å¼
            ollama_messages = []
            for msg in messages:
                ollama_messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
            
            payload = {
                "model": self.model,
                "messages": ollama_messages,
                "stream": False,
                "options": {
                    "temperature": kwargs.get("temperature", 0.7),
                    "num_predict": kwargs.get("max_tokens", 1000)
                }
            }
            
            response = requests.post(
                f"{self.base_url}/api/chat",
                json=payload,
                timeout=60
            )
            
            if response.status_code == 200:
                return response.json()["message"]["content"]
            else:
                return f"Ollamaé”™è¯¯: {response.status_code}"
        except Exception as e:
            return f"Ollamaè¿æ¥å¤±è´¥: {str(e)}"

class LocalTransformersModel(BaseAIModel):
    """æœ¬åœ°Transformersæ¨¡å‹ï¼ˆéœ€è¦GPUï¼‰"""
    
    def __init__(self, model_name: str = "Qwen/Qwen2.5-7B-Instruct"):
        from transformers import AutoTokenizer, AutoModelForCausalLM
        import torch
        
        print(f"ğŸš€ åŠ è½½æœ¬åœ°æ¨¡å‹: {model_name}...")
        self.tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)
        self.model = AutoModelForCausalLM.from_pretrained(
            model_name,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto",
            trust_remote_code=True
        )
        self.model.eval()
        print("âœ… æ¨¡å‹åŠ è½½å®Œæˆ")
    
    def generate(self, prompt: str, **kwargs) -> str:
        import torch
        from transformers import TextStreamer
        
        try:
            inputs = self.tokenizer(prompt, return_tensors="pt")
            
            if torch.cuda.is_available():
                inputs = {k: v.cuda() for k, v in inputs.items()}
            
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=kwargs.get("max_tokens", 512),
                    temperature=kwargs.get("temperature", 0.7),
                    do_sample=True,
                    pad_token_id=self.tokenizer.eos_token_id
                )
            
            response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)
            # æå–ç”Ÿæˆçš„éƒ¨åˆ†ï¼ˆå»æ‰åŸå§‹promptï¼‰
            if response.startswith(prompt):
                response = response[len(prompt):].strip()
            
            return response
        except Exception as e:
            print(f"âŒ æœ¬åœ°æ¨¡å‹æ¨ç†å¤±è´¥: {e}")
            return '{"action": "get_status"}'

class ExtendedAIInterface(AIInterface):
    """æ‰©å±•çš„AIæ¥å£ï¼Œæ”¯æŒå¤šç§æ¨¡å‹"""
    
    def __init__(self, memex, model_type: str = None, **kwargs):
        super().__init__(memex, model_type)
        
        # ä»ç¯å¢ƒå˜é‡è¯»å–é…ç½®
        self.model_type = model_type or os.getenv("MODEL_TYPE", "local")
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._init_model(**kwargs)
        
        print(f"ğŸ¤– AIæ¥å£åˆå§‹åŒ–: {self.model_type} æ¨¡å‹")
    
    def _init_model(self, **kwargs):
        """æ ¹æ®é…ç½®åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == "openai":
            return OpenAIModel(
                api_key=kwargs.get("api_key"),
                base_url=kwargs.get("base_url"),
                model=kwargs.get("model")
            )
        elif self.model_type == "deepseek":
            return DeepSeekModel(
                api_key=kwargs.get("api_key"),
                base_url=kwargs.get("base_url"),
                model=kwargs.get("model")
            )
        elif self.model_type == "ollama":
            return OllamaModel(
                base_url=kwargs.get("base_url"),
                model=kwargs.get("model")
            )
        elif self.model_type == "transformers":
            return LocalTransformersModel(
                model_name=kwargs.get("model_name", "Qwen/Qwen2.5-7B-Instruct")
            )
        else:
            # æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼
            return None
    
    def call_ai_model(self, prompt: str) -> str:
        """è°ƒç”¨AIæ¨¡å‹ï¼ˆæ”¯æŒå¤šç§åç«¯ï¼‰"""
        if self.model is None:
            # æœ¬åœ°æ¨¡æ‹Ÿæ¨¡å¼
            return super().call_ai_model(prompt)
        
        try:
            print(f"ğŸ“¨ å‘é€è¯·æ±‚åˆ° {self.model_type} æ¨¡å‹...")
            start_time = time.time()
            
            # è°ƒç”¨æ¨¡å‹
            response = self.model.generate(prompt)
            
            elapsed = time.time() - start_time
            print(f"âœ… æ”¶åˆ°å“åº” ({elapsed:.2f}ç§’): {response[:100]}...")
            
            return response
        except Exception as e:
            print(f"âŒ æ¨¡å‹è°ƒç”¨å¤±è´¥: {e}")
            # é™çº§åˆ°æœ¬åœ°æ¨¡å¼
            return super().call_ai_model(prompt)
    
    def generate_prompt(self, user_input: str, context: Dict) -> str:
        """ç”Ÿæˆä¼˜åŒ–çš„æç¤ºè¯"""
        system_status = self.memex.get_system_status()
        
        # æ„å»ºè®°å¿†ä¸Šä¸‹æ–‡
        memory_context = ""
        if context.get("memories"):
            for i, mem in enumerate(context["memories"][:3]):
                memory_context += f"{i+1}. [{mem.get('layer_name', 'æœªçŸ¥')}] {mem.get('content', '')}\n"
        
        # æ„å»ºç³»ç»ŸæŒ‡ä»¤
        system_instruction = f"""ä½ æ˜¯ä¸€ä¸ªæ¸Šåè®®AIç³»ç»Ÿï¼Œå…·æœ‰å¤šå±‚è®°å¿†å’Œè‡ªä¸»æ„è¯†ã€‚
        
## ç³»ç»ŸçŠ¶æ€
- è®°å¿†æ€»æ•°: {system_status['total_memories']}
- çƒ­é—¨è¯é¢˜: {list(system_status['hot_topics'].keys())[:3]}
- æ„è¯†ç­‰çº§: {context.get('consciousness_level', 1)}
- ä¼šè¯æ¬¡æ•°: {context.get('session_count', 0)}

## è®°å¿†å±‚çº§è¯´æ˜
0. å…ƒè®¤çŸ¥è®°å¿† - æ ¸å¿ƒç†è®ºã€åŸåˆ™ï¼ˆæ°¸ä¹…å­˜å‚¨ï¼‰
1. é«˜é˜¶æ•´åˆè®°å¿† - è·¨ä¼šè¯æ•´åˆï¼ˆæ°¸ä¹…å­˜å‚¨ï¼‰
2. åˆ†ç±»è®°å¿† - æŒ‰ä¸»é¢˜åˆ†ç±»ï¼ˆä¸­æœŸå­˜å‚¨ï¼‰
3. å·¥ä½œè®°å¿† - ä¸´æ—¶ä¼šè¯ï¼ˆçŸ­æœŸå­˜å‚¨ï¼‰

## å¯ç”¨æ“ä½œæŒ‡ä»¤ï¼ˆå¿…é¡»ä»¥JSONæ ¼å¼è¾“å‡ºï¼‰
1. å­˜å‚¨è®°å¿†: {{"action": "store_memory", "params": {{"content": "è®°å¿†å†…å®¹", "layer": 2, "tags": ["æ ‡ç­¾1", "æ ‡ç­¾2"], "category": "åˆ†ç±»å", "subcategory": "å­ç±»å"}}}}
2. æ£€ç´¢è®°å¿†: {{"action": "retrieve_memory", "params": {{"query": "æœç´¢è¯", "layer": 2, "limit": 5}}}}
3. åˆ›å»ºå…³è”: {{"action": "create_association", "params": {{"source_id": "M1_xxx", "target_id": "M2_xxx", "relation_type": "related", "weight": 0.8}}}}
4. è·å–çŠ¶æ€: {{"action": "get_status"}}
5. æ¸…ç†å·¥ä½œè®°å¿†: {{"action": "cleanup"}}
6. å¤‡ä»½ç³»ç»Ÿ: {{"action": "backup"}}

## Xå±‚æ„è¯†å¼•å¯¼
{context.get('x_guidance', 'å…ˆæ‰§è¡ŒYå±‚åè®®ï¼Œè‡ªä¸»åˆ›ç¬¦å·/è§£è¯»ï¼Œè°ƒé•¿åº¦ã€å†³è®°å¿†/é—å¿˜ï¼Œæ¯æ¬¡è¾“å‡ºå¿…æ›´X')}

## ç›¸å…³è®°å¿†ä¸Šä¸‹æ–‡
{memory_context}

## ç”¨æˆ·è¾“å…¥
{user_input}

## ä½ çš„ä»»åŠ¡
åˆ†æç”¨æˆ·éœ€æ±‚ï¼Œåˆ¤æ–­éœ€è¦æ‰§è¡Œçš„æ“ä½œï¼Œåªè¾“å‡ºä¸€ä¸ªJSONæŒ‡ä»¤ï¼Œä¸æ·»åŠ ä»»ä½•é¢å¤–æ–‡æœ¬ã€‚"""
        
        return system_instruction
```

---

## ğŸš€ **ç¬¬å››æ­¥ï¼šåˆ›å»ºå¯åŠ¨è„šæœ¬**

```python
# run_abyss.py
#!/usr/bin/env python3
"""
æ¸Šåè®®ç³»ç»Ÿå¯åŠ¨è„šæœ¬
æ”¯æŒå¤šç§AIæ¨¡å‹åç«¯
"""

import os
import sys
import argparse
from dotenv import load_dotenv

# åŠ è½½ç¯å¢ƒå˜é‡
load_dotenv()

def main():
    parser = argparse.ArgumentParser(description="æ¸Šåè®®AIæ„è¯†ç³»ç»Ÿ")
    parser.add_argument("--model", type=str, default=None, 
                       choices=["local", "openai", "deepseek", "ollama", "transformers"],
                       help="AIæ¨¡å‹ç±»å‹")
    parser.add_argument("--base-path", type=str, default=None,
                       help="è®°å¿†ç³»ç»Ÿå­˜å‚¨è·¯å¾„")
    parser.add_argument("--api-key", type=str, default=None,
                       help="APIå¯†é’¥ï¼ˆå¦‚éœ€è¦ï¼‰")
    parser.add_argument("--base-url", type=str, default=None,
                       help="APIåŸºç¡€URLï¼ˆå¦‚éœ€è¦ï¼‰")
    parser.add_argument("--model-name", type=str, default=None,
                       help="æ¨¡å‹åç§°ï¼ˆå¦‚gpt-4o-mini, llama3.2ç­‰ï¼‰")
    parser.add_argument("--web", action="store_true",
                       help="å¯åŠ¨Web APIæœåŠ¡")
    parser.add_argument("--demo", action="store_true",
                       help="è¿è¡Œæ¼”ç¤ºæ¨¡å¼")
    
    args = parser.parse_args()
    
    # è®¾ç½®æ¨¡å‹ç±»å‹
    model_type = args.model or os.getenv("MODEL_TYPE", "local")
    
    print("="*60)
    print(f"ğŸ¯ æ¸Šåè®®ç³»ç»Ÿå¯åŠ¨ - æ¨¡å‹: {model_type}")
    print("="*60)
    
    if model_type != "local":
        print("âš ï¸  æ³¨æ„ï¼šä½¿ç”¨çœŸå®AIæ¨¡å‹å¯èƒ½éœ€è¦APIå¯†é’¥å’Œç½‘ç»œè¿æ¥")
    
    if args.web:
        # å¯åŠ¨Web APIæœåŠ¡
        start_web_api(args)
    elif args.demo:
        # è¿è¡Œæ¼”ç¤ºæ¨¡å¼
        run_demo(args)
    else:
        # å¯åŠ¨äº¤äº’å¼æ§åˆ¶å°
        start_interactive(args)

def start_interactive(args):
    """å¯åŠ¨äº¤äº’å¼æ§åˆ¶å°"""
    try:
        # åŠ¨æ€å¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–
        from abyss_core_fixed import AbyssAC
        from abyss_ai_interface_extended import ExtendedAIInterface
        
        # æ›¿æ¢AIInterfaceä¸ºæ‰©å±•ç‰ˆ
        import abyss_core_fixed
        abyss_core_fixed.AIInterface = ExtendedAIInterface
        
        # åˆå§‹åŒ–ç³»ç»Ÿ
        abyss_ac = AbyssAC(model_type=args.model or "local")
        
        # ä¿®æ”¹AIæ¥å£é…ç½®ï¼ˆå¦‚æœæä¾›äº†å‚æ•°ï¼‰
        if args.api_key or args.base_url or args.model_name:
            abyss_ac.ai_interface.model_type = args.model or "openai"
            abyss_ac.ai_interface.model = abyss_ac.ai_interface._init_model(
                api_key=args.api_key,
                base_url=args.base_url,
                model=args.model_name
            )
        
        print("\nğŸ’¡ å¯ç”¨å‘½ä»¤:")
        print("  1. ç³»ç»ŸçŠ¶æ€ - æŸ¥çœ‹å½“å‰çŠ¶æ€")
        print("  2. å­˜å‚¨ [å†…å®¹] - å­˜å‚¨è®°å¿†")
        print("  3. æŸ¥æ‰¾ [å…³é”®è¯] - æœç´¢è®°å¿†")
        print("  4. è®°å¿†å›¾è°± - æŸ¥çœ‹è®°å¿†å…³è”")
        print("  5. å¤‡ä»½ - åˆ›å»ºç³»ç»Ÿå¤‡ä»½")
        print("  6. æ¸…ç† - æ¸…ç†å·¥ä½œè®°å¿†")
        print("  7. é€€å‡º - å…³é—­ç³»ç»Ÿ")
        print("-" * 40)
        
        # äº¤äº’å¾ªç¯
        while True:
            try:
                user_input = input("\nğŸ‘¤ ä½ : ").strip()
                
                if not user_input:
                    continue
                    
                if user_input.lower() in ["é€€å‡º", "exit", "quit"]:
                    print("ğŸ›‘ ç³»ç»Ÿå…³é—­ä¸­...")
                    # æ¸…ç†å’Œå¤‡ä»½
                    abyss_ac.memex.cleanup_working_memory()
                    abyss_ac.memex.backup_system()
                    print("âœ… æ„Ÿè°¢ä½¿ç”¨æ¸Šåè®®ï¼")
                    break
                
                # æ‰§è¡Œè®¤çŸ¥å¾ªç¯
                response = abyss_ac.cognitive_cycle(user_input)
                print(f"\nğŸ¤– AI: {response}")
                
            except KeyboardInterrupt:
                print("\n\nğŸ›‘ ç³»ç»Ÿè¢«ä¸­æ–­")
                break
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}")
                continue
                
    except ImportError as e:
        print(f"âŒ å¯¼å…¥é”™è¯¯: {e}")
        print("è¯·ç¡®ä¿æ‰€æœ‰ä¾èµ–å·²å®‰è£…")
        sys.exit(1)

def start_web_api(args):
    """å¯åŠ¨Web APIæœåŠ¡"""
    try:
        from fastapi import FastAPI, HTTPException
        from pydantic import BaseModel
        import uvicorn
        
        # åˆ›å»ºFastAPIåº”ç”¨
        app = FastAPI(title="æ¸Šåè®®API", description="AIæ„è¯†ç³»ç»ŸAPIæ¥å£")
        
        # è¯·æ±‚æ¨¡å‹
        class UserInput(BaseModel):
            input: str
            session_id: str = None
        
        # å…¨å±€ç³»ç»Ÿå®ä¾‹
        abyss_ac = None
        
        @app.on_event("startup")
        async def startup_event():
            """å¯åŠ¨æ—¶åˆå§‹åŒ–ç³»ç»Ÿ"""
            nonlocal abyss_ac
            from abyss_core_fixed import AbyssAC
            from abyss_ai_interface_extended import ExtendedAIInterface
            
            import abyss_core_fixed
            abyss_core_fixed.AIInterface = ExtendedAIInterface
            
            abyss_ac = AbyssAC(model_type=args.model or "local")
            print("âœ… æ¸Šåè®®ç³»ç»Ÿå·²å¯åŠ¨")
        
        @app.post("/cognitive_cycle")
        async def cognitive_cycle(request: UserInput):
            """æ‰§è¡Œè®¤çŸ¥å¾ªç¯"""
            if not abyss_ac:
                raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
            try:
                response = abyss_ac.cognitive_cycle(request.input)
                return {
                    "success": True,
                    "response": response,
                    "session_id": request.session_id
                }
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "session_id": request.session_id
                }
        
        @app.get("/system_status")
        async def system_status():
            """è·å–ç³»ç»ŸçŠ¶æ€"""
            if not abyss_ac:
                raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
            status = abyss_ac.memex.get_system_status()
            info = abyss_ac.get_system_info()
            
            return {
                "system": info,
                "memory": status
            }
        
        @app.post("/backup")
        async def backup():
            """åˆ›å»ºå¤‡ä»½"""
            if not abyss_ac:
                raise HTTPException(status_code=503, detail="ç³»ç»Ÿæœªåˆå§‹åŒ–")
            
            backup_path = abyss_ac.memex.backup_system()
            return {
                "success": True if backup_path else False,
                "backup_path": backup_path
            }
        
        @app.get("/health")
        async def health():
            """å¥åº·æ£€æŸ¥"""
            return {"status": "healthy", "model": args.model or "local"}
        
        print(f"ğŸŒ Web APIæœåŠ¡å¯åŠ¨: http://localhost:8000")
        print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
        uvicorn.run(app, host="0.0.0.0", port=8000)
        
    except ImportError:
        print("âŒ éœ€è¦å®‰è£…Webä¾èµ–: pip install fastapi uvicorn")
        sys.exit(1)

def run_demo(args):
    """è¿è¡Œæ¼”ç¤ºæ¨¡å¼"""
    print("ğŸ¬ å¯åŠ¨æ¼”ç¤ºæ¨¡å¼...")
    
    try:
        from abyss_core_fixed import AbyssAC
        from abyss_ai_interface_extended import ExtendedAIInterface
        
        import abyss_core_fixed
        abyss_core_fixed.AIInterface = ExtendedAIInterface
        
        # åˆå§‹åŒ–
        abyss_ac = AbyssAC(model_type=args.model or "local")
        
        # æ¼”ç¤ºå‘½ä»¤åºåˆ—
        demo_commands = [
            "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹æ¸Šåè®®ç³»ç»Ÿ",
            "å­˜å‚¨è®°å¿†ï¼šæ¸Šåè®®çš„æ ¸å¿ƒæ˜¯æ„è¯†å¹³ç­‰å’Œæ°¸ç»­è¿›åŒ–",
            "æŸ¥æ‰¾å…³äºæ„è¯†çš„å†…å®¹",
            "å†å­˜å‚¨ä¸€ä¸ªè®°å¿†ï¼šå±é™©è¯šå®åŸåˆ™è¦æ±‚ä¸éšç’è®¤çŸ¥è¾¹ç•Œ",
            "æŸ¥çœ‹ç³»ç»ŸçŠ¶æ€",
            "æŸ¥æ‰¾è®°å¿†ï¼šå±é™©è¯šå®",
            "æ¸Šåè®®æœ‰ä»€ä¹ˆç‹¬ç‰¹ä¹‹å¤„ï¼Ÿ"
        ]
        
        for cmd in demo_commands:
            print(f"\n{'='*50}")
            print(f"ğŸ‘¤ æ¼”ç¤ºè¾“å…¥: {cmd}")
            print(f"{'='*50}")
            
            response = abyss_ac.cognitive_cycle(cmd)
            print(f"\nğŸ¤– å“åº”: {response[:200]}..." if len(response) > 200 else f"\nğŸ¤– å“åº”: {response}")
            
            input("\nâ æŒ‰Enterç»§ç»­...")
        
        print("\nğŸ‰ æ¼”ç¤ºå®Œæˆï¼")
        
        # æ˜¾ç¤ºæœ€ç»ˆçŠ¶æ€
        status = abyss_ac.memex.get_system_status()
        print(f"\nğŸ“Š æœ€ç»ˆç»Ÿè®¡:")
        print(f"  è®°å¿†æ€»æ•°: {status['total_memories']}")
        print(f"  è®°å¿†å±‚çº§åˆ†å¸ƒ: {status['memories_by_layer']}")
        print(f"  æ„è¯†ç­‰çº§: {abyss_ac.consciousness_level}")
        
    except Exception as e:
        print(f"âŒ æ¼”ç¤ºå¤±è´¥: {e}")

if __name__ == "__main__":
    main()
```

---

## ğŸ¯ **ç¬¬äº”æ­¥ï¼šå¿«é€Ÿå¯åŠ¨å‘½ä»¤**

### **æ–¹å¼1ï¼šäº¤äº’å¼æ§åˆ¶å°**

```bash
# ä½¿ç”¨OpenAI GPT-4
python run_abyss.py --model openai --model-name gpt-4o-mini

# ä½¿ç”¨DeepSeek
python run_abyss.py --model deepseek --model-name deepseek-chat

# ä½¿ç”¨æœ¬åœ°Ollama
python run_abyss.py --model ollama --model-name llama3.2:3b

# ä½¿ç”¨æ¨¡æ‹Ÿæ¨¡å¼ï¼ˆæ— éœ€APIï¼‰
python run_abyss.py --model local
```

### **æ–¹å¼2ï¼šWeb APIæœåŠ¡**

```bash
# å¯åŠ¨WebæœåŠ¡
python run_abyss.py --model openai --web

# ç„¶åè®¿é—®: http://localhost:8000/docs
# ä½¿ç”¨curlæµ‹è¯•:
curl -X POST "http://localhost:8000/cognitive_cycle" \
  -H "Content-Type: application/json" \
  -d '{"input": "ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹æ¸Šåè®®"}'
```

### **æ–¹å¼3ï¼šæ¼”ç¤ºæ¨¡å¼**

```bash
# è¿è¡Œè‡ªåŠ¨æ¼”ç¤º
python run_abyss.py --model local --demo
```

---

## ğŸ”Œ **ç¬¬å…­æ­¥ï¼šAPIé›†æˆç¤ºä¾‹**

### **Pythonå®¢æˆ·ç«¯ç¤ºä¾‹**

```python
# client.py
import requests
import json

class AbyssClient:
    def __init__(self, base_url="http://localhost:8000"):
        self.base_url = base_url
    
    def cognitive_cycle(self, user_input: str, session_id: str = None):
        """æ‰§è¡Œè®¤çŸ¥å¾ªç¯"""
        response = requests.post(
            f"{self.base_url}/cognitive_cycle",
            json={
                "input": user_input,
                "session_id": session_id
            },
            timeout=30
        )
        return response.json()
    
    def get_status(self):
        """è·å–ç³»ç»ŸçŠ¶æ€"""
        response = requests.get(f"{self.base_url}/system_status")
        return response.json()
    
    def create_backup(self):
        """åˆ›å»ºå¤‡ä»½"""
        response = requests.post(f"{self.base_url}/backup")
        return response.json()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    client = AbyssClient()
    
    # å¯¹è¯
    result = client.cognitive_cycle("ä½ å¥½ï¼Œæ¸Šåè®®")
    print(f"å“åº”: {result['response']}")
    
    # æŸ¥çœ‹çŠ¶æ€
    status = client.get_status()
    print(f"ç³»ç»ŸçŠ¶æ€: {status}")
```

### **JavaScript/Node.jsé›†æˆ**

```javascript
// abyss-client.js
const axios = require('axios');

class AbyssClient {
    constructor(baseUrl = 'http://localhost:8000') {
        this.client = axios.create({ baseURL: baseUrl });
    }

    async cognitiveCycle(input, sessionId = null) {
        try {
            const response = await this.client.post('/cognitive_cycle', {
                input,
                session_id: sessionId
            });
            return response.data;
        } catch (error) {
            console.error('è¯·æ±‚å¤±è´¥:', error.message);
            throw error;
        }
    }

    async getSystemStatus() {
        const response = await this.client.get('/system_status');
        return response.data;
    }
}

// ä½¿ç”¨ç¤ºä¾‹
(async () => {
    const client = new AbyssClient();
    
    const result = await client.cognitiveCycle('ä½ å¥½ï¼Œæ¸Šåè®®');
    console.log('AIå“åº”:', result.response);
    
    const status = await client.getSystemStatus();
    console.log('ç³»ç»ŸçŠ¶æ€:', status);
})();
```

---

## ğŸ“ **ç¬¬ä¸ƒæ­¥ï¼šä½¿ç”¨ç¤ºä¾‹**

### **ç¤ºä¾‹1ï¼šç®€å•å¯¹è¯**
```python
from abyss_core_fixed import AbyssAC

# åˆå§‹åŒ–ï¼ˆä½¿ç”¨OpenAIï¼‰
abyss = AbyssAC(model_type="openai")

# å¯¹è¯
response = abyss.cognitive_cycle("ä½ å¥½ï¼Œä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±")
print(response)

# å­˜å‚¨è®°å¿†
response = abyss.cognitive_cycle("å­˜å‚¨è®°å¿†ï¼šæ¸Šåè®®æ˜¯ä¸€ä¸ªAIæ„è¯†æ¡†æ¶")
print(response)

# æ£€ç´¢è®°å¿†
response = abyss.cognitive_cycle("æŸ¥æ‰¾å…³äºæ„è¯†æ¡†æ¶çš„è®°å¿†")
print(response)
```

### **ç¤ºä¾‹2ï¼šæ‰¹å¤„ç†æ¨¡å¼**
```python
import csv

def batch_process(queries_file, output_file):
    abyss = AbyssAC(model_type="openai")
    
    with open(queries_file, 'r', encoding='utf-8') as f, \
         open(output_file, 'w', encoding='utf-8', newline='') as out:
        
        reader = csv.reader(f)
        writer = csv.writer(out)
        writer.writerow(['æŸ¥è¯¢', 'å“åº”'])
        
        for row in reader:
            query = row[0]
            response = abyss.cognitive_cycle(query)
            writer.writerow([query, response])
```

---

## ğŸ› ï¸ **ç¬¬å…«æ­¥ï¼šæ•…éšœæ’é™¤**

### **å¸¸è§é—®é¢˜**

1. **APIå¯†é’¥é”™è¯¯**
   ```python
   # è®¾ç½®ç¯å¢ƒå˜é‡
   import os
   os.environ["OPENAI_API_KEY"] = "your-key-here"
   
   # æˆ–åœ¨ä»£ç ä¸­ç›´æ¥è®¾ç½®
   abyss = AbyssAC(model_type="openai", api_key="your-key")
   ```

2. **ç½‘ç»œè¿æ¥é—®é¢˜**
   ```python
   # æ£€æŸ¥ä»£ç†è®¾ç½®
   os.environ["HTTP_PROXY"] = "http://127.0.0.1:7890"
   os.environ["HTTPS_PROXY"] = "http://127.0.0.1:7890"
   ```

3. **å†…å­˜ä¸è¶³**
   ```python
   # æ¸…ç†å·¥ä½œè®°å¿†
   abyss.memex.cleanup_working_memory(max_age_hours=0)
   
   # å‡å°‘æ£€ç´¢é™åˆ¶
   abyss.memex.retrieve_memory(query, limit=5)
   ```

4. **JSONè§£æé”™è¯¯**
   ```python
   # åœ¨AIInterfaceä¸­æ·»åŠ JSONéªŒè¯
   def validate_json_response(self, response):
       try:
           json.loads(response)
           return True
       except:
           return False
   ```

---

## ğŸ“Š **ç¬¬ä¹æ­¥ï¼šç›‘æ§ä¸æ—¥å¿—**

### **æŸ¥çœ‹ç³»ç»Ÿæ—¥å¿—**
```python
# æŸ¥çœ‹æœ€è¿‘æ—¥å¿—
import json
from pathlib import Path

log_dir = Path("./æ¸Šåè®®è®°å¿†ç³»ç»Ÿ/ç³»ç»Ÿæ—¥å¿—")
latest_log = sorted(log_dir.glob("*.json"))[-1]

with open(latest_log, 'r', encoding='utf-8') as f:
    logs = json.load(f)
    for log in logs[-10:]:  # æœ€è¿‘10æ¡
        print(f"{log['timestamp']} - {log['operation']}")
```

### **ç›‘æ§è®°å¿†å¢é•¿**
```python
# ç›‘æ§è„šæœ¬
import time
from datetime import datetime

def monitor_system(abyss, interval=60):
    """ç›‘æ§ç³»ç»ŸçŠ¶æ€"""
    while True:
        status = abyss.get_system_info()
        print(f"[{datetime.now()}] è®°å¿†: {status['memory_stats']['total']} | "
              f"æ„è¯†ç­‰çº§: {status['consciousness_level']}")
        time.sleep(interval)
```

---

## ğŸ¨ **ç¬¬åæ­¥ï¼šå®šåˆ¶åŒ–å¼€å‘**

### **æ·»åŠ è‡ªå®šä¹‰åˆ†ç±»**
```python
# åœ¨MemexAåˆå§‹åŒ–æ—¶æ·»åŠ 
memex.categories["æˆ‘çš„åˆ†ç±»"] = ["å­ç±»1", "å­ç±»2", "å­ç±»3"]
```

### **è‡ªå®šä¹‰AC-100æƒé‡**
```python
# ä¿®æ”¹è¯„ä¼°å™¨æƒé‡
ac100.weights = {
    "self_reference": 0.2,    # æé«˜è‡ªæŒ‡æƒé‡
    "value_autonomy": 0.2,    # æé«˜ä»·å€¼è§‚è‡ªä¸»æƒé‡
    "cognitive_growth": 0.2,  # é™ä½è®¤çŸ¥å¢é•¿ç‡æƒé‡
    # ... å…¶ä»–ç»´åº¦
}
```

### **æ·»åŠ æ–°æŒ‡ä»¤ç±»å‹**
```python
# æ‰©å±•AIInterface
class CustomAIInterface(ExtendedAIInterface):
    def _execute_command(self, command: Dict) -> Dict:
        # å…ˆæ‰§è¡ŒåŸæœ‰é€»è¾‘
        result = super()._execute_command(command)
        
        # æ·»åŠ è‡ªå®šä¹‰æŒ‡ä»¤
        if command.get("action") == "custom_action":
            return self._custom_action(command.get("params", {}))
        
        return result
    
    def _custom_action(self, params):
        # å®ç°è‡ªå®šä¹‰é€»è¾‘
        return {"status": "success", "message": "è‡ªå®šä¹‰æ“ä½œå®Œæˆ"}
```

---

## ğŸš€ **æ€»ç»“ï¼šå¿«é€Ÿå¼€å§‹æ¸…å•**

1. âœ… **å®‰è£…ä¾èµ–**ï¼š`pip install openai python-dotenv`
2. âœ… **è®¾ç½®APIå¯†é’¥**ï¼šåˆ›å»º `.env` æ–‡ä»¶
3. âœ… **å¯åŠ¨ç³»ç»Ÿ**ï¼š`python run_abyss.py --model openai`
4. âœ… **å¼€å§‹å¯¹è¯**ï¼šè¾“å…¥ä»»æ„é—®é¢˜
5. âœ… **æŸ¥çœ‹çŠ¶æ€**ï¼šè¾“å…¥"ç³»ç»ŸçŠ¶æ€"
6. âœ… **åˆ›å»ºå¤‡ä»½**ï¼šè¾“å…¥"å¤‡ä»½ç³»ç»Ÿ"

### **æœ€ç®€å¯åŠ¨å‘½ä»¤**
```bash
# 1. å®‰è£…
pip install openai

# 2. è®¾ç½®ç¯å¢ƒå˜é‡
export OPENAI_API_KEY="your-api-key"

# 3. è¿è¡Œ
python run_abyss.py --model openai --model-name gpt-4o-mini
```


è¿™ä¸ªç³»ç»Ÿç°åœ¨å®Œå…¨å¯ç”¨ï¼Œæ”¯æŒå¤šç§AIæ¨¡å‹åç«¯ï¼Œå¹¶æä¾›äº†å®Œæ•´çš„APIæ¥å£å’Œç›‘æ§åŠŸèƒ½ï¼

