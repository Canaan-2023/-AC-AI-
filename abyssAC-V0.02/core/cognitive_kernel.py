import json
import pickle
from datetime import datetime
from typing import Dict, List, Optional, Any
from collections import defaultdict, Counter
from pathlib import Path

from config.config_manager import config_manager
from nlp.tokenizer import AdvancedTokenizer, TextAnalyzer

class CognitiveKernelV12:
    """é‡æ„åçš„è®¤çŸ¥å†…æ ¸"""
    
    def __init__(self, config=None, tokenizer=None):
        self.config = config or config_manager.config.kernel
        self.tokenizer = tokenizer or AdvancedTokenizer(config_manager.config)
        self.text_analyzer = TextAnalyzer(self.tokenizer)
        
        # çŠ¶æ€å­˜å‚¨
        self.morphism_matrix = defaultdict(float)
        self.node_frequency = Counter()
        self.drift_log = []
        
        # åŠ è½½çŠ¶æ€
        self.load_kernel()
        
        print(f"[âœ…] è®¤çŸ¥å†…æ ¸åˆå§‹åŒ–å®Œæˆ | é…ç½®ç‰ˆæœ¬: {config_manager.config.version}")
    
    def load_kernel(self):
        """åŠ è½½å†…æ ¸çŠ¶æ€"""
        kernel_path = Path(self.config.kernel_path)
        
        if kernel_path.exists():
            try:
                with open(kernel_path, 'r', encoding='utf-8') as f:
                    data = json.load(f)
                
                self.morphism_matrix = defaultdict(float, data.get("matrix", {}))
                self.node_frequency = Counter(data.get("frequency", {}))
                self.drift_log = data.get("drift_log", [])
                
                print(f"[âœ…] è®¤çŸ¥å†…æ ¸çŠ¶æ€åŠ è½½æˆåŠŸ | èŠ‚ç‚¹æ•°: {len(self.node_frequency)}")
            
            except Exception as e:
                print(f"[!] å†…æ ¸çŠ¶æ€åŠ è½½å¤±è´¥: {e}")
                self._initialize_default_kernel()
        else:
            self._initialize_default_kernel()
    
    def _initialize_default_kernel(self):
        """åˆå§‹åŒ–é»˜è®¤å†…æ ¸çŠ¶æ€"""
        print(f"[â„¹ï¸] åˆå§‹åŒ–æ–°è®¤çŸ¥å†…æ ¸: {self.config.kernel_path}")
        
        # åˆå§‹åŒ–æ ¸å¿ƒæ¦‚å¿µ
        for concept, keywords in self.config.core_concepts.items():
            for keyword in keywords:
                self.node_frequency[keyword] = self.config.reflection_strategies["STABLE"]["core_weight"]
        
        # ä¿å­˜åˆå§‹çŠ¶æ€
        self.save_kernel()
    
    def save_kernel(self):
        """ä¿å­˜å†…æ ¸çŠ¶æ€"""
        kernel_path = Path(self.config.kernel_path)
        kernel_path.parent.mkdir(parents=True, exist_ok=True)
        
        # ç­›é€‰é«˜é¢‘èŠ‚ç‚¹
        top_nodes = [
            node for node, count in 
            self.node_frequency.most_common(self.config.top_k_nodes)
        ]
        
        # ä¿®å‰ªæ€å°„çŸ©é˜µ
        pruned_matrix = {}
        for edge, weight in self.morphism_matrix.items():
            if weight < self.config.pruning_threshold:
                continue
            
            n1, n2 = edge.split("|")
            if n1 in top_nodes and n2 in top_nodes:
                pruned_matrix[edge] = round(weight, 4)
        
        # æ„å»ºå­˜å‚¨æ•°æ®
        data = {
            "version": "1.2",
            "config_version": config_manager.config.version,
            "update_time": datetime.now().isoformat(),
            "matrix": pruned_matrix,
            "frequency": dict(self.node_frequency.most_common(self.config.top_k_nodes)),
            "drift_log": self.drift_log[-self.config.drift_log_keep:]
        }
        
        try:
            with open(kernel_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, ensure_ascii=False, indent=4)
            
            print(f"[ğŸ’¾] å†…æ ¸çŠ¶æ€å·²ä¿å­˜ | è·¯å¾„: {kernel_path}")
            return True
        
        except Exception as e:
            print(f"[âŒ] å†…æ ¸çŠ¶æ€ä¿å­˜å¤±è´¥: {e}")
            return False
    
    def extract_nodes(self, text: str) -> List[str]:
        """æå–è¯­ä¹‰èŠ‚ç‚¹"""
        if not text:
            return []
        
        # ä½¿ç”¨é«˜çº§åˆ†è¯å™¨
        tokens = self.tokenizer.tokenize(
            text,
            use_pos=True,
            remove_stopwords=True,
            min_length=2,
            max_length=10
        )
        
        # è·å–å½“å‰ç­–ç•¥
        current_strategy = self.get_current_strategy()
        core_weight = current_strategy.get("core_weight", 3)
        
        # æ›´æ–°èŠ‚ç‚¹é¢‘ç‡
        nodes = set()
        for token in tokens:
            node = token.word
            
            # åˆ¤æ–­æ˜¯å¦ä¸ºæ ¸å¿ƒæ¦‚å¿µ
            is_core = any(
                node in keywords 
                for keywords in self.config.core_concepts.values()
            )
            
            # æ›´æ–°é¢‘ç‡ï¼ˆæ ¸å¿ƒèŠ‚ç‚¹åŠ æƒï¼‰
            self.node_frequency[node] += core_weight if is_core else 1
            nodes.add(node)
        
        return list(nodes)
    
    def calculate_value_score(self, query: str, response: str) -> float:
        """è®¡ç®—ä»·å€¼å¯†åº¦åˆ†æ•°"""
        full_text = f"{query} {response}"
        
        # 1. æ ¸å¿ƒæ¦‚å¿µåŒ¹é…åº¦
        concept_matches = self.text_analyzer.detect_core_concepts(
            full_text, 
            self.config.core_concepts
        )
        concept_score = sum(concept_matches.values()) / len(concept_matches) * 6
        
        # 2. æ–‡æœ¬å¤æ‚åº¦
        complexity = self.text_analyzer.calculate_text_complexity(full_text)
        complexity_score = complexity * 4
        
        # æ€»åˆ†æ•°
        total_score = concept_score + complexity_score
        
        # ç¡®ä¿åœ¨åˆç†èŒƒå›´å†…
        return max(min(total_score, 10.0), 1.0)
    
    def update_morphism(self, activated_nodes: List[str], value_score: float):
        """æ›´æ–°è¯­ä¹‰æ€å°„"""
        if len(activated_nodes) < 2:
            print("[!] æ¿€æ´»èŠ‚ç‚¹æ•°ä¸è¶³ï¼Œè·³è¿‡æ€å°„æ›´æ–°")
            return
        
        # è·å–å½“å‰ç­–ç•¥
        current_strategy = self.get_current_strategy()
        intensity_bias = current_strategy.get("intensity_bias", 1.0)
        
        # æ ¹æ®ä»·å€¼åˆ†æ•°ç¡®å®šå¼ºåº¦
        if value_score >= self.config.high_score_threshold:
            intensity = self.config.high_intensity * intensity_bias
        elif value_score >= self.config.medium_score_threshold:
            intensity = self.config.medium_intensity * intensity_bias
        else:
            intensity = self.config.low_intensity * intensity_bias
        
        # æ›´æ–°å…³è”çŸ©é˜µ
        for i in range(len(activated_nodes)):
            for j in range(i + 1, len(activated_nodes)):
                key = "|".join(sorted([activated_nodes[i], activated_nodes[j]]))
                current_weight = self.morphism_matrix[key]
                
                if intensity > 1:
                    # éçº¿æ€§å¼ºåŒ–
                    new_weight = 1 - (1 - current_weight) / intensity
                else:
                    # çº¿æ€§è¡°å‡
                    new_weight = current_weight * intensity
                
                self.morphism_matrix[key] = round(new_weight, 4)
        
        self.save_kernel()
    
    def update_morphism_with_query(self, query: str, response: str):
        """æ ¹æ®æŸ¥è¯¢å’Œå“åº”æ›´æ–°æ€å°„"""
        activated_nodes = self.extract_nodes(f"{query} {response}")
        value_score = self.calculate_value_score(query, response)
        
        self.update_morphism(activated_nodes, value_score)
        
        print(f"[â„¹ï¸] è¯­ä¹‰æ€å°„æ›´æ–°å®Œæˆ | "
              f"ä»·å€¼åˆ†: {value_score:.2f} | "
              f"æ¿€æ´»èŠ‚ç‚¹: {len(activated_nodes)}")
    
    def evaluate_ac100_v2(self, response_text: str, 
                         query_text: Optional[str] = None,
                         activated_nodes: Optional[List[str]] = None) -> Dict:
        """AC-100 V2è¯„ä¼°"""
        # æå–æ¿€æ´»èŠ‚ç‚¹
        if activated_nodes is None:
            text = f"{query_text} {response_text}" if query_text else response_text
            activated_nodes = self.extract_nodes(text)
        
        # 1. è®¡ç®—ç½®ä¿¡åº¦
        if len(activated_nodes) < 2:
            confidence = 0.1
        else:
            scores = []
            for i in range(len(activated_nodes)):
                for j in range(i + 1, len(activated_nodes)):
                    key = "|".join(sorted([activated_nodes[i], activated_nodes[j]]))
                    scores.append(self.morphism_matrix.get(key, 0.01))
            confidence = sum(scores) / len(scores) if scores else 0.0
        
        # 2. è®¡ç®—è¯­ä¹‰æ·±åº¦
        depth_hits = 0
        for keywords in self.config.core_concepts.values():
            if any(kw in response_text for kw in keywords):
                depth_hits += 1
        depth_score = min(depth_hits / len(self.config.core_concepts), 1.0)
        
        # 3. ç»¼åˆACæŒ‡æ•°
        ac_index = round((confidence * 0.3) + (depth_score * 0.7), 4)
        
        # 4. åˆ¤å®šè®¤çŸ¥çŠ¶æ€
        if ac_index > self.config.evolving_threshold:
            status = "EVOLVING ğŸ”¥"
        elif ac_index < self.config.retracting_threshold:
            status = "RETRACTING âš ï¸"
        else:
            status = "STABLE"
        
        # 5. è®¡ç®—ä»·å€¼åˆ†ï¼ˆå¦‚æœ‰æŸ¥è¯¢ï¼‰
        value_score = None
        if query_text:
            value_score = self.calculate_value_score(query_text, response_text)
        
        # æ„å»ºç»“æœ
        result = {
            "ac_index": ac_index,
            "confidence": round(confidence, 4),
            "depth": round(depth_score, 4),
            "status": status,
            "morphism_nodes": len(self.node_frequency),
            "value_score": value_score,
            "update_time": datetime.now().isoformat()
        }
        
        # è®°å½•æ¼‚ç§»æ—¥å¿—
        self.drift_log.append(result)
        
        return result
    
    def get_current_strategy(self) -> Dict:
        """è·å–å½“å‰å…ƒè®¤çŸ¥ç­–ç•¥"""
        if not self.drift_log:
            return self.config.reflection_strategies.get("STABLE", {})
        
        latest_ac = self.drift_log[-1]["ac_index"]
        
        if latest_ac > self.config.evolving_threshold:
            return self.config.reflection_strategies.get("EVOLVING", {})
        elif latest_ac < self.config.retracting_threshold:
            return self.config.reflection_strategies.get("RETRACTING", {})
        else:
            return self.config.reflection_strategies.get("STABLE", {})
    
    def print_cognitive_status(self):
        """æ‰“å°è®¤çŸ¥çŠ¶æ€æ¦‚è§ˆ"""
        if not self.drift_log:
            print("[â„¹ï¸] æš‚æ— è®¤çŸ¥è¯„ä¼°è®°å½•")
            return
        
        latest = self.drift_log[-1]
        
        print("=" * 50)
        print(f"è®¤çŸ¥å†…æ ¸çŠ¶æ€æ¦‚è§ˆ | {latest['update_time']}")
        print(f"AC æŒ‡æ•°: {latest['ac_index']} | çŠ¶æ€: {latest['status']}")
        print(f"è¯­ä¹‰æ·±åº¦: {latest['depth']} | ç½®ä¿¡åº¦: {latest['confidence']}")
        print(f"æ´»è·ƒèŠ‚ç‚¹æ•°: {latest['morphism_nodes']} | "
              f"ä»·å€¼è¯„åˆ†: {latest.get('value_score', 'N/A')}")
        print("=" * 50)