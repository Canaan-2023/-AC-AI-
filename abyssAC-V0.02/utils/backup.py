# utils/backup.py
#!/usr/bin/env python3
"""
å¤‡ä»½å’Œæ¢å¤ç³»ç»Ÿ
"""

import zipfile
import tarfile
import json
import shutil
import hashlib
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional
import tempfile

class BackupSystem:
    """å¤‡ä»½ç³»ç»Ÿ"""
    
    def __init__(self, backup_dir: str = "./backups", max_backups: int = 10):
        self.backup_dir = Path(backup_dir)
        self.max_backups = max_backups
        self.backup_dir.mkdir(parents=True, exist_ok=True)
    
    def create_backup(self, source_dirs: List[str], backup_name: str = None, 
                     compression: str = "zip") -> Optional[str]:
        """åˆ›å»ºå¤‡ä»½"""
        try:
            # ç”Ÿæˆå¤‡ä»½åç§°
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = backup_name or f"backup_{timestamp}"
            
            backup_path = self.backup_dir / f"{backup_name}.{compression}"
            
            # åˆ›å»ºå¤‡ä»½
            if compression == "zip":
                with zipfile.ZipFile(backup_path, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for source_dir in source_dirs:
                        source_path = Path(source_dir)
                        if source_path.exists():
                            self._add_directory_to_zip(zipf, source_path)
            
            elif compression == "tar.gz":
                with tarfile.open(backup_path, "w:gz") as tar:
                    for source_dir in source_dirs:
                        source_path = Path(source_dir)
                        if source_path.exists():
                            tar.add(source_path, arcname=source_path.name)
            
            # åˆ›å»ºå¤‡ä»½å…ƒæ•°æ®
            metadata = {
                "backup_name": backup_name,
                "backup_path": str(backup_path),
                "created_at": datetime.now().isoformat(),
                "compression": compression,
                "source_dirs": source_dirs,
                "checksum": self._calculate_checksum(backup_path),
                "size_bytes": backup_path.stat().st_size
            }
            
            metadata_path = backup_path.with_suffix('.json')
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            # æ¸…ç†æ—§å¤‡ä»½
            self._cleanup_old_backups()
            
            print(f"âœ… å¤‡ä»½åˆ›å»ºæˆåŠŸ: {backup_path}")
            return str(backup_path)
        
        except Exception as e:
            print(f"âŒ å¤‡ä»½åˆ›å»ºå¤±è´¥: {e}")
            return None
    
    def restore_backup(self, backup_path: str, target_dir: str = None, 
                      overwrite: bool = False) -> bool:
        """æ¢å¤å¤‡ä»½"""
        try:
            backup_path = Path(backup_path)
            if not backup_path.exists():
                print(f"âŒ å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
                return False
            
            # éªŒè¯å¤‡ä»½å®Œæ•´æ€§
            if not self._verify_backup(backup_path):
                print("âŒ å¤‡ä»½æ–‡ä»¶æŸåæˆ–éªŒè¯å¤±è´¥")
                return False
            
            # ç¡®å®šç›®æ ‡ç›®å½•
            if target_dir is None:
                # ä»å…ƒæ•°æ®ä¸­è¯»å–åŸå§‹ç›®å½•
                metadata_path = backup_path.with_suffix('.json')
                if metadata_path.exists():
                    with open(metadata_path, 'r', encoding='utf-8') as f:
                        metadata = json.load(f)
                    target_dir = metadata.get("source_dirs", ["./"])[0]
                else:
                    target_dir = "./"
            
            target_path = Path(target_dir)
            
            # æ£€æŸ¥ç›®æ ‡ç›®å½•
            if target_path.exists() and not overwrite:
                print(f"âŒ ç›®æ ‡ç›®å½•å·²å­˜åœ¨ä¸”æœªæŒ‡å®šè¦†ç›–: {target_path}")
                return False
            
            # åˆ›å»ºä¸´æ—¶ç›®å½•ç”¨äºè§£å‹
            with tempfile.TemporaryDirectory() as temp_dir:
                temp_path = Path(temp_dir)
                
                # è§£å‹å¤‡ä»½
                if backup_path.suffix == '.zip':
                    with zipfile.ZipFile(backup_path, 'r') as zipf:
                        zipf.extractall(temp_path)
                
                elif backup_path.suffix in ['.tar.gz', '.tgz']:
                    with tarfile.open(backup_path, "r:gz") as tar:
                        tar.extractall(temp_path)
                
                # æ¢å¤æ–‡ä»¶
                self._restore_files(temp_path, target_path, overwrite)
            
            print(f"âœ… å¤‡ä»½æ¢å¤æˆåŠŸ: {backup_path} -> {target_path}")
            return True
        
        except Exception as e:
            print(f"âŒ å¤‡ä»½æ¢å¤å¤±è´¥: {e}")
            return False
    
    def list_backups(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        
        for backup_file in self.backup_dir.glob("*.json"):
            try:
                with open(backup_file, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                
                # æ£€æŸ¥å¯¹åº”çš„å¤‡ä»½æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                backup_path = Path(metadata.get("backup_path", ""))
                if backup_path.exists():
                    metadata["exists"] = True
                    metadata["actual_size"] = backup_path.stat().st_size
                else:
                    metadata["exists"] = False
                
                backups.append(metadata)
            
            except Exception:
                continue
        
        # æŒ‰åˆ›å»ºæ—¶é—´æ’åº
        backups.sort(key=lambda x: x.get("created_at", ""), reverse=True)
        return backups
    
    def delete_backup(self, backup_name: str) -> bool:
        """åˆ é™¤å¤‡ä»½"""
        try:
            # æŸ¥æ‰¾å¤‡ä»½æ–‡ä»¶
            backup_files = list(self.backup_dir.glob(f"{backup_name}.*"))
            if not backup_files:
                print(f"âŒ æœªæ‰¾åˆ°å¤‡ä»½: {backup_name}")
                return False
            
            # åˆ é™¤æ‰€æœ‰ç›¸å…³æ–‡ä»¶
            for backup_file in backup_files:
                backup_file.unlink()
            
            print(f"âœ… å¤‡ä»½å·²åˆ é™¤: {backup_name}")
            return True
        
        except Exception as e:
            print(f"âŒ å¤‡ä»½åˆ é™¤å¤±è´¥: {e}")
            return False
    
    def _add_directory_to_zip(self, zipf: zipfile.ZipFile, directory: Path):
        """é€’å½’æ·»åŠ ç›®å½•åˆ°zipæ–‡ä»¶"""
        for item in directory.rglob("*"):
            if item.is_file():
                arcname = item.relative_to(directory.parent)
                zipf.write(item, arcname)
    
    def _calculate_checksum(self, file_path: Path) -> str:
        """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
        sha256_hash = hashlib.sha256()
        with open(file_path, "rb") as f:
            for byte_block in iter(lambda: f.read(4096), b""):
                sha256_hash.update(byte_block)
        return sha256_hash.hexdigest()
    
    def _verify_backup(self, backup_path: Path) -> bool:
        """éªŒè¯å¤‡ä»½å®Œæ•´æ€§"""
        try:
            # æ£€æŸ¥å…ƒæ•°æ®æ–‡ä»¶
            metadata_path = backup_path.with_suffix('.json')
            if not metadata_path.exists():
                return False
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # éªŒè¯æ ¡éªŒå’Œ
            expected_checksum = metadata.get("checksum")
            if expected_checksum:
                actual_checksum = self._calculate_checksum(backup_path)
                if actual_checksum != expected_checksum:
                    print(f"âŒ æ ¡éªŒå’Œä¸åŒ¹é…: {actual_checksum} != {expected_checksum}")
                    return False
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            expected_size = metadata.get("size_bytes", 0)
            actual_size = backup_path.stat().st_size
            if actual_size != expected_size:
                print(f"âŒ æ–‡ä»¶å¤§å°ä¸åŒ¹é…: {actual_size} != {expected_size}")
                return False
            
            return True
        
        except Exception as e:
            print(f"âŒ å¤‡ä»½éªŒè¯å¤±è´¥: {e}")
            return False
    
    def _restore_files(self, source_dir: Path, target_dir: Path, overwrite: bool):
        """æ¢å¤æ–‡ä»¶åˆ°ç›®æ ‡ç›®å½•"""
        # å¦‚æœç›®æ ‡ç›®å½•å­˜åœ¨ä¸”éœ€è¦è¦†ç›–ï¼Œå…ˆåˆ é™¤
        if target_dir.exists() and overwrite:
            shutil.rmtree(target_dir)
        
        # å¤åˆ¶æ–‡ä»¶
        shutil.copytree(source_dir, target_dir, dirs_exist_ok=True)
    
    def _cleanup_old_backups(self):
        """æ¸…ç†æ—§å¤‡ä»½"""
        backups = self.list_backups()
        
        if len(backups) > self.max_backups:
            # ä¿ç•™æœ€æ–°çš„max_backupsä¸ªå¤‡ä»½
            backups_to_delete = backups[self.max_backups:]
            
            for backup in backups_to_delete:
                backup_path = backup.get("backup_path")
                if backup_path:
                    try:
                        Path(backup_path).unlink()
                        print(f"ğŸ—‘ï¸  åˆ é™¤æ—§å¤‡ä»½: {backup_path}")
                    except Exception:
                        pass

# å…¨å±€å¤‡ä»½ç³»ç»Ÿå®ä¾‹
backup_system = BackupSystem(max_backups=10)