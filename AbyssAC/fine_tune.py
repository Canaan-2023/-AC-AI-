# ä¿®æ”¹åçš„ä»£ç æ”¯æŒåŠ¨æ€æŒ‡å®šä»»æ„æ¨¡å‹ï¼Œä¸»è¦å˜æ›´å¦‚ä¸‹ï¼š
import json
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
import logging
import time
from typing import List, Dict, Any, Optional
from transformers import (
    AutoModelForCausalLM, AutoTokenizer,
    TrainingArguments, Trainer, DataCollatorForLanguageModeling
)
from datasets import Dataset, DatasetDict
import torch
from memex_a import MemexA, Config
# ===================== æ—¥å¿—é…ç½® =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - [FineTune] %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("fine_tune.log", encoding="utf-8")
    ]
)
logger = logging.getLogger("Memex-FineTune")
# ===================== æ ‡å‡†åŒ–è®­ç»ƒé…ç½® =====================
# ç§»é™¤å›ºå®šæ¨¡å‹åç§°ï¼Œæ”¹ä¸ºåŠ¨æ€ä¼ å…¥
BASE_CONFIG = {
    "output_dir": "./finetuned_model",
    "overwrite_output_dir": True,
    "epochs": 3,
    "batch_size": 4,
    "gradient_accumulation_steps": 2,
    "learning_rate": 1e-4,
    "warmup_ratio": 0.1,
    "weight_decay": 0.01,
    "max_seq_length": 2048,
    "logging_steps": 10,
    "save_steps": 50,
    "save_total_limit": 2,
    "fp16": torch.cuda.is_available(),
    "load_best_model_at_end": True,
    "metric_for_best_model": "loss",
    "greater_is_better": False,
    "report_to": "none"
}
# é™çº§é…ç½®ä¹Ÿæ”¹ä¸ºç›¸å¯¹å€¼è°ƒæ•´ï¼Œä¸å›ºå®šæ¨¡å‹
FALLBACK_ADJUSTMENTS = {
    "batch_size": 0.5,  # åŸæ‰¹æ¬¡å¤§å°çš„ä¸€åŠ
    "learning_rate": 0.5,  # åŸå­¦ä¹ ç‡çš„ä¸€åŠ
    "epochs": 0.67,  # çº¦2/3åŸepochs
    "max_seq_length": 0.5  # çº¦ä¸€åŠåºåˆ—é•¿åº¦
}
# ===================== æ•°æ®åŠ è½½ =====================
def load_finetune_data(max_samples_per_level: int = 20) -> List[Dict[str, str]]:
    try:
        config = Config.from_json()
        memex = MemexA(config=config)
        data = []
        system_prompt = {
            "prompt": "ä½ æ˜¯Memex-Aè®°å¿†è¾…åŠ©AIï¼Œç²¾é€šè®¤çŸ¥ç§‘å­¦å’Œè®°å¿†ç®¡ç†ï¼Œèƒ½å¤ŸåŸºäºç”¨æˆ·çš„è®°å¿†å†…å®¹æä¾›ä¸“ä¸šã€ç®€æ´çš„è§£è¯»ã€åˆ†æå’Œå…³è”å»ºè®®ã€‚",
            "response": "æ˜ç™½ï¼æˆ‘å°†åŸºäºä½ çš„è®°å¿†å†…å®¹ï¼Œæä¾›ç¬¦åˆè®¤çŸ¥ç§‘å­¦åŸç†çš„ä¸“ä¸šè§£è¯»ã€åˆ†æå’Œå…³è”å»ºè®®ï¼Œå¸®åŠ©ä½ æ·±åŒ–è®°å¿†ç†è§£å’ŒçŸ¥è¯†æ•´åˆã€‚"
        }
        data.append(system_prompt)
        memory_levels = ["æ ¸å¿ƒ", "å…ƒè®¤çŸ¥", "å·¥ä½œ", "æƒ…æ„Ÿ"]
        for level in memory_levels:
            logger.info(f"ğŸ“¥ åŠ è½½ã€Œ{level}ã€å±‚çº§è®°å¿†ï¼ˆæœ€å¤š{max_samples_per_level}æ¡ï¼‰")
            memories = memex.advanced_search(
                filters={
                    "levels": [level],
                    "min_strength": 0.7,
                    "exclude_expired": True
                }
            )
            memories_sorted = sorted(memories, key=lambda x: x["æœ€å¤§å…³è”å¼ºåº¦"], reverse=True)
            selected_memories = memories_sorted[:max_samples_per_level]
            for mem in selected_memories:
                mid = mem["è®°å¿†ID"]
                full_content = memex.get_full_content(mid) or mem["å†…å®¹æ‘˜è¦"]
                prompt = f"è¯·åŸºäºè®¤çŸ¥ç§‘å­¦åŸç†ï¼Œè§£è¯»ä»¥ä¸‹{level}è®°å¿†çš„æ ¸å¿ƒä»·å€¼ã€å…³è”æ„ä¹‰å’Œæ·±åŒ–å»ºè®®ï¼š\n\nè®°å¿†å†…å®¹ï¼š{full_content}"
                response = generate_reference_response(level, full_content, mem["å…³è”è®°å¿†"])
                data.append({"prompt": prompt, "response": response})
        with memex._cache_lock:
            be_token_cache = memex._read_json(config.BE_TOKEN_PATH)
            active_tokens = [t for t in be_token_cache.values() if t.get("status") == "active"]
            if active_tokens:
                token = active_tokens[0]
                prompt = f"è¯·åˆ†æä»¥ä¸‹BE Tokençš„ç›®æ ‡ç»´åº¦ã€å½“å‰è¿›åº¦ï¼Œå¹¶æä¾›åŠ é€Ÿè¾¾æˆç›®æ ‡çš„è®°å¿†ç®¡ç†å»ºè®®ï¼š\n\nç›®æ ‡ç»´åº¦ï¼š{token['target_dimension']}\nå½“å‰è¿›åº¦ï¼š{token['current_progress']}/{token['target_value']}\nå…³è”è®°å¿†ï¼š{token['related_memory_ids']}"
                response = f"### ç›®æ ‡ç»´åº¦åˆ†æï¼š{token['target_dimension']}\n- æ ¸å¿ƒæ„ä¹‰ï¼š{'æå‡è®°å¿†æ•´åˆæ•ˆç‡' if token['target_dimension'] == 'å…ƒå—æ•´åˆåº¦' else 'å¢å¼ºè®°å¿†è¿è´¯æ€§' if token['target_dimension'] == 'è·¨ä¼šè¯ç›¸å¹²æ€§' else 'åŠ é€Ÿè®¤çŸ¥èƒ½åŠ›æˆé•¿'}\n- å½“å‰è¿›åº¦è¯„ä¼°ï¼š{'è‰¯å¥½' if token['current_progress'] >= 0.7 else 'ä¸€èˆ¬' if token['current_progress'] >= 0.5 else 'å¾…æå‡'}\n- ä¼˜åŒ–å»ºè®®ï¼š1. å¢åŠ å…³è”è®°å¿†çš„æ£€ç´¢é¢‘ç‡ï¼›2. è¡¥å……ç›¸å…³é¢†åŸŸçš„æ ¸å¿ƒè®°å¿†ï¼›3. å®šæœŸå¤ç›˜å…³è”å¼ºåº¦å˜åŒ–ï¼›4. åˆ©ç”¨é—´éš”é‡å¤å¼ºåŒ–å…³é”®è®°å¿†ã€‚"
                data.append({"prompt": prompt, "response": response})
        logger.info(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼šå…±{len(data)}æ¡è®­ç»ƒæ ·æœ¬")
        return data
    except Exception as e:
        logger.error(f"âŒ åŠ è½½å¾®è°ƒæ•°æ®å¤±è´¥ï¼š{e}", exc_info=True)
        return []

def generate_reference_response(level: str, content: str, related_memories: Dict[str, float]) -> str:
    level_analysis = {
        "æ ¸å¿ƒ": "è¯¥æ ¸å¿ƒè®°å¿†æ˜¯çŸ¥è¯†ä½“ç³»çš„åŸºçŸ³ï¼Œå…·æœ‰é«˜ç¨³å®šæ€§å’Œå¼ºå…³è”ä»·å€¼ã€‚",
        "å…ƒè®¤çŸ¥": "è¯¥å…ƒè®¤çŸ¥è®°å¿†åæ˜ äº†å¯¹è‡ªèº«è®¤çŸ¥è¿‡ç¨‹çš„ç†è§£ï¼Œæœ‰åŠ©äºä¼˜åŒ–å­¦ä¹ ç­–ç•¥ã€‚",
        "å·¥ä½œ": "è¯¥å·¥ä½œè®°å¿†æ˜¯å½“å‰ä»»åŠ¡çš„å…³é”®ä¿¡æ¯ï¼Œéœ€åŠæ—¶ä¸æ ¸å¿ƒè®°å¿†å»ºç«‹å…³è”ä»¥ä¿ƒè¿›è½¬åŒ–ã€‚",
        "æƒ…æ„Ÿ": "è¯¥æƒ…æ„Ÿè®°å¿†ä¸ºè®¤çŸ¥è¿‡ç¨‹æä¾›åŠ¨æœºæ”¯æŒï¼Œç§¯ææƒ…æ„Ÿæœ‰åŠ©äºè®°å¿†å·©å›ºã€‚"
    }
    related_analysis = f"å…³è”è®°å¿†å¼ºåº¦åˆ†æï¼š{related_memories}" if related_memories else "æš‚æœªå»ºç«‹æœ‰æ•ˆå…³è”ï¼Œå»ºè®®ä¸»åŠ¨å…³è”æ ¸å¿ƒ/å…ƒè®¤çŸ¥è®°å¿†"
    return (
        f"### è®°å¿†è§£è¯»ï¼ˆ{level}å±‚çº§ï¼‰\n"
        f"- æ ¸å¿ƒä»·å€¼ï¼š{level_analysis[level]}\n"
        f"- å†…å®¹è¦ç‚¹ï¼š{content[:100]}...\n"
        f"- å…³è”æ„ä¹‰ï¼š{related_analysis}\n"
        f"- æ·±åŒ–å»ºè®®ï¼š1. å®šæœŸæ£€ç´¢å¼ºåŒ–è®°å¿†ç—•è¿¹ï¼›2. è¡¥å……ç›¸å…³é¢†åŸŸçš„å»¶ä¼¸è®°å¿†ï¼›3. å°è¯•ç”¨è‡ªå·±çš„è¯­è¨€é‡æ„è®°å¿†å†…å®¹ï¼›4. å»ºç«‹è·¨å±‚çº§çš„è®°å¿†å…³è”ã€‚"
    )
# ===================== æ•°æ®é¢„å¤„ç† =====================
# ä¿æŒä¸å˜ï¼Œä½†ç¡®ä¿å…¼å®¹ä¸åŒtokenizer
def preprocess_data(data: List[Dict[str, str]], tokenizer: AutoTokenizer, max_seq_length: int) -> DatasetDict:
    try:
        formatted_texts = []
        for item in data:
            text = f"ç”¨æˆ·ï¼š{item['prompt']}\nAIï¼š{item['response']}\n"
            formatted_texts.append(text)
        dataset = Dataset.from_dict({"text": formatted_texts})
        
        def tokenize_function(examples):
            return tokenizer(
                examples["text"],
                truncation=True,
                max_length=max_seq_length,
                padding="max_length",
                return_overflowing_tokens=False
            )
        
        tokenized_dataset = dataset.map(
            tokenize_function,
            batched=True,
            remove_columns=["text"]
        )
        split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)
        logger.info(f"âœ… æ•°æ®é¢„å¤„ç†å®Œæˆï¼šè®­ç»ƒé›†{len(split_dataset['train'])}æ¡ï¼ŒéªŒè¯é›†{len(split_dataset['test'])}æ¡")
        return split_dataset
    except Exception as e:
        logger.error(f"âŒ æ•°æ®é¢„å¤„ç†å¤±è´¥ï¼š{e}", exc_info=True)
        return DatasetDict()
# ===================== è®­ç»ƒé€»è¾‘ =====================
def train(model_name: str, config: Dict[str, Any] = None) -> bool:
    """
    æ”¯æŒä»»æ„æ¨¡å‹çš„è®­ç»ƒå‡½æ•°
    :param model_name: æ¨¡å‹åç§°æˆ–æœ¬åœ°è·¯å¾„
    :param config: è®­ç»ƒé…ç½®å‚æ•°
    """
    current_config = {**BASE_CONFIG,** config} if config else BASE_CONFIG
    logger.info(f"\n" + "="*60)
    logger.info(f"ğŸš€ å¯åŠ¨å¾®è°ƒè®­ç»ƒï¼ˆæ¨¡å‹ï¼š{model_name}ï¼‰")
    logger.info(f"="*60)
    
    try:
        raw_data = load_finetune_data()
        if len(raw_data) < 10:
            logger.error(f"âŒ è®­ç»ƒæ•°æ®ä¸è¶³ï¼ˆä»…{len(raw_data)}æ¡ï¼‰ï¼Œç»ˆæ­¢è®­ç»ƒ")
            return False
            
        logger.info(f"ğŸ“¥ åŠ è½½æ¨¡å‹ï¼š{model_name}")
        # å¢åŠ æ¨¡å‹åŠ è½½çš„çµæ´»æ€§ï¼Œæ”¯æŒä¸åŒæ¨¡å‹çš„ç‰¹æ®Šå‚æ•°
        tokenizer_kwargs = {
            "trust_remote_code": True,
            "padding_side": "right"
        }
        model_kwargs = {
            "trust_remote_code": True,
            "torch_dtype": torch.float16 if current_config["fp16"] else torch.float32,
            "device_map": "auto"
        }
        
        # é’ˆå¯¹ä¸åŒæ¨¡å‹å®¶æ—çš„ç‰¹æ®Šå¤„ç†
        if "llama" in model_name.lower() or "alpaca" in model_name.lower():
            tokenizer_kwargs["use_fast"] = False
        
        tokenizer = AutoTokenizer.from_pretrained(model_name, **tokenizer_kwargs)
        if tokenizer.pad_token is None:
            tokenizer.pad_token = tokenizer.eos_token
            
        model = AutoModelForCausalLM.from_pretrained(model_name,** model_kwargs)
        model.config.use_cache = False
        
        tokenized_dataset = preprocess_data(raw_data, tokenizer, current_config["max_seq_length"])
        if not tokenized_dataset or len(tokenized_dataset["train"]) == 0:
            logger.error(f"âŒ é¢„å¤„ç†åæ— æœ‰æ•ˆè®­ç»ƒæ•°æ®ï¼Œç»ˆæ­¢è®­ç»ƒ")
            return False
            
        training_args = TrainingArguments(
            output_dir=current_config["output_dir"],
            overwrite_output_dir=current_config["overwrite_output_dir"],
            num_train_epochs=current_config["epochs"],
            per_device_train_batch_size=current_config["batch_size"],
            per_device_eval_batch_size=current_config["batch_size"] * 2,
            gradient_accumulation_steps=current_config["gradient_accumulation_steps"],
            learning_rate=current_config["learning_rate"],
            warmup_ratio=current_config["warmup_ratio"],
            weight_decay=current_config["weight_decay"],
            logging_steps=current_config["logging_steps"],
            save_steps=current_config["save_steps"],
            save_total_limit=current_config["save_total_limit"],
            fp16=current_config["fp16"],
            load_best_model_at_end=current_config["load_best_model_at_end"],
            metric_for_best_model=current_config["metric_for_best_model"],
            greater_is_better=current_config["greater_is_better"],
            report_to=current_config["report_to"],
            evaluation_strategy="epoch",
            eval_accumulation_steps=current_config["gradient_accumulation_steps"]
        )
        
        data_collator = DataCollatorForLanguageModeling(
            tokenizer=tokenizer,
            mlm=False,
            pad_to_multiple_of=8 if current_config["fp16"] else None
        )
        
        trainer = Trainer(
            model=model,
            args=training_args,
            train_dataset=tokenized_dataset["train"],
            eval_dataset=tokenized_dataset["test"],
            data_collator=data_collator,
            tokenizer=tokenizer
        )
        
        logger.info(f"ğŸ¬ å¼€å§‹è®­ç»ƒï¼ˆè®¾å¤‡ï¼š{trainer.args.device}ï¼‰")
        start_time = time.time()
        train_result = trainer.train()
        training_time = (time.time() - start_time) / 3600
        
        # è®­ç»ƒç»“æœå¤„ç†
        trainer.save_model(current_config["output_dir"])
        metrics = train_result.metrics
        trainer.log_metrics("train", metrics)
        trainer.save_metrics("train", metrics)
        trainer.save_state()
        
        logger.info(f"\n" + "="*60)
        logger.info(f"ğŸ“Š è®­ç»ƒå®Œæˆï¼")
        logger.info(f"è®­ç»ƒæ—¶é•¿ï¼š{training_time:.2f}å°æ—¶")
        logger.info(f"è®­ç»ƒæŸå¤±ï¼š{metrics['train_loss']:.4f}")
        logger.info(f"è¯„ä¼°æŸå¤±ï¼š{trainer.evaluate()['eval_loss']:.4f}")
        logger.info("="*60)
        
        return True
    except Exception as e:
        logger.error(f"âŒ è®­ç»ƒå¤±è´¥ï¼š{e}", exc_info=True)
        return False
# ===================== å¸¦é™çº§ç­–ç•¥çš„è®­ç»ƒå…¥å£ =====================
def train_with_fallback(model_names: List[str], base_config: Optional[Dict[str, Any]] = None):
    """
    æ”¯æŒå¤šæ¨¡å‹å°è¯•çš„è®­ç»ƒå…¥å£
    :param model_names: æ¨¡å‹åç§°åˆ—è¡¨ï¼ŒæŒ‰ä¼˜å…ˆçº§æ’åº
    :param base_config: åŸºç¡€è®­ç»ƒé…ç½®
    """
    logger.info(f"\n" + "="*80)
    logger.info(f"ğŸš€ å¯åŠ¨å¤šæ¨¡å‹å¾®è°ƒè®­ç»ƒï¼ˆå°è¯•æ¨¡å‹ï¼š{model_names}ï¼‰")
    logger.info("="*80)
    
    base_config = base_config or BASE_CONFIG
    
    for i, model_name in enumerate(model_names):
        # ç¬¬ä¸€æ¬¡å°è¯•ä½¿ç”¨åŸå§‹é…ç½®
        if i == 0:
            logger.info(f"ğŸ“Œ ç¬¬{i+1}æ¬¡å°è¯•ï¼šä½¿ç”¨æ¨¡å‹ {model_name}ï¼ˆåŸå§‹é…ç½®ï¼‰")
            success = train(model_name, base_config)
            if success:
                logger.info(f"\nğŸ‰ æ¨¡å‹ {model_name} è®­ç»ƒæˆåŠŸï¼")
                verify_finetuned_model(base_config["output_dir"])
                return True
        
        # åç»­å°è¯•ä½¿ç”¨é™çº§é…ç½®
        else:
            adjusted_config = {}
            for key, ratio in FALLBACK_ADJUSTMENTS.items():
                if key in base_config:
                    adjusted_config[key] = int(base_config[key] * ratio) if isinstance(base_config[key], int) else base_config[key] * ratio
            
            fallback_config = {**base_config,** adjusted_config}
            logger.info(f"ğŸ“Œ ç¬¬{i+1}æ¬¡å°è¯•ï¼šä½¿ç”¨æ¨¡å‹ {model_name}ï¼ˆé™çº§é…ç½®ï¼š{adjusted_config}ï¼‰")
            success = train(model_name, fallback_config)
            if success:
                logger.info(f"\nğŸ‰ æ¨¡å‹ {model_name} è®­ç»ƒæˆåŠŸï¼")
                verify_finetuned_model(fallback_config["output_dir"])
                return True
    
    logger.error(f"\nâŒâŒ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥é…ç½®å’Œç¯å¢ƒ")
    return False
# ===================== æ¨¡å‹éªŒè¯å’Œé›†æˆéªŒè¯ =====================
def verify_finetuned_model(model_dir: str):
    logger.info(f"\n" + "="*60)
    logger.info(f"ğŸ” éªŒè¯å¾®è°ƒæ¨¡å‹ï¼š{model_dir}")
    logger.info("="*60)
    try:
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        model.eval()
        test_prompts = [
            "è¯·è§£è¯»ä»¥ä¸‹æ ¸å¿ƒè®°å¿†çš„æ ¸å¿ƒä»·å€¼ï¼šè®°å¿†å†…å®¹ï¼šç³»ç»ŸåŸºç¡€åŠŸèƒ½éªŒè¯ï¼šæ·»åŠ æ ¸å¿ƒè®°å¿†",
            "å¦‚ä½•æå‡å…ƒå—æ•´åˆåº¦çš„BE Tokenè¿›åº¦ï¼Ÿ"
        ]
        for prompt in test_prompts:
            logger.info(f"\nğŸ“¥ æµ‹è¯•è¾“å…¥ï¼š{prompt[:50]}...")
            input_text = f"ç”¨æˆ·ï¼š{prompt}\nAIï¼š"
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=150,
                    temperature=0.7,
                    top_p=0.9,
                    repetition_penalty=1.1,
                    eos_token_id=tokenizer.eos_token_id
                )
            response = tokenizer.decode(outputs[0], skip_special_tokens=True).split("AIï¼š")[-1].strip()
            logger.info(f"ğŸ“¤ æ¨¡å‹è¾“å‡ºï¼š{response[:100]}...")
        logger.info(f"\nâœ… å¾®è°ƒæ¨¡å‹éªŒè¯é€šè¿‡ï¼")
    except Exception as e:
        logger.error(f"âŒ éªŒè¯å¾®è°ƒæ¨¡å‹å¤±è´¥ï¼š{e}", exc_info=True)

def integrate_with_memex():
    logger.info(f"\n" + "="*80)
    logger.info(f"ğŸ”— éªŒè¯å¾®è°ƒæ¨¡å‹ä¸Memex-Aé›†æˆ")
    logger.info("="*80)
    try:
        config = Config.from_json()
        memex = MemexA(config=config)
        model_dir = BASE_CONFIG["output_dir"]
        tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            trust_remote_code=True,
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            device_map="auto"
        )
        model.eval()
        core_memories = memex.search_memory(level="æ ¸å¿ƒ")
        if core_memories:
            mem = core_memories[0]
            mid = mem["è®°å¿†ID"]
            full_content = memex.get_full_content(mid)
            prompt = f"ä½œä¸ºMemex-Açš„è®°å¿†è¾…åŠ©AIï¼Œåˆ†æä»¥ä¸‹æ ¸å¿ƒè®°å¿†çš„AC-100ç»´åº¦è´¡çŒ®ï¼Œå¹¶æä¾›ä¼˜åŒ–å»ºè®®ï¼š\n\nè®°å¿†IDï¼š{mid}\nè®°å¿†å†…å®¹ï¼š{full_content[:150]}..."
            input_text = f"ç”¨æˆ·ï¼š{prompt}\nAIï¼š"
            inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
            with torch.no_grad():
                outputs = model.generate(
                    **inputs,
                    max_new_tokens=200,
                    temperature=0.6,
                    top_p=0.85,
                    repetition_penalty=1.1
                )
            response = tokenizer.decode(outputs[0], skip_special_tokens=True).split("AIï¼š")[-1].strip()
            logger.info(f"\nğŸ“‹ Memex-Aæ ¸å¿ƒè®°å¿†åˆ†æç»“æœï¼š")
            logger.info(f"è®°å¿†IDï¼š{mid}")
            logger.info(f"åˆ†æç»“è®ºï¼š{response}")
        logger.info(f"\nâœ… æ¨¡å‹ä¸Memex-Aé›†æˆéªŒè¯é€šè¿‡ï¼")
        return True
    except Exception as e:
        logger.error(f"âŒ æ¨¡å‹ä¸Memex-Aé›†æˆå¤±è´¥ï¼š{e}", exc_info=True)
        return False

# ===================== ä¸»å‡½æ•° =====================
if __name__ == "__main__":
    print("ğŸ”¥ Memex-A é€šç”¨å¾®è°ƒè„šæœ¬ï¼ˆæ”¯æŒä»»æ„æ¨¡å‹ï¼‰")
    print("="*60)
    
    # å…è®¸é€šè¿‡å‘½ä»¤è¡Œå‚æ•°æŒ‡å®šæ¨¡å‹åˆ—è¡¨
    if len(sys.argv) > 1:
        model_candidates = sys.argv[1:]
    else:
        # é»˜è®¤æ¨¡å‹å€™é€‰åˆ—è¡¨
        model_candidates = [
            "Qwen/Qwen2.5-0.5B",
            "mistralai/Mistral-7B-v0.1",
            "meta-llama/Llama-2-7b-hf",
            "baichuan-inc/Baichuan2-7B-Base"
        ]
    
    print(f"ğŸ“‹ å‡†å¤‡å°è¯•çš„æ¨¡å‹åˆ—è¡¨ï¼š{model_candidates}")
    
    raw_data = load_finetune_data()
    if len(raw_data) < 10:
        print(f"âŒ è®­ç»ƒæ•°æ®ä¸è¶³ï¼ˆä»…{len(raw_data)}æ¡ï¼‰ï¼Œé€€å‡ºè®­ç»ƒï¼ˆéœ€è‡³å°‘10æ¡æœ‰æ•ˆæ ·æœ¬ï¼‰")
        exit(1)
    
    # æ‰§è¡Œå¤šæ¨¡å‹è®­ç»ƒå°è¯•
    train_success = train_with_fallback(model_candidates)
    if not train_success:
        print(f"âŒ æ‰€æœ‰æ¨¡å‹è®­ç»ƒå°è¯•å‡å¤±è´¥ï¼Œè¯·æ£€æŸ¥ç¯å¢ƒå’Œé…ç½®")
        exit(1)
    
    # è®­ç»ƒæˆåŠŸåï¼ŒéªŒè¯ä¸Memex-Açš„é›†æˆèƒ½åŠ›
    integrate_success = integrate_with_memex()
    if not integrate_success:
        print(f"âš ï¸ æ¨¡å‹è®­ç»ƒæˆåŠŸï¼Œä½†ä¸Memex-Aé›†æˆéªŒè¯å¤±è´¥ï¼Œå¯æ‰‹åŠ¨æµ‹è¯•é›†æˆé€»è¾‘ï¼ˆè·¯å¾„ï¼š{BASE_CONFIG['output_dir']}ï¼‰")
        exit(1)
    
    # å…¨æµç¨‹å®Œæˆæç¤º
    print("\n" + "="*80)
    print("ğŸ‰ å¾®è°ƒå…¨æµç¨‹å®Œæˆï¼æ¨¡å‹å·²å°±ç»ªå¹¶é›†æˆåˆ°Memex-Aç³»ç»Ÿ")
    print(f"ğŸ“ å¾®è°ƒæ¨¡å‹è·¯å¾„ï¼š{BASE_CONFIG['output_dir']}")
    print(f"ğŸ“Š è®­ç»ƒæ•°æ®é‡ï¼š{len(raw_data)}æ¡ï¼ˆå«ç³»ç»Ÿæç¤ºè¯+4å±‚çº§è®°å¿†+BE Tokenåˆ†æï¼‰")
    print(f"âœ… åŠŸèƒ½éªŒè¯ï¼šæ¨¡å‹è®­ç»ƒâ†’æ•ˆæœéªŒè¯â†’Memex-Aé›†æˆ å…¨é€šè¿‡")
    print("ğŸ’¡ ä½¿ç”¨æ–¹å¼ï¼šåœ¨main.pyä¸­è°ƒç”¨MemexA.auto_finetune()å¯è‡ªåŠ¨è§¦å‘å¾®è°ƒ")
    print("="*80)