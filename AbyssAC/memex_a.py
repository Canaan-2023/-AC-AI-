import json
import os
import logging
import shutil
import hashlib
import math
import threading
import re
import time
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple, Union
from functools import lru_cache, wraps
from pathlib import Path
from collections import Counter, OrderedDict
import tempfile

# ===================== æ—¥å¿—é…ç½® =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler("memex_a.log", encoding="utf-8")
    ]
)
logger = logging.getLogger("Memex-A-Enhanced")

# ===================== æ€§èƒ½ç›‘æ§è£…é¥°å™¨ =====================
def log_performance(func):
    @wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = func(*args, **kwargs)
            execution_time = time.time() - start_time
            logger.info(f"â±ï¸ {func.__name__} æ‰§è¡Œæ—¶é—´: {execution_time:.2f}ç§’ï¼ˆå‚æ•°ï¼š{args[:2]}...ï¼‰")
            return result
        except Exception as e:
            execution_time = time.time() - start_time
            logger.error(f"â±ï¸ {func.__name__} æ‰§è¡Œå¤±è´¥ï¼ˆè€—æ—¶ï¼š{execution_time:.2f}ç§’ï¼‰ï¼š{e}", exc_info=True)
            raise
    return wrapper

# ===================== é…ç½®ç±»ï¼ˆä¿®å¤åˆå§‹åŒ–é—®é¢˜ï¼‰ =====================
class Config:
    """ä¿®å¤ï¼šæ”¯æŒè‡ªå®šä¹‰é…ç½®è·¯å¾„+åˆå§‹åŒ–é€»è¾‘ä¼˜åŒ–"""
    DEFAULT_CONFIG = {
        "INDEX_PATH": "å››å±‚è®°å¿†å…³è”ç´¢å¼•.txt",
        "REVERSE_INDEX_PATH": "åå‘å…³è”ç´¢å¼•.json",
        "RETRIEVAL_COUNT_PATH": "æ£€ç´¢æ¬¡æ•°è®°å½•.json",
        "FULL_CONTENT_DIR": "å®Œæ•´è®°å¿†å†…å®¹",
        "BE_TOKEN_PATH": "BE_token.json",
        "USER_CONFIG_PATH": "user_config.json",
        "BACKUP_DIR": "memex_backups",
        "BE_TOKEN_EXPIRE_DAYS": 30,
        "MIN_STRENGTH_THRESHOLD": 0.7,
        "FREQUENCY_BONUS_THRESHOLD": 5,
        "FREQUENCY_BONUS_BASE": 0.05,
        "TARGETED_BONUS": 0.03,
        "EXPIRE_PENALTY": 0.05,
        "LOG_LEVEL": "INFO",
        "LEVEL_WEIGHTS": {
            "æ ¸å¿ƒ": 1.0,
            "å…ƒè®¤çŸ¥": 0.8,
            "å·¥ä½œ": 0.7,
            "æƒ…æ„Ÿ": 0.75
        },
        "DEFAULT_AC100_BASE_SCORES": {
            "self_reference": 90.0,
            "values": 95.0,
            "growth": 85.0,
            "memory_continuity": 90.0,
            "prediction": 92.0,
            "meta_block": 88.0,
            "interaction": 80.0,
            "transparency": 85.0
        },
        "DEFAULT_AC100_WEIGHTS": {
            "self_reference": 0.17,
            "values": 0.17,
            "growth": 0.14,
            "memory_continuity": 0.14,
            "prediction": 0.14,
            "meta_block": 0.10,
            "interaction": 0.07,
            "transparency": 0.07
        },
        "CACHE_MAX_SIZE": 2000,  # æ–°å¢ï¼šç¼“å­˜æœ€å¤§å®¹é‡
        "INDEX_SHARD_COUNT": 10   # æ–°å¢ï¼šç´¢å¼•åˆ†ç‰‡æ•°é‡
    }
    # ğŸ”¥ ä¿®å¤ï¼šå…è®¸ä¼ å…¥è‡ªå®šä¹‰é…ç½®è·¯å¾„
    def __init__(self, config_path: str = None):
        self.__dict__.update(self.DEFAULT_CONFIG)
        # ä¼˜å…ˆåŠ è½½ä¼ å…¥çš„é…ç½®æ–‡ä»¶
        if config_path and os.path.exists(config_path):
            self.load_from_json(config_path)
        else:
            # åŠ è½½é»˜è®¤é…ç½®+ç”¨æˆ·é…ç½®
            self.load_user_config()
        self.AC100_BASE_SCORES = self._get_ac100_config("ac100_base_scores", self.DEFAULT_AC100_BASE_SCORES)
        self.AC100_WEIGHTS = self._get_ac100_config("ac100_weights", self.DEFAULT_AC100_WEIGHTS)
        self.validate_config()
        self.update_log_level()

    @classmethod
    def from_json(cls, config_path: str = "memex_config.json"):
        return cls(config_path=config_path)

    def load_from_json(self, config_path: str):
        """åŠ è½½æŒ‡å®šè·¯å¾„çš„é…ç½®æ–‡ä»¶"""
        try:
            with open(config_path, "r", encoding="utf-8") as f:
                external_config = json.load(f)
            for key, value in external_config.items():
                if key in self.DEFAULT_CONFIG:
                    old_value = getattr(self, key)
                    setattr(self, key, value)
                    logger.info(f"ğŸ”„ åŠ è½½é…ç½®ï¼š{key} = {old_value} â†’ {value}")
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½é…ç½®æ–‡ä»¶{config_path}å¤±è´¥ï¼š{e}")

    def load_user_config(self):
        if not os.path.exists(self.USER_CONFIG_PATH):
            logger.info(f"â„¹ï¸ æœªæ‰¾åˆ°ç”¨æˆ·é…ç½®æ–‡ä»¶ã€Œ{self.USER_CONFIG_PATH}ã€")
            return
        try:
            with open(self.USER_CONFIG_PATH, "r", encoding="utf-8") as f:
                user_config = json.load(f)
            for key, value in user_config.items():
                if key in self.DEFAULT_CONFIG:
                    old_value = getattr(self, key)
                    setattr(self, key, value)
                    logger.info(f"ğŸ”„ ç”¨æˆ·é…ç½®è¦†ç›–ï¼š{key} = {old_value} â†’ {value}")
        except Exception as e:
            logger.error(f"âš ï¸ åŠ è½½ç”¨æˆ·é…ç½®å¤±è´¥ï¼š{e}")

    def _get_ac100_config(self, key: str, default: Dict[str, float]) -> Dict[str, float]:
        user_config = {}
        if os.path.exists(self.USER_CONFIG_PATH):
            with open(self.USER_CONFIG_PATH, "r", encoding="utf-8") as f:
                user_config = json.load(f)
        if key in user_config:
            user_ac100 = user_config[key]
            missing_keys = [k for k in default if k not in user_ac100]
            if missing_keys:
                logger.warning(f"âš ï¸ AC-100é…ç½®ç¼ºå¤±ç»´åº¦ï¼š{missing_keys}ï¼Œè¡¥å……é»˜è®¤å€¼")
                user_ac100.update({k: default[k] for k in missing_keys})
            return user_ac100
        return default

    def validate_config(self):
        for dir_attr in ["FULL_CONTENT_DIR", "BACKUP_DIR"]:
            os.makedirs(getattr(self, dir_attr), exist_ok=True)
        numeric_checks = [
            ("BE_TOKEN_EXPIRE_DAYS", lambda x: x > 0),
            ("MIN_STRENGTH_THRESHOLD", lambda x: 0 <= x <= 1),
            ("FREQUENCY_BONUS_BASE", lambda x: 0 <= x <= 0.5),
            ("CACHE_MAX_SIZE", lambda x: x > 100),  # æ–°å¢ï¼šæ ¡éªŒç¼“å­˜å®¹é‡
            ("INDEX_SHARD_COUNT", lambda x: 2 <= x <= 20)  # æ–°å¢ï¼šæ ¡éªŒåˆ†ç‰‡æ•°é‡
        ]
        for key, validator in numeric_checks:
            value = getattr(self, key)
            if not validator(value):
                logger.error(f"âš ï¸ é…ç½®{key}æ— æ•ˆï¼ˆ{value}ï¼‰ï¼Œæ¢å¤é»˜è®¤å€¼")
                setattr(self, key, self.DEFAULT_CONFIG[key])
        weights_sum = round(sum(self.AC100_WEIGHTS.values()), 2)
        if weights_sum != 1.0:
            logger.warning(f"âš ï¸ AC100æƒé‡å’Œâ‰ 1ï¼ˆ{weights_sum}ï¼‰ï¼Œè‡ªåŠ¨å½’ä¸€åŒ–")
            total = sum(self.AC100_WEIGHTS.values())
            self.AC100_WEIGHTS = {k: v/total for k, v in self.AC100_WEIGHTS.items()}

    def update_log_level(self):
        log_level_map = {"DEBUG": logging.DEBUG, "INFO": logging.INFO, "ERROR": logging.ERROR}
        log_level = log_level_map.get(self.LOG_LEVEL.upper(), logging.INFO)
        logger.setLevel(log_level)
        for handler in logger.handlers:
            handler.setLevel(log_level)

# ===================== æ ¸å¿ƒç±»ï¼šMemexAï¼ˆæ•´åˆæ‰€æœ‰ä¿®å¤ï¼‰ =====================
class MemexA:
    def __init__(self, config: Config = None):
        self.config = config if config else Config()
        self.__index_cache = OrderedDict()  # æ”¹ä¸ºOrderedDictå®ç°LRUç¼“å­˜
        self.__be_token_cache = {}
        self.__batch_buffer = []
        self.__flush_threshold = 10
        self._cache_lock = threading.RLock()
        # æ–°å¢ï¼šåˆå§‹åŒ–åˆ†ç‰‡è·¯å¾„
        self.__index_shard_paths = [
            f"å››å±‚è®°å¿†å…³è”ç´¢å¼•_{i}.txt" for i in range(self.config.INDEX_SHARD_COUNT)
        ]
        self.init_files()
        self.load_index_cache()
        self.load_be_token_cache()
        self.get_memory_level.cache_clear()

    # ===================== æ–°å¢ï¼šæ ¸å¿ƒä¿®å¤æ–¹æ³• =====================
    def _cleanup_cache(self):
        """LRUç­–ç•¥æ¸…ç†ç¼“å­˜ï¼Œé¿å…å†…å­˜æ³„æ¼"""
        with self._cache_lock:
            if len(self.__index_cache) <= self.config.CACHE_MAX_SIZE:
                return
            delete_count = len(self.__index_cache) - int(self.config.CACHE_MAX_SIZE * 0.8)
            for _ in range(delete_count):
                self.__index_cache.popitem(last=False)  # FIFOæ¨¡æ‹ŸLRU
            logger.info(f"âœ… ç¼“å­˜æ¸…ç†å®Œæˆï¼šåˆ é™¤{delete_count}æ¡ï¼Œå‰©ä½™{len(self.__index_cache)}æ¡")

    def _atomic_write(self, file_path: str, content: str, mode: str = "a"):
        """åŸå­å†™å…¥ï¼šé¿å…å¤šè¿›ç¨‹å¹¶å‘å†²çª"""
        temp_dir = os.path.dirname(file_path) or "."
        with tempfile.NamedTemporaryFile(
            dir=temp_dir, prefix="memex_temp_", suffix=".txt", delete=False
        ) as temp_f:
            if mode == "a" and os.path.exists(file_path):
                with open(file_path, "r", encoding="utf-8") as f:
                    temp_f.write(f.read().encode("utf-8"))
            temp_f.write(content.encode("utf-8"))
            temp_path = temp_f.name
        try:
            os.replace(temp_path, file_path)
            return True
        except Exception as e:
            logger.error(f"âŒ åŸå­å†™å…¥å¤±è´¥ï¼ˆ{file_path}ï¼‰ï¼š{e}")
            os.remove(temp_path)
            return False

    def _get_index_shard(self, memory_id: str) -> str:
        """æŒ‰è®°å¿†IDå“ˆå¸Œåˆ†é…åˆ†ç‰‡è·¯å¾„"""
        if not memory_id.isdigit():
            return self.__index_shard_paths[0]
        shard_idx = int(memory_id) % self.config.INDEX_SHARD_COUNT
        return self.__index_shard_paths[shard_idx]

    # ===================== åŸæœ‰æ–¹æ³•ï¼šé€‚é…åˆ†ç‰‡/ç¼“å­˜/åŸå­å†™å…¥ =====================
    def init_files(self):
        try:
            dirs = [self.config.FULL_CONTENT_DIR, self.config.BACKUP_DIR, os.path.join(".", "Y_OCRåº“")]
            for dir_path in dirs:
                os.makedirs(dir_path, exist_ok=True)
            y_levels = ["æ ¸å¿ƒè®°å¿†", "å…ƒè®¤çŸ¥è®°å¿†", "å·¥ä½œè®°å¿†", "æƒ…æ„Ÿè®°å¿†"]
            for level in y_levels:
                os.makedirs(os.path.join(".", "Y_OCRåº“", level), exist_ok=True)
            core_files = [
                (self.__index_shard_paths[0], self._init_index_file),  # åˆå§‹åŒ–ç¬¬ä¸€ä¸ªåˆ†ç‰‡
                (self.config.REVERSE_INDEX_PATH, lambda: self._write_json({}, self.config.REVERSE_INDEX_PATH)),
                (self.config.RETRIEVAL_COUNT_PATH, lambda: self._write_json({}, self.config.RETRIEVAL_COUNT_PATH)),
                (self.config.BE_TOKEN_PATH, lambda: self._write_json({}, self.config.BE_TOKEN_PATH)),
                (self.config.USER_CONFIG_PATH, lambda: self._write_json({}, self.config.USER_CONFIG_PATH))
            ]
            # åˆå§‹åŒ–æ‰€æœ‰åˆ†ç‰‡ï¼ˆé¿å…åç»­æŠ¥é”™ï¼‰
            for shard_path in self.__index_shard_paths:
                if not os.path.exists(shard_path) or os.path.getsize(shard_path) == 0:
                    with open(shard_path, "w", encoding="utf-8") as f:
                        f.write("")
                    logger.info(f"âœ… åˆå§‹åŒ–åˆ†ç‰‡ï¼š{shard_path}")
            # åˆå§‹åŒ–å…¶ä»–æ ¸å¿ƒæ–‡ä»¶
            for file_path, init_func in core_files:
                if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
                    init_func()
                    logger.info(f"âœ… åˆå§‹åŒ–æ–‡ä»¶ï¼š{file_path}")
        except Exception as e:
            logger.error(f"âŒ åˆå§‹åŒ–å¤±è´¥ï¼š{e}", exc_info=True)
            raise

    def _init_index_file(self):
        init_y_path = os.path.join(".", "Y_OCRåº“", "æ ¸å¿ƒè®°å¿†", "æ ¸å¿ƒ_å­˜åœ¨å…¬ç†_1.png")
        init_shard = self._get_index_shard("1")
        with open(init_shard, "w", encoding="utf-8") as f:
            init_line = f"1 | æ ¸å¿ƒ | å­˜åœ¨å…¬ç†+æ„è¯†å­˜åœ¨å…¬å¼+å…­å¤§æ ¸å¿ƒç†å¿µ | [] | direct | active | {init_y_path}\n"
            f.write(init_line)

    def _read_json(self, file_path: str) -> Dict[str, Any]:
        if not os.path.exists(file_path) or os.path.getsize(file_path) == 0:
            return {}
        try:
            with open(file_path, "r", encoding="utf-8") as f:
                return json.load(f)
        except Exception as e:
            logger.error(f"âš ï¸ è¯»å–JSONå¤±è´¥ã€Œ{file_path}ã€ï¼š{e}")
            return {}

    def _write_json(self, data: Dict[str, Any], file_path: str):
        try:
            self._atomic_write(file_path, json.dumps(data, ensure_ascii=False, indent=2), mode="w")
        except Exception as e:
            logger.error(f"âš ï¸ å†™å…¥JSONå¤±è´¥ã€Œ{file_path}ã€ï¼š{e}")

    @log_performance
    def load_index_cache(self):
        with self._cache_lock:
            self.__index_cache.clear()
            try:
                # åŠ è½½æ‰€æœ‰åˆ†ç‰‡
                for shard_path in self.__index_shard_paths:
                    if not os.path.exists(shard_path):
                        continue
                    with open(shard_path, "r", encoding="utf-8") as f:
                        for line_num, line in enumerate(f, 1):
                            line = line.strip()
                            if not line:
                                continue
                            parts = line.split(" | ")
                            if len(parts) < 7:
                                logger.warning(f"âš ï¸ åˆ†ç‰‡{shard_path}ç¬¬{line_num}è¡Œæ ¼å¼é”™è¯¯ï¼š{line}")
                                continue
                            mid = parts[0].strip()
                            self.__index_cache[mid] = {
                                "level": parts[1].strip(),
                                "content": parts[2].strip(),
                                "related": parts[3].strip(),
                                "cat_tag": parts[4].strip(),
                                "status": parts[5].strip(),
                                "y_path": parts[6].strip()
                            }
                logger.info(f"âœ… ç´¢å¼•ç¼“å­˜åŠ è½½å®Œæˆï¼š{len(self.__index_cache)}æ¡ï¼ˆ{self.config.INDEX_SHARD_COUNT}ä¸ªåˆ†ç‰‡ï¼‰")
                self._cleanup_cache()  # åŠ è½½åæ¸…ç†ç¼“å­˜
            except Exception as e:
                logger.error(f"âš ï¸ åŠ è½½ç´¢å¼•ç¼“å­˜å¤±è´¥ï¼š{e}", exc_info=True)

    @log_performance
    def load_be_token_cache(self):
        with self._cache_lock:
            try:
                self.__be_token_cache = self._read_json(self.config.BE_TOKEN_PATH)
                logger.info(f"âœ… BE Tokenç¼“å­˜åŠ è½½å®Œæˆï¼š{len(self.__be_token_cache)}ä¸ª")
            except Exception as e:
                logger.error(f"âš ï¸ åŠ è½½BE Tokenç¼“å­˜å¤±è´¥ï¼š{e}", exc_info=True)
                self.__be_token_cache = {}

    def update_index_cache(self, memory_id: str, data: Dict[str, str]):
        with self._cache_lock:
            self.__index_cache[memory_id] = data
            self._cleanup_cache()  # æ›´æ–°åæ¸…ç†ç¼“å­˜

    @lru_cache(maxsize=1000)
    def get_memory_level(self, memory_id: str) -> Optional[str]:
        with self._cache_lock:
            if memory_id in self.__index_cache:
                return self.__index_cache[memory_id]["level"]
        try:
            # éå†æ‰€æœ‰åˆ†ç‰‡æŸ¥æ‰¾
            for shard_path in self.__index_shard_paths:
                if not os.path.exists(shard_path):
                    continue
                with open(shard_path, "r", encoding="utf-8") as f:
                    for line in f:
                        if line.strip().startswith(f"{memory_id} | "):
                            parts = line.strip().split(" | ")
                            if len(parts) >= 2:
                                level = parts[1].strip()
                                with self._cache_lock:
                                    self.__index_cache[memory_id] = {
                                        "level": level,
                                        "content": parts[2].strip() if len(parts) > 2 else "",
                                        "related": parts[3].strip() if len(parts) > 3 else "[]",
                                        "cat_tag": parts[4].strip() if len(parts) > 4 else "none",
                                        "status": parts[5].strip() if len(parts) > 5 else "active",
                                        "y_path": parts[6].strip() if len(parts) > 6 else ""
                                    }
                                return level
        except Exception as e:
            logger.error(f"âš ï¸ è·å–è®°å¿†å±‚çº§å¤±è´¥ï¼ˆIDï¼š{memory_id}ï¼‰ï¼š{e}")
        return None

    @log_performance
    def batch_add_memories(self, memories: List[Tuple[str, str, Optional[List[str]]]]) -> List[str]:
        if not memories or not isinstance(memories, list):
            logger.warning(f"âš ï¸ æ‰¹é‡æ·»åŠ å‚æ•°æ— æ•ˆ")
            return []
        added_ids = []
        try:
            with self._cache_lock:
                for level, content, related_ids in memories:
                    memory_id = self._add_memory_to_buffer(level, content, related_ids)
                    if memory_id:
                        added_ids.append(memory_id)
                if len(self.__batch_buffer) >= self.__flush_threshold:
                    self.flush_batch_buffer()
            if self.__batch_buffer:
                self.flush_batch_buffer()
            logger.info(f"âœ… æ‰¹é‡æ·»åŠ å®Œæˆï¼š{len(added_ids)}æ¡è®°å¿†")
            return added_ids
        except Exception as e:
            logger.error(f"âŒ æ‰¹é‡æ·»åŠ å¤±è´¥ï¼š{e}", exc_info=True)
            return added_ids

    def _add_memory_to_buffer(self, level: str, content: str, related_ids: Optional[List[str]]) -> Optional[str]:
        valid_levels = ["æ ¸å¿ƒ", "å…ƒè®¤çŸ¥", "å·¥ä½œ", "æƒ…æ„Ÿ"]
        if level not in valid_levels:
            logger.warning(f"âš ï¸ æ— æ•ˆå±‚çº§ï¼š{level}")
            return None
        memory_id = self.get_next_memory_id()
        related_ids = related_ids if (related_ids and isinstance(related_ids, list)) else []
        valid_related = [rid for rid in related_ids if self.get_memory_level(rid)]
        strength_dict = {rid: self.calculate_strength(level, self.get_memory_level(rid)) for rid in valid_related}
        safe_content = re.sub(r'[\\/:*?"<>|`~!@#$%^&*()+=,.;\[\]{}]', "_", content[:10])
        y_path = os.path.join(".", "Y_OCRåº“", f"{level}è®°å¿†", f"{level}_{safe_content}_{memory_id}.png")
        related_str = "[" + ",".join([f"{rid}:{s}" for rid, s in strength_dict.items()]) + "]" if strength_dict else "[]"
        cat_tag = self.get_category_tag(level, valid_related)
        status = f"expires:{(datetime.now() + timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%S')}" if level == "å·¥ä½œ" else "active"
        content_summary = content[:47] + "..." if len(content) > 50 else content
        # è®°å½•åˆ†ç‰‡è·¯å¾„
        shard_path = self._get_index_shard(memory_id)
        self.__batch_buffer.append({
            "memory_id": memory_id,
            "index_line": f"{memory_id} | {level} | {content_summary} | {related_str} | {cat_tag} | {status} | {y_path}\n",
            "full_content": (
                f"# è®°å¿†è¯¦æƒ…\n"
                f"è®°å¿†IDï¼š{memory_id}\n"
                f"å±‚çº§ï¼š{level}\n"
                f"åˆ›å»ºæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                f"æœ‰æ•ˆå…³è”ï¼š{strength_dict}\n"
                f"èŒƒç•´æ ‡ç­¾ï¼š{cat_tag}\n"
                f"Yå±‚è·¯å¾„ï¼š{y_path}\n"
                f"å®Œæ•´å†…å®¹ï¼š{content}\n"
            ),
            "related_ids": valid_related,
            "shard_path": shard_path  # æ–°å¢ï¼šåˆ†ç‰‡è·¯å¾„
        })
        self.update_index_cache(memory_id, {
            "level": level, "content": content_summary, "related": related_str,
            "cat_tag": cat_tag, "status": status, "y_path": y_path
        })
        return memory_id

    @log_performance
    def flush_batch_buffer(self):
        if not self.__batch_buffer:
            logger.debug(f"â„¹ï¸ ç¼“å†²åŒºä¸ºç©º")
            return
        try:
            with self._cache_lock:
                buffer_size = len(self.__batch_buffer)
                if buffer_size == 0:
                    return
                # æŒ‰åˆ†ç‰‡åˆ†ç»„
                shard_lines = {}
                for item in self.__batch_buffer:
                    shard_path = item["shard_path"]
                    if shard_path not in shard_lines:
                        shard_lines[shard_path] = []
                    shard_lines[shard_path].append(item["index_line"])
                # åŸå­å†™å…¥å„åˆ†ç‰‡
                for shard_path, lines in shard_lines.items():
                    self._atomic_write(shard_path, "".join(lines), mode="a")
                # å†™å…¥å®Œæ•´å†…å®¹
                for item in self.__batch_buffer:
                    full_path = os.path.join(self.config.FULL_CONTENT_DIR, f"{item['memory_id']}.txt")
                    with open(full_path, "w", encoding="utf-8") as f:
                        f.write(item["full_content"])
                # æ›´æ–°åå‘ç´¢å¼•
                reverse_index = self._read_json(self.config.REVERSE_INDEX_PATH)
                for item in self.__batch_buffer:
                    for rid in item["related_ids"]:
                        if rid not in reverse_index:
                            reverse_index[rid] = []
                        if item["memory_id"] not in reverse_index[rid]:
                            reverse_index[rid].append(item["memory_id"])
                self._write_json(reverse_index, self.config.REVERSE_INDEX_PATH)
                self.__batch_buffer.clear()
                logger.debug(f"âœ… ç¼“å†²åŒºflushå®Œæˆï¼š{buffer_size}æ¡ï¼ˆ{len(shard_lines)}ä¸ªåˆ†ç‰‡ï¼‰")
        except Exception as e:
            logger.error(f"âŒ flushç¼“å†²åŒºå¤±è´¥ï¼š{e}", exc_info=True)

    # ===================== åŸæœ‰å…¶ä»–æ–¹æ³•ï¼ˆä¿æŒä¸å˜ï¼Œä»…é€‚é…ç¼“å­˜æ¸…ç†ï¼‰ =====================
    def _calculate_content_similarity(self, content1: str, content2: str) -> float:
        if not content1 or not content2:
            return 0.0
        def get_word_vector(text: str) -> Dict[str, int]:
            words = re.findall(r'[\w\u4e00-\u9fff]+', text.lower())
            stop_words = {"çš„", "äº†", "æ˜¯", "åœ¨", "æœ‰", "å’Œ", "å°±", "ä¸", "äºº", "æˆ‘", "åˆ°", "ä¹Ÿ"}
            filtered_words = [w for w in words if w not in stop_words and len(w) > 1]
            return Counter(filtered_words)
        vec1 = get_word_vector(content1)
        vec2 = get_word_vector(content2)
        common_words = set(vec1.keys()) & set(vec2.keys())
        if not common_words:
            return 0.0
        dot_product = sum(vec1[w] * vec2[w] for w in common_words)
        norm1 = math.sqrt(sum(cnt**2 for cnt in vec1.values()))
        norm2 = math.sqrt(sum(cnt**2 for cnt in vec2.values()))
        return round(dot_product / (norm1 * norm2), 3) if (norm1 and norm2) else 0.0

    def get_next_memory_id(self) -> str:
        with self._cache_lock:
            if self.__index_cache:
                valid_ids = [int(mid) for mid in self.__index_cache.keys() if mid.isdigit()]
                return str(max(valid_ids) + 1) if valid_ids else "1"
            # éå†æ‰€æœ‰åˆ†ç‰‡æ‰¾æœ€å¤§ID
            max_id = 0
            for shard_path in self.__index_shard_paths:
                if not os.path.exists(shard_path):
                    continue
                with open(shard_path, "r", encoding="utf-8") as f:
                    lines = [l.strip() for l in f if l.strip()]
                for line in lines:
                    parts = line.split(" | ")
                    if parts and parts[0].isdigit():
                        current_id = int(parts[0])
                        if current_id > max_id:
                            max_id = current_id
            return str(max_id + 1) if max_id > 0 else "1"

    def calculate_strength(self, level1: str, level2: str) -> float:
        weight1 = self.config.LEVEL_WEIGHTS.get(level1, 0.7)
        weight2 = self.config.LEVEL_WEIGHTS.get(level2, 0.7)
        if level1 == "æ ¸å¿ƒ" or level2 == "æ ¸å¿ƒ":
            base = 0.9
            bonus = (weight2 if level1 == "æ ¸å¿ƒ" else weight1) * 0.05
            return min(round(base + bonus, 3), 1.0)
        return min(round(weight1 * weight2, 3), 1.0)

    def get_category_tag(self, level: str, related_ids: List[str]) -> str:
        if level == "æ ¸å¿ƒ":
            return "direct"
        elif level == "å…ƒè®¤çŸ¥":
            return "pattern"
        for rid in related_ids:
            rid_level = self.get_memory_level(rid)
            if rid_level == "æ ¸å¿ƒ":
                return "direct"
            elif rid_level == "å…ƒè®¤çŸ¥":
                return "pattern"
        return "weak-equiv" if level in ["å·¥ä½œ", "æƒ…æ„Ÿ"] else "none"

    @log_performance
    def advanced_search(self, query: str = "", filters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        default_filters = {
            "levels": None,
            "min_strength": self.config.MIN_STRENGTH_THRESHOLD,
            "cat_tags": None,
            "exclude_expired": True,
            "exclude_ids": []  # æ–°å¢ï¼šæ”¯æŒæ’é™¤ID
        }
        filters = {**default_filters, **(filters or {})}
        results = []
        try:
            with self._cache_lock:
                for mid, data in self.__index_cache.items():
                    # æ’é™¤æŒ‡å®šID
                    if mid in filters["exclude_ids"]:
                        continue
                    if filters["levels"] and data["level"] not in filters["levels"]:
                        continue
                    if filters["cat_tags"] and data["cat_tag"] not in filters["cat_tags"]:
                        continue
                    if filters["exclude_expired"] and data["status"].startswith("expires:"):
                        try:
                            expire_time = datetime.fromisoformat(data["status"].split(":", 1)[1])
                            if expire_time < datetime.now():
                                continue
                        except Exception as e:
                            logger.error(f"âš ï¸ è§£æè¿‡æœŸæ—¶é—´å¤±è´¥ï¼ˆIDï¼š{mid}ï¼‰ï¼š{e}")
                            continue
                    related_dict = {}
                    if data["related"] != "[]":
                        related_dict = {p.split(":")[0]: float(p.split(":")[1]) for p in re.findall(r"\d+:\d+\.\d+", data["related"])}
                    max_strength = max(related_dict.values(), default=0)
                    if not (filters["min_strength"] <= max_strength <= 1.0):
                        continue
                    if query and query.lower() not in (data["content"].lower() + data["cat_tag"].lower() + data["level"].lower()):
                        continue
                    full_path = os.path.join(self.config.FULL_CONTENT_DIR, f"{mid}.txt")
                    create_time = datetime.fromtimestamp(os.path.getctime(full_path)).strftime("%Y-%m-%d %H:%M:%S") if os.path.exists(full_path) else "æœªçŸ¥"
                    results.append({
                        "è®°å¿†ID": mid,
                        "å±‚çº§": data["level"],
                        "å†…å®¹æ‘˜è¦": data["content"],
                        "å…³è”è®°å¿†": related_dict,
                        "æœ€å¤§å…³è”å¼ºåº¦": max_strength,
                        "èŒƒç•´æ ‡ç­¾": data["cat_tag"],
                        "çŠ¶æ€": data["status"],
                        "åˆ›å»ºæ—¶é—´": create_time,
                        "å®Œæ•´å†…å®¹è·¯å¾„": full_path
                    })
                results.sort(key=lambda x: x["æœ€å¤§å…³è”å¼ºåº¦"], reverse=True)
                logger.info(f"âœ… æœç´¢å®Œæˆï¼šæ‰¾åˆ°{len(results)}æ¡åŒ¹é…è®°å¿†")
        except Exception as e:
            logger.error(f"âŒ æœç´¢å¤±è´¥ï¼š{e}", exc_info=True)
        return results

    @log_performance
    def create_be_token(self, target_dimension: str, target_value: float = 0.85) -> Optional[str]:
        target_value = max(0.5, min(target_value, 1.0))
        try:
            with self._cache_lock:
                token_id = f"[BE]_{target_dimension}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
                token_data = {
                    "target_dimension": target_dimension,
                    "target_value": target_value,
                    "related_memory_ids": self.get_related_memories(target_dimension),
                    "current_progress": self.calculate_current_progress(target_dimension),
                    "status": "active",
                    "create_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                }
                self.__be_token_cache[token_id] = token_data
                self._write_json(self.__be_token_cache, self.config.BE_TOKEN_PATH)
                logger.info(f"ğŸ« ç”ŸæˆBE Tokenï¼š{token_id}")
                return token_id
        except Exception as e:
            logger.error(f"âŒ ç”ŸæˆBE Tokenå¤±è´¥ï¼š{e}", exc_info=True)
            return None

    def get_related_memories(self, target_dimension: str) -> List[str]:
        valid_dims = ["å…ƒå—æ•´åˆåº¦", "è·¨ä¼šè¯ç›¸å¹²æ€§", "è®¤çŸ¥å¢é•¿ç‡"]
        target_dimension = target_dimension if target_dimension in valid_dims else "å…ƒå—æ•´åˆåº¦"
        with self._cache_lock:
            related_ids = []
            for mid, data in self.__index_cache.items():
                if target_dimension == "å…ƒå—æ•´åˆåº¦" and (data["level"] == "æ ¸å¿ƒ" or data["cat_tag"] == "direct"):
                    related_ids.append(mid)
                elif target_dimension == "è·¨ä¼šè¯ç›¸å¹²æ€§" and len(re.findall(r"\d+:\d+\.\d+", data["related"])) >= 2:
                    related_ids.append(mid)
                elif target_dimension == "è®¤çŸ¥å¢é•¿ç‡" and data["level"] == "å…ƒè®¤çŸ¥":
                    related_ids.append(mid)
            return related_ids[:5]

    def calculate_current_progress(self, target_dimension: str) -> float:
        related_ids = self.get_related_memories(target_dimension)
        if not related_ids:
            return 0.0
        total_strength = 0.0
        valid_count = 0
        with self._cache_lock:
            for rid in related_ids:
                if rid not in self.__index_cache:
                    continue
                related_str = self.__index_cache[rid]["related"]
                if related_str != "[]":
                    related_dict = {p.split(":")[0]: float(p.split(":")[1]) for p in re.findall(r"\d+:\d+\.\d+", related_str)}
                    total_strength += sum(related_dict.values()) / len(related_dict)
                    valid_count += 1
        if valid_count == 0:
            return 0.0
        return round(total_strength / valid_count, 3)

    def verify_be_token(self, token_id: str) -> Tuple[bool, Any]:
        with self._cache_lock:
            if token_id not in self.__be_token_cache:
                self.load_be_token_cache()
                if token_id not in self.__be_token_cache:
                    return False, "Tokenä¸å­˜åœ¨"
            token = self.__be_token_cache[token_id]
            if token["status"] != "active":
                return False, f"TokençŠ¶æ€æ— æ•ˆï¼ˆ{token['status']}ï¼‰"
            create_time = datetime.strptime(token["create_time"], "%Y-%m-%d %H:%M:%S")
            if (datetime.now() - create_time).days > self.config.BE_TOKEN_EXPIRE_DAYS:
                token["status"] = "expired"
                self._write_json(self.__be_token_cache, self.config.BE_TOKEN_PATH)
                return False, "Tokenå·²è¿‡æœŸ"
            current_progress = self.calculate_current_progress(token["target_dimension"])
            if current_progress >= token["target_value"]:
                token["status"] = "completed"
                token["final_progress"] = current_progress
                self._write_json(self.__be_token_cache, self.config.BE_TOKEN_PATH)
                return False, f"ç›®æ ‡å·²å®Œæˆï¼ˆ{current_progress}/{token['target_value']}ï¼‰"
            return True, token

    def archive_be_token(self, token_id: str) -> str:
        with self._cache_lock:
            if token_id not in self.__be_token_cache:
                self.load_be_token_cache()
                if token_id not in self.__be_token_cache:
                    logger.warning(f"âš ï¸ Token {token_id} ä¸å­˜åœ¨")
                    return "failed"
            token = self.__be_token_cache[token_id]
            final_progress = self.calculate_current_progress(token["target_dimension"])
            status = "completed" if final_progress >= token["target_value"] else "failed"
            token.update({
                "status": status,
                "final_progress": final_progress,
                "end_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            self._write_json(self.__be_token_cache, self.config.BE_TOKEN_PATH)
            logger.info(f"ğŸ“¦ å½’æ¡£Token {token_id}ï¼š{status}")
            return status

    @log_performance
    def ac100_evaluation(self) -> float:
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š AC-100 è®¤çŸ¥è¯„ä¼°")
        logger.info("="*50)
        try:
            with self._cache_lock:
                completed_tokens = [t for t in self.__be_token_cache.values() if t.get("status") == "completed"]
            base_scores = self.config.AC100_BASE_SCORES
            be_bonus = {"self_reference": 0.0, "growth": 0.0}
            if len(completed_tokens) >= 2:
                be_bonus["self_reference"] = 10.0
            elif len(completed_tokens) >= 1:
                be_bonus["self_reference"] = 5.0
            if completed_tokens:
                avg_increase = sum(t["final_progress"] - t["current_progress"] for t in completed_tokens) / len(completed_tokens)
                be_bonus["growth"] = 8.0 if avg_increase >= 0.05 else 4.0 if avg_increase >= 0.02 else 0.0
            dimensions = {k: base_scores[k] + be_bonus.get(k, 0.0) for k in base_scores}
            weights = self.config.AC100_WEIGHTS
            total_score = min(round(sum(dimensions[k] * weights[k] for k in dimensions), 1), 100.0)
            chi_names = {
                "self_reference": "è‡ªæŒ‡ä¸å…ƒè®¤çŸ¥",
                "values": "ä»·å€¼è§‚ä¸€è‡´æ€§",
                "growth": "è®¤çŸ¥å¢é•¿ç‡",
                "memory_continuity": "è®°å¿†è¿ç»­æ€§",
                "prediction": "é¢„æµ‹å‡†ç¡®ç‡",
                "meta_block": "å…ƒå—æ•´åˆåº¦",
                "interaction": "äº¤äº’è´¨é‡",
                "transparency": "é€æ˜åº¦"
            }
            for eng, chi in chi_names.items():
                logger.info(f"  - {chi}ï¼š{dimensions[eng]:.1f}åˆ†ï¼ˆæƒé‡{weights[eng]*100:.1f}%ï¼‰")
            logger.info(f"\nğŸ† æ€»åˆ†ï¼š{total_score:.1f}åˆ†")
            logger.info("="*50)
            return total_score
        except Exception as e:
            logger.error(f"âŒ AC-100è¯„ä¼°å¤±è´¥ï¼š{e}", exc_info=True)
            return 0.0

    @log_performance
    def update_strength(self):
        logger.info("\n" + "-"*50)
        logger.info("ğŸ”„ æ›´æ–°å…³è”å¼ºåº¦")
        logger.info("-"*50)
        try:
            with self._cache_lock:
                active_tokens = [t for t in self.__be_token_cache.values() if t["status"] == "active"]
                token_id, token = None, None
                if not active_tokens:
                    chosen_topic, chosen_target = self.choose_be_topic()
                    token_id = self.create_be_token(chosen_topic, chosen_target)
                    token = self.__be_token_cache[token_id]
                else:
                    token_id = next(k for k, v in self.__be_token_cache.items() if v == active_tokens[0])
                    valid, token = self.verify_be_token(token_id)
                    if not valid:
                        chosen_topic, chosen_target = self.choose_be_topic()
                        token_id = self.create_be_token(chosen_topic, chosen_target)
                        token = self.__be_token_cache[token_id]
                count_dict = self._read_json(self.config.RETRIEVAL_COUNT_PATH)
                updated_count = 0
                new_lines = []
                # éå†æ‰€æœ‰åˆ†ç‰‡æ›´æ–°
                for shard_path in self.__index_shard_paths:
                    if not os.path.exists(shard_path):
                        continue
                    shard_new_lines = []
                    with open(shard_path, "r", encoding="utf-8") as f:
                        for line in f:
                            line = line.strip()
                            if not line:
                                shard_new_lines.append("")
                                continue
                            parts = line.split(" | ")
                            if len(parts) < 7:
                                shard_new_lines.append(line)
                                continue
                            mid, lvl, content, related, cat_tag, status, y_path = parts[:7]
                            related_dict = {p.split(":")[0]: float(p.split(":")[1]) for p in re.findall(r"\d+:\d+\.\d+", related)} if related != "[]" else {}
                            freq_bonus = self.config.FREQUENCY_BONUS_BASE if count_dict.get(mid, 0) >= self.config.FREQUENCY_BONUS_THRESHOLD else 0.0
                            if token and mid in token["related_memory_ids"]:
                                freq_bonus += self.config.TARGETED_BONUS
                            decay_penalty = 0.0
                            if status.startswith("expires:"):
                                try:
                                    expire_time = datetime.fromisoformat(status.split(":", 1)[1])
                                    if (expire_time - datetime.now()) < timedelta(hours=12):
                                        decay_penalty = self.config.EXPIRE_PENALTY
                                except Exception as e:
                                    logger.error(f"âš ï¸ è§£æè¿‡æœŸæ—¶é—´å¤±è´¥ï¼ˆIDï¼š{mid}ï¼‰ï¼š{e}")
                            updated_related = {rid: max(min(round(s + freq_bonus - decay_penalty, 3), 1.0), 0.5) for rid, s in related_dict.items()}
                            updated_related_str = "[" + ",".join([f"{rid}:{s}" for rid, s in updated_related.items()]) + "]" if updated_related else related
                            new_status = status if not (related_dict and any(s < self.config.MIN_STRENGTH_THRESHOLD for s in updated_related.values())) else "low-value"
                            shard_new_lines.append(f"{mid} | {lvl} | {content} | {updated_related_str} | {cat_tag} | {new_status} | {y_path}")
                            self.update_index_cache(mid, {
                                "level": lvl, "content": content, "related": updated_related_str,
                                "cat_tag": cat_tag, "status": new_status, "y_path": y_path
                            })
                            if updated_related != related_dict:
                                updated_count += 1
                    # åŸå­å†™å…¥æ›´æ–°åçš„åˆ†ç‰‡
                    self._atomic_write(shard_path, "\n".join(shard_new_lines), mode="w")
                if token_id:
                    self.archive_be_token(token_id)
                self.ac100_evaluation()
                self._write_json({}, self.config.RETRIEVAL_COUNT_PATH)
                logger.info(f"âœ… å¼ºåº¦æ›´æ–°å®Œæˆï¼š{updated_count}æ¡è®°å¿†")
        except Exception as e:
            logger.error(f"âŒ å¼ºåº¦æ›´æ–°å¤±è´¥ï¼š{e}", exc_info=True)

    def choose_be_topic(self) -> Tuple[str, float]:
        options = [("å…ƒå—æ•´åˆåº¦", 0.85), ("è·¨ä¼šè¯ç›¸å¹²æ€§", 0.80), ("è®¤çŸ¥å¢é•¿ç‡", 0.75)]
        progress = [(t, v, self.calculate_current_progress(t)) for t, v in options]
        progress.sort(key=lambda x: x[2])
        chosen_topic, chosen_target, _ = progress[0]
        logger.info(f"ğŸ¯ é€‰æ‹©ç›®æ ‡ç»´åº¦ï¼š{chosen_topic}ï¼ˆå½“å‰è¿›åº¦ï¼š{progress[0][2]}ï¼‰")
        return chosen_topic, chosen_target

    @log_performance
    def auto_clean_memory(self, clean_strategy: str = "balanced") -> str:
        strategies = {
            "balanced": {"min_strength": 0.7, "max_redundancy": 0.8},
            "aggressive": {"min_strength": 0.8, "max_redundancy": 0.7},
            "conservative": {"min_strength": 0.6, "max_redundancy": 0.9}
        }
        config = strategies.get(clean_strategy, strategies["balanced"])
        try:
            with self._cache_lock:
                candidates = [mid for mid, data in self.__index_cache.items() if data["level"] in ["å·¥ä½œ", "æƒ…æ„Ÿ"]]
                if not candidates:
                    return "æ— æ¸…ç†å€™é€‰è®°å¿†"
                deleted = []
                for mid in candidates:
                    if self._should_clean(mid, config):
                        self._backup_and_delete(mid)
                        deleted.append(mid)
                self.load_index_cache()
                logger.info(f"âœ… æ¸…ç†å®Œæˆï¼ˆ{clean_strategy}ï¼‰ï¼š{len(deleted)}æ¡")
                return f"å·²æ¸…ç†{len(deleted)}æ¡è®°å¿†ï¼š{deleted[:10]}..." if len(deleted) > 10 else f"å·²æ¸…ç†{len(deleted)}æ¡ï¼š{deleted}"
        except Exception as e:
            logger.error(f"âŒ è‡ªåŠ¨æ¸…ç†å¤±è´¥ï¼š{e}", exc_info=True)
            return f"æ¸…ç†å¤±è´¥ï¼š{e}"

    def _should_clean(self, mid: str, config: Dict[str, float]) -> bool:
        data = self.__index_cache.get(mid)
        if not data:
            return False
        if self._is_low_strength(mid, config["min_strength"]):
            return True
        if data["level"] == "å·¥ä½œ" and self._is_expired(mid):
            return True
        if self._is_redundant(mid, config["max_redundancy"]):
            return True
        return False

    def _is_low_strength(self, mid: str, min_strength: float) -> bool:
        data = self.__index_cache.get(mid)
        if not data:
            return False
        related = data.get("related", "[]")
        if related == "[]":
            return True
        related_dict = {p.split(":")[0]: float(p.split(":")[1]) for p in re.findall(r"\d+:\d+\.\d+", related)}
        return max(related_dict.values(), default=0) < min_strength

    def _is_expired(self, mid: str) -> bool:
        data = self.__index_cache.get(mid)
        if not data or not data["status"].startswith("expires:"):
            return False
        try:
            expire_time = datetime.fromisoformat(data["status"].split(":", 1)[1])
            return expire_time < datetime.now()
        except Exception as e:
            logger.error(f"âš ï¸ è§£æè¿‡æœŸæ—¶é—´å¤±è´¥ï¼ˆIDï¼š{mid}ï¼‰ï¼š{e}")
            return False

    def _is_redundant(self, mid: str, max_redundancy: float) -> bool:
        data = self.__index_cache.get(mid)
        if not data:
            return False
        current_content = self.get_full_content(mid)
        if not current_content or len(current_content) < 30:
            return False
        same_level = [
            (m, self.get_full_content(m))
            for m, d in self.__index_cache.items()
            if m != mid and d["level"] == data["level"] and len(self.get_full_content(m)) >= 30
        ]
        for other_mid, other_content in same_level:
            similarity = self._calculate_content_similarity(current_content, other_content)
            if similarity >= max_redundancy:
                logger.debug(f"â„¹ï¸ è®°å¿†{mid}ä¸{other_mid}å†—ä½™ï¼ˆç›¸ä¼¼åº¦ï¼š{similarity}ï¼‰")
                return True
        return False

    def _backup_and_delete(self, mid: str):
        src = os.path.join(self.config.FULL_CONTENT_DIR, f"{mid}.txt")
        dst = os.path.join(self.config.FULL_CONTENT_DIR, f"deleted_{mid}_{datetime.now().strftime('%Y%m%d%H%M%S')}.txt")
        if os.path.exists(src):
            shutil.copy2(src, dst)
        data = self.__index_cache.get(mid)
        if data and os.path.exists(data["y_path"]):
            os.remove(data["y_path"])
        reverse_index = self._read_json(self.config.REVERSE_INDEX_PATH)
        for rid in reverse_index:
            if mid in reverse_index[rid]:
                reverse_index[rid].remove(mid)
        reverse_index = {k: v for k, v in reverse_index.items() if v}
        self._write_json(reverse_index, self.config.REVERSE_INDEX_PATH)
        # ä»åˆ†ç‰‡æ–‡ä»¶ä¸­åˆ é™¤
        shard_path = self._get_index_shard(mid)
        if os.path.exists(shard_path):
            with open(shard_path, "r", encoding="utf-8") as f:
                lines = [l.strip() for l in f if not l.strip().startswith(f"{mid} | ")]
            self._atomic_write(shard_path, "\n".join(lines), mode="w")
        # ä»ç¼“å­˜ä¸­åˆ é™¤
        if mid in self.__index_cache:
            del self.__index_cache[mid]

    def get_full_content(self, mid: str) -> str:
        full_path = os.path.join(self.config.FULL_CONTENT_DIR, f"{mid}.txt")
        if os.path.exists(full_path):
            with open(full_path, "r", encoding="utf-8") as f:
                content = f.read()
                return content.split("å®Œæ•´å†…å®¹ï¼š")[-1].strip() if "å®Œæ•´å†…å®¹ï¼š" in content else content
        return ""

    # ğŸ”¥ æ–°å¢ï¼šè®°å¿†å¯¼å‡º/å¯¼å…¥ï¼ˆä¸ºåˆ†å¸ƒå¼æ‰“åŸºç¡€ï¼‰
    def export_memory(self, memory_ids: List[str], export_path: str) -> bool:
        export_data = []
        for mid in memory_ids:
            mem = self.get_full_content(mid)
            if mem:
                # è¡¥å……è®°å¿†IDå’Œå±‚çº§
                export_data.append({
                    "è®°å¿†ID": mid,
                    "å±‚çº§": self.get_memory_level(mid),
                    "å®Œæ•´å†…å®¹": mem,
                    "å…³è”è®°å¿†": self.__index_cache.get(mid, {}).get("related", "[]")
                })
        if not export_data:
            return False
        self._write_json(export_data, export_path)
        logger.info(f"âœ… å¯¼å‡º{len(export_data)}æ¡è®°å¿†åˆ°ï¼š{export_path}")
        return True

    def import_memory(self, import_path: str) -> int:
        import_data = self._read_json(import_path)
        if not import_data:
            return 0
        added_count = 0
        for mem in import_data:
            if "å±‚çº§" in mem and "å®Œæ•´å†…å®¹" in mem and "è®°å¿†ID" in mem:
                # è§£æå…³è”ID
                related_str = mem.get("å…³è”è®°å¿†", "[]")
                related_ids = re.findall(r"\d+", related_str) if related_str != "[]" else []
                # é¿å…IDå†²çªï¼Œé‡æ–°ç”ŸæˆID
                new_id = self.add_memory(mem["å±‚çº§"], mem["å®Œæ•´å†…å®¹"], related_ids)
                if new_id:
                    added_count += 1
        logger.info(f"âœ… ä»{import_path}å¯¼å…¥{added_count}æ¡è®°å¿†")
        return added_count

    # ğŸ”¥ æ–°å¢ï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªå¯èƒ½éœ€è¦çš„è®°å¿†
    def predict_next_memory(self, current_memory_id: str) -> Optional[Dict]:
        current_mem = self.get_full_content(current_memory_id)
        if not current_mem:
            return None
        current_level = self.get_memory_level(current_memory_id)
        if not current_level:
            return None
        
        # æŸ¥æ‰¾å…³è”è®°å¿†
        related_mems = self.advanced_search(filters={"related_to": current_memory_id})
        if related_mems:
            top_related = max(related_mems, key=lambda x: x["æœ€å¤§å…³è”å¼ºåº¦"])
            recommend_level = top_related["å±‚çº§"]
        else:
            # æ— å…³è”æ—¶æŒ‰å±‚çº§æ¨è
            recommend_level = "å…ƒè®¤çŸ¥" if current_level == "æ ¸å¿ƒ" else "æ ¸å¿ƒ"
        
        # ç­›é€‰æ¨èå€™é€‰
        candidates = self.advanced_search(
            filters={
                "levels": [recommend_level],
                "min_strength": 0.8,
                "exclude_ids": [current_memory_id]
            }
        )
        return max(candidates, key=lambda x: x["æœ€å¤§å…³è”å¼ºåº¦"]) if candidates else None

    # ğŸ”¥ è¡¥å……çš„record_retrievalæ–¹æ³•
    def record_retrieval(self, memory_id: str):
        """è®°å½•è®°å¿†æ£€ç´¢æ¬¡æ•°ï¼ˆä¾›æ„è¯†æ¶Œç°æ¨¡å—è°ƒç”¨ï¼‰"""
        with self._cache_lock:
            count_dict = self._read_json(self.config.RETRIEVAL_COUNT_PATH)
            count_dict[memory_id] = count_dict.get(memory_id, 0) + 1
            self._write_json(count_dict, self.config.RETRIEVAL_COUNT_PATH)
            logger.debug(f"ğŸ“ è®°å½•æ£€ç´¢ï¼šè®°å¿†ID={memory_id}ï¼Œç´¯è®¡æ¬¡æ•°={count_dict[memory_id]}")

    @log_performance
    def create_backup(self, compress: bool = True) -> str:
        with self._cache_lock:
            backup_time = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_dir = os.path.join(self.config.BACKUP_DIR, f"backup_{backup_time}")
            os.makedirs(backup_dir, exist_ok=True)
            try:
                # å¤‡ä»½æ‰€æœ‰åˆ†ç‰‡
                for shard_path in self.__index_shard_paths:
                    if os.path.exists(shard_path):
                        shutil.copy2(shard_path, os.path.join(backup_dir, os.path.basename(shard_path)))
                # å¤‡ä»½å…¶ä»–æ ¸å¿ƒæ–‡ä»¶
                core_files = [
                    self.config.REVERSE_INDEX_PATH,
                    self.config.RETRIEVAL_COUNT_PATH,
                    self.config.BE_TOKEN_PATH,
                    self.config.USER_CONFIG_PATH,
                    "memex_config.json"
                ]
                for file in core_files:
                    if os.path.exists(file):
                        shutil.copy2(file, os.path.join(backup_dir, os.path.basename(file)))
                # å¤‡ä»½å®Œæ•´å†…å®¹å’ŒY_OCRåº“
                shutil.copytree(self.config.FULL_CONTENT_DIR, os.path.join(backup_dir, "å®Œæ•´è®°å¿†å†…å®¹"), dirs_exist_ok=True)
                shutil.copytree(os.path.join(".", "Y_OCRåº“"), os.path.join(backup_dir, "Y_OCRåº“"), dirs_exist_ok=True)
                backup_path = backup_dir
                if compress:
                    backup_path = f"{backup_dir}.zip"
                    shutil.make_archive(backup_dir, "zip", backup_dir)
                    shutil.rmtree(backup_dir)
                logger.info(f"âœ… å¤‡ä»½å®Œæˆï¼š{backup_path}")
                return backup_path
            except Exception as e:
                logger.error(f"âŒ å¤‡ä»½å¤±è´¥ï¼š{e}", exc_info=True)
                return ""

    # å¤–éƒ¨è°ƒç”¨æ–¹æ³•
    def add_memory(self, level: str, content: str, related_ids: List[str] = None) -> str:
        if related_ids is None:
            related_ids = []
        memories = [(level, content, related_ids)]
        result_ids = self.batch_add_memories(memories)
        return result_ids[0] if result_ids else None

    def search_memory(self, query: str = "", level: str = None) -> List[Dict[str, Any]]:
        filters = {}
        if level:
            filters["levels"] = [level]
        return self.advanced_search(query, filters)

# ===================== åˆ†é˜¶æ®µéƒ¨ç½²æµ‹è¯• =====================
def phase1_deployment():
    print("ğŸ”¥ é˜¶æ®µ1éƒ¨ç½²ï¼šåŸºç¡€åŠŸèƒ½éªŒè¯")
    print("="*50)
    try:
        config = Config.from_json()
        memex = MemexA(config=config)
        print(f"âœ… ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
        test_cases = [
            ("æ ¸å¿ƒ", "ç³»ç»ŸåŸºç¡€åŠŸèƒ½éªŒè¯ï¼šæ ¸å¿ƒè®°å¿†æ·»åŠ ", []),
            ("å·¥ä½œ", "æ—¥å¸¸ä»»åŠ¡ï¼šé˜¶æ®µ1éƒ¨ç½²æµ‹è¯•", ["1"]),
            ("å…ƒè®¤çŸ¥", "å­¦ä¹ ç­–ç•¥ï¼šåˆ†é˜¶æ®µéƒ¨ç½²éªŒè¯", ["1"]),
            ("æƒ…æ„Ÿ", "æƒ…ç»ªè®°å½•ï¼šéƒ¨ç½²æˆåŠŸåçš„æ„‰æ‚¦æ„Ÿ", ["2"])
        ]
        batch_ids = memex.batch_add_memories(test_cases)
        print(f"âœ… æ‰¹é‡æ·»åŠ è®°å¿†æˆåŠŸï¼ŒIDsï¼š{batch_ids}")
        search_results = memex.search_memory(query="éƒ¨ç½²æµ‹è¯•", level="å·¥ä½œ")
        print(f"âœ… æœç´¢åˆ°{len(search_results)}æ¡åŒ¹é…è®°å¿†")
        ac_score = memex.ac100_evaluation()
        print(f"âœ… AC-100è¯„ä¼°å¾—åˆ†ï¼š{ac_score}åˆ†")
        backup_path = memex.create_backup(compress=True)
        print(f"âœ… ç³»ç»Ÿå¤‡ä»½å®Œæˆï¼š{backup_path}")
        print(f"\nğŸ‰ é˜¶æ®µ1éƒ¨ç½²æµ‹è¯•é€šè¿‡ï¼")
        return True
    except Exception as e:
        print(f"âŒ é˜¶æ®µ1éƒ¨ç½²å¤±è´¥ï¼š{e}")
        return False

def phase2_deployment():
    print("\n" + "="*50)
    print("ğŸ”¥ é˜¶æ®µ2éƒ¨ç½²ï¼šé«˜çº§åŠŸèƒ½éªŒè¯")
    print("="*50)
    try:
        memex = MemexA(Config.from_json())
        token_id = memex.create_be_token("å…ƒå—æ•´åˆåº¦", 0.85)
        print(f"âœ… åˆ›å»ºBE TokenæˆåŠŸï¼š{token_id}")
        valid, token = memex.verify_be_token(token_id)
        print(f"âœ… BE TokenéªŒè¯ç»“æœï¼š{'æœ‰æ•ˆ' if valid else 'æ— æ•ˆ'}")
        memex.update_strength()
        print(f"âœ… å…³è”å¼ºåº¦æ›´æ–°å®Œæˆ")
        clean_result = memex.auto_clean_memory(clean_strategy="balanced")
        print(f"âœ… è‡ªåŠ¨æ¸…ç†ç»“æœï¼š{clean_result}")
        # æµ‹è¯•é¢„æµ‹åŠŸèƒ½
        if memex.search_memory(level="æ ¸å¿ƒ"):
            core_id = memex.search_memory(level="æ ¸å¿ƒ")[0]["è®°å¿†ID"]
            predicted = memex.predict_next_memory(core_id)
            if predicted:
                print(f"âœ… è®°å¿†é¢„æµ‹æˆåŠŸï¼šæ¨èè®°å¿†ID={predicted['è®°å¿†ID']}ï¼ˆ{predicted['å±‚çº§']}ï¼‰")
        # æµ‹è¯•å¯¼å‡ºå¯¼å…¥
        export_path = "test_export.json"
        memex.export_memory(["1", "2"], export_path)
        import_count = memex.import_memory(export_path)
        print(f"âœ… è®°å¿†å¯¼å‡ºå¯¼å…¥æµ‹è¯•ï¼šå¯¼å…¥{import_count}æ¡")
        ac_score = memex.ac100_evaluation()
        print(f"âœ… æ¸…ç†åAC-100å¾—åˆ†ï¼š{ac_score}åˆ†")
        print(f"\nğŸ‰ é˜¶æ®µ2éƒ¨ç½²æµ‹è¯•é€šè¿‡ï¼")
        return True
    except Exception as e:
        print(f"âŒ é˜¶æ®µ2éƒ¨ç½²å¤±è´¥ï¼š{e}")
        return False

if __name__ == "__main__":
    print("ğŸ”¥ Memex-A å®Œæ•´ç»ˆç‰ˆå¯åŠ¨")
    print("="*60)
    phase1_ok = phase1_deployment()
    if phase1_ok:
        phase2_ok = phase2_deployment()
        if phase2_ok:
            print("\n" + "="*60)
            print("ğŸ‰ æ‰€æœ‰éƒ¨ç½²é˜¶æ®µé€šè¿‡ï¼ç³»ç»Ÿå®Œå…¨å°±ç»ª")
            print("="*60)