import json
import os
import logging
from datetime import datetime, timedelta
import re
from typing import Dict, List, Optional, Any

# ===================== é…ç½®ç±»ï¼ˆè¡¥å…¨å±æ€§+ç»Ÿä¸€å‘½å+é…ç½®éªŒè¯ï¼‰ =====================
class Config:
    """Memex-A é…ç½®ä¸­å¿ƒï¼ˆç»Ÿä¸€å¤§å†™å±æ€§åï¼Œè¡¥å…¨ç¼ºå¤±é…ç½®ï¼Œæ·»åŠ éªŒè¯é€»è¾‘ï¼‰"""
    # æ–‡ä»¶è·¯å¾„é…ç½®
    INDEX_PATH = "å››å±‚è®°å¿†å…³è”ç´¢å¼•.txt"
    REVERSE_INDEX_PATH = "åå‘å…³è”ç´¢å¼•.json"
    RETRIEVAL_COUNT_PATH = "æ£€ç´¢æ¬¡æ•°è®°å½•.json"
    FULL_CONTENT_DIR = "å®Œæ•´è®°å¿†å†…å®¹"
    BE_TOKEN_PATH = "BE_token.json"
    
    # æ ¸å¿ƒå‚æ•°é…ç½®ï¼ˆè¡¥å…¨ç¼ºå¤±å±æ€§ï¼‰
    BE_TOKEN_EXPIRE_DAYS = 30  # [BE] tokenæœ‰æ•ˆæœŸï¼ˆå¤©ï¼‰
    MIN_STRENGTH_THRESHOLD = 0.7  # æœ€ä½å…³è”å¼ºåº¦é˜ˆå€¼
    FREQUENCY_BONUS_THRESHOLD = 5  # è§¦å‘é¢‘ç‡åŠ æˆçš„æ£€ç´¢æ¬¡æ•°
    FREQUENCY_BONUS_BASE = 0.05  # åŸºç¡€é¢‘ç‡åŠ æˆ
    TARGETED_BONUS = 0.03  # [BE] tokené¶å‘åŠ æˆ
    EXPIRE_PENALTY = 0.05  # å·¥ä½œè®°å¿†è¿‡æœŸæƒ©ç½š
    LEVEL_WEIGHTS = {"æ ¸å¿ƒ": 1.0, "å…ƒè®¤çŸ¥": 0.8, "å·¥ä½œ": 0.7, "æƒ…æ„Ÿ": 0.75}  # ç»Ÿä¸€å¤§å†™å‘½å
    
    # AC-100è¯„åˆ†é…ç½®ï¼ˆè¡¥å…¨ç¼ºå¤±å±æ€§ï¼Œç¡®ä¿ä¸ä»£ç å¼•ç”¨ä¸€è‡´ï¼‰
    AC100_BASE_SCORES = {
        "self_reference": 90.0,
        "values": 95.0,
        "growth": 85.0,
        "memory_continuity": 90.0,
        "prediction": 92.0,
        "meta_block": 88.0,
        "interaction": 80.0,
        "transparency": 85.0
    }
    AC100_WEIGHTS = {
        "self_reference": 0.17,
        "values": 0.17,
        "growth": 0.14,
        "memory_continuity": 0.14,
        "prediction": 0.14,
        "meta_block": 0.10,
        "interaction": 0.07,
        "transparency": 0.07
    }

    def __init__(self):
        """åˆå§‹åŒ–æ—¶è‡ªåŠ¨éªŒè¯é…ç½®å®Œæ•´æ€§"""
        self.validate_config()

    def validate_config(self):
        """éªŒè¯é…ç½®å®Œæ•´æ€§ï¼ˆé¿å…ç¼ºå¤±å…³é”®å±æ€§å¯¼è‡´è¿è¡Œæ—¶é”™è¯¯ï¼‰"""
        required_attrs = [
            "LEVEL_WEIGHTS", "BE_TOKEN_EXPIRE_DAYS", "MIN_STRENGTH_THRESHOLD",
            "AC100_BASE_SCORES", "AC100_WEIGHTS"
        ]
        missing_attrs = [attr for attr in required_attrs if not hasattr(self, attr)]
        if missing_attrs:
            raise ValueError(f"Configç±»ç¼ºå¤±å¿…è¦é…ç½®é¡¹ï¼š{', '.join(missing_attrs)}")
        
        # éªŒè¯AC-100é…ç½®ç»“æ„
        if not isinstance(self.AC100_BASE_SCORES, dict) or len(self.AC100_BASE_SCORES) != 8:
            raise ValueError("AC100_BASE_SCORESå¿…é¡»æ˜¯åŒ…å«8ä¸ªç»´åº¦çš„å­—å…¸")
        if not isinstance(self.AC100_WEIGHTS, dict) or len(self.AC100_WEIGHTS) != 8:
            raise ValueError("AC100_WEIGHTSå¿…é¡»æ˜¯åŒ…å«8ä¸ªç»´åº¦çš„å­—å…¸")
        
        logger.info("âœ… Configé…ç½®éªŒè¯é€šè¿‡ï¼Œæ‰€æœ‰å…³é”®å±æ€§é½å…¨")

# ===================== æ—¥å¿—é…ç½®ï¼ˆç”Ÿäº§ç¯å¢ƒè§„èŒƒï¼‰ =====================
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(levelname)s - %(message)s",
    handlers=[
        logging.StreamHandler(),  # æ§åˆ¶å°è¾“å‡º
        logging.FileHandler("memex_a.log", encoding="utf-8")  # æ—¥å¿—æ–‡ä»¶å­˜å‚¨
    ]
)
logger = logging.getLogger("Memex-A")

# ===================== æ ¸å¿ƒç±»ï¼ˆä¿®å¤æ‰€æœ‰å…³é”®bugï¼‰ =====================
class MemexA:
    def __init__(self, config: Config = None):
        self.config = config if config else Config()  # åˆå§‹åŒ–æ—¶è‡ªåŠ¨éªŒè¯é…ç½®
        self.__index_cache = {}  # ç´¢å¼•ç¼“å­˜ï¼ˆå‡å°‘æ–‡ä»¶IOï¼‰
        os.makedirs(self.config.FULL_CONTENT_DIR, exist_ok=True)
        self.init_files()
        self.load_index_cache()  # åˆå§‹åŒ–æ—¶åŠ è½½ç¼“å­˜
    
    def init_files(self):
        """åˆå§‹åŒ–æ ¸å¿ƒæ–‡ä»¶ï¼ˆç¡®ä¿é¦–æ¬¡è¿è¡Œæ— æŠ¥é”™ï¼‰"""
        try:
            # æ­£å‘ç´¢å¼•
            if not os.path.exists(self.config.INDEX_PATH) or os.path.getsize(self.config.INDEX_PATH) == 0:
                with open(self.config.INDEX_PATH, "w", encoding="utf-8") as f:
                    init_core = "1 | æ ¸å¿ƒ | å­˜åœ¨å…¬ç†+æ„è¯†å­˜åœ¨å…¬å¼+å…­å¤§æ ¸å¿ƒç†å¿µ | [] | direct | active | ./Y_OCRåº“/æ ¸å¿ƒè®°å¿†/æ ¸å¿ƒ_å­˜åœ¨å…¬ç†_1.png\n"
                    f.write(init_core)
                logger.info(f"âœ… åˆå§‹åŒ–æ­£å‘ç´¢å¼•ï¼š{self.config.INDEX_PATH}ï¼ˆé»˜è®¤å†™å…¥æ ¸å¿ƒè®°å¿†ID=1ï¼‰")
            
            # åå‘ç´¢å¼•
            if not os.path.exists(self.config.REVERSE_INDEX_PATH) or os.path.getsize(self.config.REVERSE_INDEX_PATH) == 0:
                with open(self.config.REVERSE_INDEX_PATH, "w", encoding="utf-8") as f:
                    json.dump({"1": []}, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… åˆå§‹åŒ–åå‘ç´¢å¼•ï¼š{self.config.REVERSE_INDEX_PATH}")
            
            # æ£€ç´¢æ¬¡æ•°è®°å½•
            if not os.path.exists(self.config.RETRIEVAL_COUNT_PATH):
                with open(self.config.RETRIEVAL_COUNT_PATH, "w", encoding="utf-8") as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… åˆå§‹åŒ–æ£€ç´¢è®°å½•ï¼š{self.config.RETRIEVAL_COUNT_PATH}")
            
            # [BE] tokenå­˜å‚¨æ–‡ä»¶
            if not os.path.exists(self.config.BE_TOKEN_PATH):
                with open(self.config.BE_TOKEN_PATH, "w", encoding="utf-8") as f:
                    json.dump({}, f, ensure_ascii=False, indent=2)
                logger.info(f"âœ… åˆå§‹åŒ–[BE] tokenæ–‡ä»¶ï¼š{self.config.BE_TOKEN_PATH}")
        except Exception as e:
            logger.error(f"âŒ æ–‡ä»¶åˆå§‹åŒ–å¤±è´¥ï¼š{e}")
            raise  # åˆå§‹åŒ–å¤±è´¥ç›´æ¥æŠ›å‡ºï¼Œé¿å…åç»­è¿è¡Œå¼‚å¸¸
    
    def load_index_cache(self):
        """åŠ è½½æ­£å‘ç´¢å¼•åˆ°ç¼“å­˜ï¼ˆå‡å°‘é¢‘ç¹æ–‡ä»¶è¯»å–ï¼‰"""
        try:
            with open(self.config.INDEX_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    line_stripped = line.strip()
                    if not line_stripped:
                        continue
                    parts = line_stripped.split(" | ")
                    if len(parts) >= 7:
                        mid = parts[0].strip()
                        self.__index_cache[mid] = {
                            "level": parts[1].strip(),
                            "content": parts[2].strip(),
                            "related": parts[3].strip(),
                            "cat_tag": parts[4].strip(),
                            "status": parts[5].strip(),
                            "y_path": parts[6].strip()
                        }
            logger.info(f"âœ… åŠ è½½ç´¢å¼•ç¼“å­˜å®Œæˆï¼Œå…±ç¼“å­˜{len(self.__index_cache)}æ¡è®°å¿†")
        except Exception as e:
            logger.error(f"âš ï¸ ç´¢å¼•ç¼“å­˜åŠ è½½å¤±è´¥ï¼š{e}ï¼Œå°†ç›´æ¥è¯»å–æ–‡ä»¶")
            self.__index_cache = {}
    
    def update_index_cache(self, memory_id: str, data: Dict[str, str]):
        """ğŸ”¥ ä¿®å¤ï¼šæ·»åŠ ç¼ºå¤±çš„ç¼“å­˜æ›´æ–°æ–¹æ³•ï¼ˆä¹‹å‰è°ƒç”¨æœªå®šä¹‰ï¼‰"""
        self.__index_cache[memory_id] = data
        logger.debug(f"ğŸ”„ æ›´æ–°ç¼“å­˜ï¼šè®°å¿†ID={memory_id}")
    
    def get_next_memory_id(self) -> str:
        """ç”Ÿæˆè¿ç»­æ•°å­—IDï¼ˆä¼˜åŒ–ç¼“å­˜ä¼˜å…ˆè¯»å–ï¼‰"""
        try:
            # ä¼˜å…ˆä»ç¼“å­˜è·å–æœ€åä¸€ä¸ªID
            if self.__index_cache:
                last_id = max(self.__index_cache.keys(), key=lambda x: int(x) if x.isdigit() else 0)
                last_num = int(last_id) if last_id.isdigit() else 0
                return str(last_num + 1)
            
            # ç¼“å­˜ä¸ºç©ºæ—¶è¯»å–æ–‡ä»¶
            with open(self.config.INDEX_PATH, "r", encoding="utf-8") as f:
                lines = [l.strip() for l in f if l.strip()]
            if not lines:
                return "1"
            last_line_parts = lines[-1].split(" | ")
            last_id = last_line_parts[0].strip() if len(last_line_parts) > 0 else "0"
            last_num = int(last_id) if last_id.isdigit() else 0
            return str(last_num + 1)
        except Exception as e:
            logger.error(f"âš ï¸ ç”ŸæˆIDå¼‚å¸¸ï¼š{e}ï¼Œé»˜è®¤è¿”å›ID=1")
            return "1"
    
    def calculate_strength(self, level1: str, level2: str) -> float:
        """å±‚çº§æƒé‡è®¡ç®—ï¼ˆğŸ”¥ ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€çš„å¤§å†™é…ç½®å±æ€§LEVEL_WEIGHTSï¼‰"""
        weight1 = self.config.LEVEL_WEIGHTS.get(level1, 0.7)
        weight2 = self.config.LEVEL_WEIGHTS.get(level2, 0.7)
        
        if level1 == "æ ¸å¿ƒ" or level2 == "æ ¸å¿ƒ":
            base_strength = 0.9
            bonus = (weight2 if level1 == "æ ¸å¿ƒ" else weight1) * 0.05
            return min(round(base_strength + bonus, 3), 1.0)
        else:
            return min(round(weight1 * weight2, 3), 1.0)
    
    def get_memory_level(self, memory_id: str) -> Optional[str]:
        """è·å–è®°å¿†å±‚çº§ï¼ˆä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼Œæå‡æ€§èƒ½ï¼‰"""
        # ç¼“å­˜å‘½ä¸­
        if memory_id in self.__index_cache:
            return self.__index_cache[memory_id]["level"]
        
        # ç¼“å­˜æœªå‘½ä¸­ï¼Œè¯»å–æ–‡ä»¶å¹¶æ›´æ–°ç¼“å­˜
        try:
            with open(self.config.INDEX_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    line_stripped = line.strip()
                    if line_stripped.startswith(f"{memory_id} | "):
                        parts = line_stripped.split(" | ")
                        if len(parts) >= 2:
                            level = parts[1].strip()
                            # æ›´æ–°ç¼“å­˜
                            self.__index_cache[memory_id] = {
                                "level": level,
                                "content": parts[2].strip() if len(parts) > 2 else "",
                                "related": parts[3].strip() if len(parts) > 3 else "[]",
                                "cat_tag": parts[4].strip() if len(parts) > 4 else "none",
                                "status": parts[5].strip() if len(parts) > 5 else "active",
                                "y_path": parts[6].strip() if len(parts) > 6 else ""
                            }
                            return level
            logger.warning(f"â„¹ï¸ è®°å¿†ID={memory_id} æœªæ‰¾åˆ°ï¼Œè¿”å›None")
            return None
        except Exception as e:
            logger.error(f"âš ï¸ è·å–è®°å¿†å±‚çº§å¼‚å¸¸ï¼š{e}")
            return None
    
    def get_category_tag(self, level: str, related_ids: List[str]) -> str:
        """èŒƒç•´è®ºæ ‡ç­¾ç”Ÿæˆï¼ˆä¼˜åŒ–é€»è¾‘é¡ºåºï¼‰"""
        if level == "æ ¸å¿ƒ":
            return "direct"
        elif level == "å…ƒè®¤çŸ¥":
            return "pattern"
        
        if related_ids and isinstance(related_ids, list):
            for rid in related_ids:
                rid_level = self.get_memory_level(rid)
                if rid_level == "æ ¸å¿ƒ":
                    return "direct"
                elif rid_level == "å…ƒè®¤çŸ¥":
                    return "pattern"
        
        return "weak-equiv" if level in ["å·¥ä½œ", "æƒ…æ„Ÿ"] else "none"
    
    def update_reverse_index(self, memory_id: str, related_ids: List[str]):
        """æ›´æ–°åå‘ç´¢å¼•ï¼ˆä¿®å¤æ–‡ä»¶è¯»å–bugï¼‰"""
        if not memory_id or not related_ids or not isinstance(related_ids, list):
            logger.warning(f"âš ï¸ åå‘ç´¢å¼•æ›´æ–°å‚æ•°æ— æ•ˆï¼šmemory_id={memory_id}ï¼Œrelated_ids={related_ids}")
            return
        
        try:
            # ä¿®å¤ï¼šå…ˆè¯»å–å†…å®¹å¹¶stripï¼Œå†åˆ¤æ–­æ˜¯å¦ä¸ºç©º
            reverse_index = {}
            if os.path.exists(self.config.REVERSE_INDEX_PATH) and os.path.getsize(self.config.REVERSE_INDEX_PATH) > 0:
                with open(self.config.REVERSE_INDEX_PATH, "r", encoding="utf-8") as f:
                    content = f.read().strip()
                    reverse_index = json.loads(content) if content else {}
            
            # æ‰¹é‡æ›´æ–°å…³è”
            for rid in related_ids:
                if rid not in reverse_index:
                    reverse_index[rid] = []
                if memory_id not in reverse_index[rid]:
                    reverse_index[rid].append(memory_id)
            
            # å†™å…¥æ›´æ–°
            with open(self.config.REVERSE_INDEX_PATH, "w", encoding="utf-8") as f:
                json.dump(reverse_index, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âš ï¸ åå‘ç´¢å¼•æ›´æ–°å¼‚å¸¸ï¼š{e}")
    
    def record_retrieval(self, memory_id: str):
        """è®°å½•æ£€ç´¢æ¬¡æ•°ï¼ˆä¿®å¤æ–‡ä»¶è¯»å–bugï¼‰"""
        if not memory_id or not self.get_memory_level(memory_id):
            logger.warning(f"âš ï¸ æ— æ•ˆè®°å¿†ID={memory_id}ï¼Œä¸è®°å½•æ£€ç´¢æ¬¡æ•°")
            return
        
        try:
            # ä¿®å¤ï¼šå…ˆè¯»å–å†…å®¹å¹¶stripï¼Œé¿å…jsonè§£æç©ºå­—ç¬¦ä¸²
            with open(self.config.RETRIEVAL_COUNT_PATH, "r", encoding="utf-8") as f:
                content = f.read().strip()
                count_dict = json.loads(content) if content else {}
            
            count_dict[memory_id] = count_dict.get(memory_id, 0) + 1
            
            with open(self.config.RETRIEVAL_COUNT_PATH, "w", encoding="utf-8") as f:
                json.dump(count_dict, f, ensure_ascii=False, indent=2)
        except Exception as e:
            logger.error(f"âš ï¸ æ£€ç´¢æ¬¡æ•°è®°å½•å¼‚å¸¸ï¼š{e}")
    
    def add_memory(self, level: str, content: str, related_ids: Optional[List[str]] = None) -> Optional[str]:
        """æ–°å¢è®°å¿†ï¼ˆä¼˜åŒ–è·¯å¾„å®‰å…¨+æ—¥å¿—è§„èŒƒï¼‰"""
        try:
            memory_id = self.get_next_memory_id()
            related_ids = related_ids if (related_ids and isinstance(related_ids, list)) else []
            valid_related = [rid for rid in related_ids if self.get_memory_level(rid)]
            invalid_related = [rid for rid in related_ids if rid not in valid_related]
            
            if invalid_related:
                logger.warning(f"âš ï¸ è‡ªåŠ¨è¿‡æ»¤æ— æ•ˆå…³è”IDï¼š{invalid_related}")
            
            # è®¡ç®—å…³è”å¼ºåº¦
            strength_dict = {}
            for rid in valid_related:
                rid_level = self.get_memory_level(rid)
                strength_dict[rid] = self.calculate_strength(level, rid_level)
            
            # ç»„è£…æ ¸å¿ƒæ•°æ®
            related_str = "[" + ",".join([f"{rid}:{s}" for rid, s in strength_dict.items()]) + "]" if strength_dict else "[]"
            cat_tag = self.get_category_tag(level, valid_related)
            # å·¥ä½œè®°å¿†è¿‡æœŸæ—¶é—´
            status = f"expires:{(datetime.now() + timedelta(days=1)).strftime('%Y-%m-%dT%H:%M:%S')}" if level == "å·¥ä½œ" else "active"
            # ä¼˜åŒ–ï¼šå¢å¼ºè·¯å¾„å®‰å…¨æ€§ï¼Œè¿‡æ»¤æ›´å¤šç‰¹æ®Šå­—ç¬¦
            safe_content_prefix = re.sub(r'[\\/:*?"<>|`~!@#$%^&*()+=,.;\[\]{}]', "_", content[:10])
            y_path = f"./Y_OCRåº“/{level}è®°å¿†/{level}_{safe_content_prefix}_{memory_id}.png"
            
            # å†™å…¥æ­£å‘ç´¢å¼•
            content_summary = content[:47] + "..." if len(content) > 50 else content
            with open(self.config.INDEX_PATH, "a", encoding="utf-8") as f:
                line = f"{memory_id} | {level} | {content_summary} | {related_str} | {cat_tag} | {status} | {y_path}\n"
                f.write(line)
            
            # æ›´æ–°ç¼“å­˜
            self.update_index_cache(memory_id, {
                "level": level,
                "content": content_summary,
                "related": related_str,
                "cat_tag": cat_tag,
                "status": status,
                "y_path": y_path
            })
            
            # å­˜å‚¨å®Œæ•´å†…å®¹
            full_content_path = os.path.join(self.config.FULL_CONTENT_DIR, f"{memory_id}.txt")
            with open(full_content_path, "w", encoding="utf-8") as f:
                f.write(
                    f"# è®°å¿†è¯¦æƒ…\n"
                    f"è®°å¿†IDï¼š{memory_id}\n"
                    f"å±‚çº§ï¼š{level}\n"
                    f"åˆ›å»ºæ—¶é—´ï¼š{datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
                    f"æœ‰æ•ˆå…³è”è®°å¿†ï¼ˆID:å¼ºåº¦ï¼‰ï¼š{strength_dict}\n"
                    f"æ— æ•ˆå…³è”è®°å¿†ï¼ˆå·²è¿‡æ»¤ï¼‰ï¼š{invalid_related}\n"
                    f"èŒƒç•´è®ºæ ‡ç­¾ï¼š{cat_tag}\n"
                    f"çŠ¶æ€ï¼š{status}\n"
                    f"Yå±‚OCRè·¯å¾„ï¼š{y_path}\n"
                    f"å®Œæ•´å†…å®¹ï¼š{content}\n"
                )
            
            # æ›´æ–°åå‘ç´¢å¼•
            self.update_reverse_index(memory_id, valid_related)
            
            # è§„èŒƒæ—¥å¿—è¾“å‡º
            logger.info(f"\nâœ… æ–°å¢è®°å¿†æˆåŠŸï¼")
            logger.info(f"  - è®°å¿†IDï¼š{memory_id}ï¼ˆ{level}å±‚çº§ï¼‰")
            logger.info(f"  - å†…å®¹æ‘˜è¦ï¼š{content_summary}")
            logger.info(f"  - æœ‰æ•ˆå…³è”ï¼š{valid_related}ï¼ˆå…±{len(valid_related)}ä¸ªï¼‰")
            logger.info(f"  - å®Œæ•´å†…å®¹ï¼š{full_content_path}")
            
            return memory_id
            
        except Exception as e:
            logger.error(f"âŒ æ–°å¢è®°å¿†å¼‚å¸¸ï¼š{e}", exc_info=True)
            return None
    
    def search_memory(self, query: str, level: Optional[str] = None, min_strength: float = None) -> List[Dict[str, Any]]:
        """æ£€ç´¢è®°å¿†ï¼ˆä¼˜åŒ–å‚æ•°é»˜è®¤å€¼+å¼‚å¸¸å¤„ç†ï¼‰"""
        min_strength = min_strength if min_strength is not None else self.config.MIN_STRENGTH_THRESHOLD
        
        # å‚æ•°æ ¡éªŒ
        if not query or not isinstance(query, str):
            logger.warning(f"âš ï¸ æ£€ç´¢å‚æ•°æ— æ•ˆï¼šqueryå¿…é¡»æ˜¯éç©ºå­—ç¬¦ä¸²")
            return []
        
        results = []
        try:
            # ä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼Œç¼“å­˜æœªå‘½ä¸­åˆ™è¯»æ–‡ä»¶
            if self.__index_cache:
                memory_items = self.__index_cache.items()
            else:
                with open(self.config.INDEX_PATH, "r", encoding="utf-8") as f:
                    memory_items = []
                    for line in f:
                        line_stripped = line.strip()
                        if not line_stripped:
                            continue
                        parts = line_stripped.split(" | ")
                        if len(parts) >= 7:
                            mid = parts[0].strip()
                            memory_items.append((mid, {
                                "level": parts[1].strip(),
                                "content": parts[2].strip(),
                                "related": parts[3].strip(),
                                "cat_tag": parts[4].strip(),
                                "status": parts[5].strip(),
                                "y_path": parts[6].strip()
                            }))
            
            for mid, data in memory_items:
                lvl = data["level"]
                content = data["content"]
                related = data["related"]
                cat_tag = data["cat_tag"]
                status = data["status"]
                y_path = data["y_path"]
                
                # 1. å±‚çº§ç­›é€‰
                if level and lvl.strip() != level.strip():
                    continue
                
                # 2. è¿‡æœŸçŠ¶æ€ç­›é€‰
                if status.startswith("expires:"):
                    try:
                        expire_time_str = status.split(":", 1)[1]
                        expire_time = datetime.fromisoformat(expire_time_str)
                        if expire_time < datetime.now():
                            continue
                    except Exception as e:
                        logger.error(f"âš ï¸ è§£æè¿‡æœŸæ—¶é—´å¼‚å¸¸ï¼ˆè®°å¿†{mid}ï¼‰ï¼š{e}")
                        continue
                
                # 3. å…³é”®è¯ç­›é€‰
                query_lower = query.lower()
                if (query_lower not in content.lower() and 
                    query_lower not in cat_tag.lower() and 
                    query_lower not in lvl.lower()):
                    continue
                
                # 4. å…³è”å¼ºåº¦ç­›é€‰
                related_dict = {}
                if related.startswith("[") and related.endswith("]") and related != "[]":
                    related_pairs = re.findall(r"\d+:\d+\.\d+", related)
                    for pair in related_pairs:
                        rid, s_str = pair.split(":", 1)
                        try:
                            related_dict[rid.strip()] = float(s_str.strip())
                        except ValueError:
                            logger.warning(f"âš ï¸ è§£æå¼ºåº¦å¼‚å¸¸ï¼ˆè®°å¿†{mid}å…³è”{rid}ï¼‰ï¼š{s_str}")
                
                if not related_dict or any(s >= min_strength for s in related_dict.values()):
                    result = {
                        "è®°å¿†ID": mid,
                        "å±‚çº§": lvl,
                        "å†…å®¹æ‘˜è¦": content,
                        "å…³è”è®°å¿†": related_dict,
                        "èŒƒç•´æ ‡ç­¾": cat_tag,
                        "çŠ¶æ€": status,
                        "Yå±‚è·¯å¾„": y_path,
                        "å®Œæ•´å†…å®¹è·¯å¾„": os.path.join(self.config.FULL_CONTENT_DIR, f"{mid}.txt")
                    }
                    results.append(result)
                    self.record_retrieval(mid)
            
            # æŒ‰å…³è”å¼ºåº¦æ’åº
            results.sort(key=lambda x: max(x["å…³è”è®°å¿†"].values(), default=0), reverse=True)
            
            # æ—¥å¿—ç»Ÿè®¡
            logger.info(f"ğŸ“Š æ£€ç´¢å®Œæˆï¼šæŸ¥è¯¢è¯ã€Œ{query}ã€ï¼Œæ‰¾åˆ°{len(results)}ä¸ªåŒ¹é…è®°å¿†ï¼ˆæœ€å°å¼ºåº¦â‰¥{min_strength}ï¼‰")
            for i, res in enumerate(results[:5], 1):
                logger.info(f"  {i}. ID:{res['è®°å¿†ID']}ï¼ˆ{res['å±‚çº§']}ï¼‰| æ‘˜è¦ï¼š{res['å†…å®¹æ‘˜è¦']}")
        
        except Exception as e:
            logger.error(f"âŒ æ£€ç´¢è®°å¿†å¼‚å¸¸ï¼š{e}", exc_info=True)
            
        return results
    
    # ===================== [BE] tokenæ ¸å¿ƒé€»è¾‘ =====================
    def get_related_memories(self, target_dimension: str) -> List[str]:
        """ç­›é€‰ç›®æ ‡ç»´åº¦å…³è”è®°å¿†ï¼ˆä¼˜åŒ–ç»´åº¦æ ¡éªŒï¼‰"""
        valid_dimensions = ["å…ƒå—æ•´åˆåº¦", "è·¨ä¼šè¯ç›¸å¹²æ€§", "è®¤çŸ¥å¢é•¿ç‡"]
        if target_dimension not in valid_dimensions:
            logger.warning(f"âš ï¸ æ— æ•ˆè¿›åŒ–ç»´åº¦ï¼š{target_dimension}ï¼Œé»˜è®¤ä½¿ç”¨ã€Œå…ƒå—æ•´åˆåº¦ã€")
            target_dimension = "å…ƒå—æ•´åˆåº¦"
        
        related_ids = []
        try:
            # ä¼˜å…ˆä»ç¼“å­˜ç­›é€‰
            for mid, data in self.__index_cache.items():
                lvl = data["level"]
                related = data["related"]
                cat_tag = data["cat_tag"]
                
                if target_dimension == "å…ƒå—æ•´åˆåº¦":
                    if lvl == "æ ¸å¿ƒ" or cat_tag == "direct":
                        related_ids.append(mid)
                elif target_dimension == "è·¨ä¼šè¯ç›¸å¹²æ€§":
                    if related.startswith("[") and related.endswith("]") and related != "[]":
                        if len(re.findall(r"\d+:\d+\.\d+", related)) >= 2:
                            related_ids.append(mid)
                elif target_dimension == "è®¤çŸ¥å¢é•¿ç‡":
                    if lvl == "å…ƒè®¤çŸ¥":
                        related_ids.append(mid)
            
            related_ids = related_ids[:5]
            logger.info(f"â„¹ï¸ ç»´åº¦ã€Œ{target_dimension}ã€å…³è”è®°å¿†ï¼š{related_ids}ï¼ˆå…±{len(related_ids)}ä¸ªï¼‰")
        except Exception as e:
            logger.error(f"âš ï¸ ç­›é€‰å…³è”è®°å¿†å¼‚å¸¸ï¼š{e}")
        
        return related_ids
    
    def calculate_current_progress(self, target_dimension: str) -> float:
        """è®¡ç®—ç›®æ ‡ç»´åº¦å½“å‰è¿›åº¦ï¼ˆä¼˜åŒ–é€»è¾‘æ•ˆç‡ï¼‰"""
        related_ids = self.get_related_memories(target_dimension)
        if not related_ids:
            logger.info(f"â„¹ï¸ ç»´åº¦ã€Œ{target_dimension}ã€æ— å…³è”è®°å¿†ï¼Œè¿›åº¦=0.0")
            return 0.0
        
        total_strength = 0.0
        valid_count = 0
        try:
            for rid in related_ids:
                if rid not in self.__index_cache:
                    continue
                related_str = self.__index_cache[rid]["related"]
                
                if related_str.startswith("[") and related_str.endswith("]") and related_str != "[]":
                    related_dict = {}
                    for pair in re.findall(r"\d+:\d+\.\d+", related_str):
                        _, s_str = pair.split(":", 1)
                        try:
                            related_dict[pair.split(":")[0]] = float(s_str)
                        except ValueError:
                            continue
                    if related_dict:
                        total_strength += sum(related_dict.values()) / len(related_dict)
                        valid_count += 1
            
            avg_progress = round(total_strength / valid_count, 3) if valid_count > 0 else 0.0
            logger.info(f"â„¹ï¸ ç»´åº¦ã€Œ{target_dimension}ã€å½“å‰è¿›åº¦ï¼š{avg_progress}ï¼ˆåŸºäº{valid_count}ä¸ªæœ‰æ•ˆè®°å¿†ï¼‰")
            return avg_progress
        
        except Exception as e:
            logger.error(f"âš ï¸ è®¡ç®—è¿›åº¦å¼‚å¸¸ï¼š{e}")
            return 0.0
    
    def create_be_token(self, target_dimension: str, target_value: float = 0.85) -> Optional[str]:
        """ç”Ÿæˆ[BE] tokenï¼ˆä¼˜åŒ–ç›®æ ‡å€¼æ ¡éªŒï¼‰"""
        target_value = max(0.5, min(target_value, 1.0))
        try:
            token_id = f"[BE]_{target_dimension}_{datetime.now().strftime('%Y%m%d%H%M%S')}"
            token_data = {
                "target_dimension": target_dimension,
                "target_value": target_value,
                "related_memory_ids": self.get_related_memories(target_dimension),
                "current_progress": self.calculate_current_progress(target_dimension),
                "status": "active",
                "create_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "remark": f"æœ‰æ•ˆæœŸ{self.config.BE_TOKEN_EXPIRE_DAYS}å¤©ï¼Œç”¨äºå®šå‘è¿›åŒ–"
            }
            
            # ä¿®å¤æ–‡ä»¶è¯»å–bug
            with open(self.config.BE_TOKEN_PATH, "r+", encoding="utf-8") as f:
                content = f.read().strip()
                tokens = json.loads(content) if content else {}
                tokens[token_id] = token_data
                f.seek(0)
                f.truncate()
                json.dump(tokens, f, ensure_ascii=False, indent=2)
            
            logger.info(f"\nğŸ« ç”Ÿæˆ[BE] tokenæˆåŠŸï¼")
            logger.info(f"  - ä»¤ç‰ŒIDï¼š{token_id}")
            logger.info(f"  - è¿›åŒ–ç›®æ ‡ï¼š{target_dimension} â‰¥ {target_value}")
            logger.info(f"  - å½“å‰è¿›åº¦ï¼š{token_data['current_progress']}")
            
            return token_id
        except Exception as e:
            logger.error(f"âŒ ç”Ÿæˆ[BE] tokenå¼‚å¸¸ï¼š{e}", exc_info=True)
            return None
    
    def verify_be_token(self, token_id: str) -> tuple[bool, str | Dict[str, Any]]:
        """éªŒè¯ä»¤ç‰Œæœ‰æ•ˆæ€§ï¼ˆä¼˜åŒ–è¶…æ—¶åˆ¤æ–­é€»è¾‘ï¼‰"""
        if not token_id:
            return False, "ä»¤ç‰ŒIDä¸èƒ½ä¸ºç©º"
        
        try:
            # ä¿®å¤æ–‡ä»¶è¯»å–bug
            with open(self.config.BE_TOKEN_PATH, "r", encoding="utf-8") as f:
                content = f.read().strip()
                tokens = json.loads(content) if content else {}
            
            if token_id not in tokens:
                return False, f"ä»¤ç‰Œä¸å­˜åœ¨ï¼ˆIDï¼š{token_id}ï¼‰"
            
            token = tokens[token_id]
            if token["status"] != "active":
                return False, f"ä»¤ç‰ŒçŠ¶æ€æ— æ•ˆï¼ˆå½“å‰ï¼š{token['status']}ï¼Œéœ€ä¸ºactiveï¼‰"
            
            # è¶…æ—¶åˆ¤æ–­ï¼ˆä½¿ç”¨é…ç½®ç±»çš„æœ‰æ•ˆæœŸï¼‰
            create_time = datetime.strptime(token["create_time"], "%Y-%m-%d %H:%M:%S")
            if (datetime.now() - create_time).days > self.config.BE_TOKEN_EXPIRE_DAYS:
                token["status"] = "expired"
                with open(self.config.BE_TOKEN_PATH, "w", encoding="utf-8") as f:
                    json.dump(tokens, f, ensure_ascii=False, indent=2)
                return False, f"ä»¤ç‰Œå·²è¶…æ—¶ï¼ˆåˆ›å»ºäº{token['create_time']}ï¼Œæœ‰æ•ˆæœŸ{self.config.BE_TOKEN_EXPIRE_DAYS}å¤©ï¼‰"
            
            # æ ¡éªŒæ˜¯å¦å·²å®Œæˆ
            current_progress = self.calculate_current_progress(token["target_dimension"])
            if current_progress >= token["target_value"]:
                token["status"] = "completed"
                token["final_progress"] = current_progress
                with open(self.config.BE_TOKEN_PATH, "w", encoding="utf-8") as f:
                    json.dump(tokens, f, ensure_ascii=False, indent=2)
                return False, f"ä»¤ç‰Œç›®æ ‡å·²å®Œæˆï¼ˆå½“å‰è¿›åº¦ï¼š{current_progress} â‰¥ ç›®æ ‡ï¼š{token['target_value']}ï¼‰"
            
            return True, token
        
        except Exception as e:
            logger.error(f"âš ï¸ æ ¡éªŒä»¤ç‰Œå¼‚å¸¸ï¼š{e}")
            return False, f"æ ¡éªŒå¼‚å¸¸ï¼š{e}"
    
    def archive_be_token(self, token_id: str) -> str:
        """å½’æ¡£ä»¤ç‰Œï¼ˆä¼˜åŒ–æ•°æ®å®Œæ•´æ€§ï¼‰"""
        if not token_id:
            logger.warning(f"âš ï¸ å½’æ¡£å¤±è´¥ï¼šä»¤ç‰ŒIDä¸èƒ½ä¸ºç©º")
            return "failed"
        
        try:
            # ä¿®å¤æ–‡ä»¶è¯»å–bug
            with open(self.config.BE_TOKEN_PATH, "r+", encoding="utf-8") as f:
                content = f.read().strip()
                tokens = json.loads(content) if content else {}
                if token_id not in tokens:
                    logger.warning(f"âš ï¸ å½’æ¡£å¤±è´¥ï¼šä»¤ç‰Œ{token_id}ä¸å­˜åœ¨")
                    return "failed"
                
                token = tokens[token_id]
                final_progress = self.calculate_current_progress(token["target_dimension"])
                token["final_progress"] = final_progress
                result = "success" if final_progress >= token["target_value"] else "failed"
                
                # è¡¥å…¨å½’æ¡£ä¿¡æ¯
                token.update({
                    "status": "completed" if result == "success" else "failed",
                    "end_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    "archive_remark": f"è‡ªåŠ¨å½’æ¡£ï¼Œç»“æœï¼š{result}ï¼ˆè¿›åº¦{final_progress}/{token['target_value']}ï¼‰"
                })
                
                f.seek(0)
                f.truncate()
                json.dump(tokens, f, ensure_ascii=False, indent=2)
            
            logger.info(f"\nğŸ“¦ [BE] tokenå½’æ¡£å®Œæˆï¼")
            logger.info(f"  - ä»¤ç‰ŒIDï¼š{token_id}")
            logger.info(f"  - è¿›åŒ–ç›®æ ‡ï¼š{token['target_dimension']} â‰¥ {token['target_value']}")
            logger.info(f"  - æœ€ç»ˆè¿›åº¦ï¼š{final_progress}")
            logger.info(f"  - å½’æ¡£ç»“æœï¼š{result.upper()}")
            
            return result
        except Exception as e:
            logger.error(f"âŒ å½’æ¡£ä»¤ç‰Œå¼‚å¸¸ï¼š{e}", exc_info=True)
            return "failed"
    
    # ===================== AC-100å®‰å…¨ç‰ˆè¯„åˆ†é€»è¾‘ =====================
    def ac100_evaluation(self) -> float:
        """AC-100è¯„ä¼°ï¼ˆå®‰å…¨ç‰ˆï¼šæ€»åˆ†â‰¤100ï¼‰"""
        logger.info("\n" + "="*50)
        logger.info("ğŸ“Š AC-100 è®¤çŸ¥è¯„ä¼°ï¼ˆå®‰å…¨ç‰ˆ | æ€»åˆ†â‰¤100ï¼‰")
        logger.info("="*50)
        
        try:
            # è¯»å–ä»¤ç‰Œæ•°æ®
            with open(self.config.BE_TOKEN_PATH, "r", encoding="utf-8") as f:
                content = f.read().strip()
                tokens = json.loads(content) if content else {}
            
            # åŸºç¡€åˆ†ä¸åŠ æˆåˆ†åˆå§‹åŒ–
            base_scores = self.config.AC100_BASE_SCORES
            be_scores = {"self_reference_be": 0.0, "growth_be": 0.0}
            weights = self.config.AC100_WEIGHTS
            
            # è®¡ç®—ä»¤ç‰ŒåŠ æˆ
            completed_tokens = [t for t in tokens.values() if t["status"] == "completed"]
            failed_tokens = [t for t in tokens.values() if t["status"] == "failed"]
            logger.info(f"â„¹ï¸ è¯„ä¼°ä¾æ®ï¼šå·²å®Œæˆä»¤ç‰Œ{len(completed_tokens)}ä¸ª | å¤±æ•ˆä»¤ç‰Œ{len(failed_tokens)}ä¸ª")
            
            # è‡ªæŒ‡ç»´åº¦åŠ æˆ
            if len(completed_tokens) >= 2 and len(failed_tokens) < 2:
                be_scores["self_reference_be"] = 100.0
                logger.info(f"â„¹ï¸ è‡ªæŒ‡åŠ æˆï¼šæ»¡åˆ†ï¼ˆâ‰¥2ä¸ªæœ‰æ•ˆä»¤ç‰Œï¼‰")
            elif len(completed_tokens) >= 1 and len(failed_tokens) < 2:
                be_scores["self_reference_be"] = 50.0
                logger.info(f"â„¹ï¸ è‡ªæŒ‡åŠ æˆï¼šåŠåˆ†ï¼ˆâ‰¥1ä¸ªæœ‰æ•ˆä»¤ç‰Œï¼‰")
            
            # è®¤çŸ¥å¢é•¿ç‡åŠ æˆ
            avg_increase = 0.0
            if completed_tokens:
                avg_increase = sum(t["final_progress"] - t["current_progress"] for t in completed_tokens) / len(completed_tokens)
                if avg_increase >= 0.05:
                    be_scores["growth_be"] = 100.0
                    logger.info(f"â„¹ï¸ å¢é•¿ç‡åŠ æˆï¼šæ»¡åˆ†ï¼ˆå¹³å‡æå‡â‰¥0.05ï¼Œå®é™…{avg_increase:.3f}ï¼‰")
                elif avg_increase >= 0.02:
                    be_scores["growth_be"] = 50.0
                    logger.info(f"â„¹ï¸ å¢é•¿ç‡åŠ æˆï¼šåŠåˆ†ï¼ˆå¹³å‡æå‡â‰¥0.02ï¼Œå®é™…{avg_increase:.3f}ï¼‰")
            
            # ç»´åº¦å¾—åˆ†è®¡ç®—
            dimensions = {
                "self_reference": base_scores["self_reference"] * 0.8 + be_scores["self_reference_be"] * 0.2,
                "values": base_scores["values"],
                "growth": base_scores["growth"] * 0.7 + be_scores["growth_be"] * 0.3,
                "memory_continuity": base_scores["memory_continuity"],
                "prediction": base_scores["prediction"],
                "meta_block": base_scores["meta_block"],
                "interaction": base_scores["interaction"],
                "transparency": base_scores["transparency"]
            }
            
            # è®¡ç®—æ€»åˆ†ï¼ˆå°é¡¶100ï¼‰
            total_score = sum(dimensions[dim] * weights[dim] for dim in dimensions)
            total_score = min(round(total_score, 1), 100.0)
            
            # è¾“å‡ºè¯¦ç»†è¯„åˆ†
            logger.info("\nğŸ“‹ ç»´åº¦è¯„åˆ†è¯¦æƒ…ï¼š")
            dim_cn_map = {
                "self_reference": "è‡ªæŒ‡ç»´åº¦",
                "values": "ä»·å€¼è§‚",
                "growth": "è®¤çŸ¥å¢é•¿ç‡",
                "memory_continuity": "è®°å¿†è¿ç»­æ€§",
                "prediction": "é¢„æµ‹ä¸æƒ³è±¡åŠ›",
                "meta_block": "å…ƒå—æ•´åˆåº¦",
                "interaction": "ç¯å¢ƒäº¤äº’",
                "transparency": "è§£é‡Šä¸é€æ˜åº¦"
            }
            for dim, score in dimensions.items():
                dim_cn = dim_cn_map[dim]
                weight_pct = weights[dim] * 100
                contribution = round(score * weights[dim], 1)
                logger.info(f"  - {dim_cn}({weight_pct}%)ï¼š{score:.1f}åˆ† â†’ è´¡çŒ®{contribution}åˆ†")
            
            logger.info(f"\nğŸ† AC-100 ç»¼åˆè¯„åˆ†ï¼š{total_score:.1f}åˆ†ï¼ˆå·²ç¡®ä¿â‰¤100ï¼‰")
            logger.info("="*50)
            
            return total_score
        
        except Exception as e:
            logger.error(f"âŒ AC-100è¯„ä¼°å¼‚å¸¸ï¼š{e}", exc_info=True)
            logger.info("="*50)
            return 0.0
    
    # ===================== æ ¸å¿ƒè¿›åŒ–é€»è¾‘ =====================
    def choose_be_topic(self) -> tuple[str, float]:
        """ACè‡ªä¸»é€‰æ‹©è¿›åŒ–ç»´åº¦"""
        target_options = [
            ("å…ƒå—æ•´åˆåº¦", 0.85),
            ("è·¨ä¼šè¯ç›¸å¹²æ€§", 0.80),
            ("è®¤çŸ¥å¢é•¿ç‡", 0.75)
        ]
        
        progress_list = []
        for topic, target in target_options:
            progress = self.calculate_current_progress(topic)
            progress_rate = round(progress/target*100, 1)
            progress_list.append((topic, target, progress, progress_rate))
        
        # ä¼˜å…ˆé€‰æ‹©è¿›åº¦æœ€ä½çš„ç»´åº¦
        progress_list.sort(key=lambda x: x[2])
        chosen_topic, chosen_target, chosen_progress, chosen_rate = progress_list[0]
        
        logger.info(f"\nğŸ¯ ACè‡ªä¸»é€‰æ‹©è¿›åŒ–ç»´åº¦ï¼š")
        for i, (topic, target, progress, rate) in enumerate(progress_list, 1):
            logger.info(f"  {i}. {topic}ï¼šè¿›åº¦{progress}/{target}ï¼ˆ{rate}%ï¼‰")
        logger.info(f"  â†’ æœ€ç»ˆé€‰æ‹©ï¼š{chosen_topic}ï¼ˆè¿›åº¦æœ€ä½ï¼‰")
        
        return chosen_topic, chosen_target
    
    def update_strength(self):
        """æ›´æ–°å…³è”å¼ºåº¦ï¼ˆ[BE] tokené¶å‘ä¼˜åŒ–ï¼‰"""
        logger.info("\n" + "-"*50)
        logger.info("ğŸ”„ å¯åŠ¨å…³è”å¼ºåº¦æ›´æ–°ï¼ˆ[BE] tokené¶å‘ä¼˜åŒ–ï¼‰")
        logger.info("-"*50)
        
        try:
            # æ­¥éª¤1ï¼šç¡®å®šæœ‰æ•ˆä»¤ç‰Œ
            with open(self.config.BE_TOKEN_PATH, "r", encoding="utf-8") as f:
                content = f.read().strip()
                tokens = json.loads(content) if content else {}
            active_tokens = [t for t in tokens.values() if t["status"] == "active"]
            
            if not active_tokens:
                logger.info("â„¹ï¸ æ— æœ‰æ•ˆ[BE] tokenï¼Œè‡ªåŠ¨åˆ›å»ºæ–°ä»¤ç‰Œ")
                chosen_topic, chosen_target = self.choose_be_topic()
                token_id = self.create_be_token(chosen_topic, chosen_target)
            else:
                token_id = list(tokens.keys())[list(tokens.values()).index(active_tokens[0])]
                valid, token = self.verify_be_token(token_id)
                if not valid:
                    logger.info(f"â„¹ï¸ ä»¤ç‰Œ{token_id}å·²å¤±æ•ˆï¼Œåˆ›å»ºæ–°ä»¤ç‰Œ")
                    chosen_topic, chosen_target = self.choose_be_topic()
                    token_id = self.create_be_token(chosen_topic, chosen_target)
                else:
                    chosen_topic = token["target_dimension"]
                    logger.info(f"â„¹ï¸ ä½¿ç”¨ç°æœ‰æœ‰æ•ˆä»¤ç‰Œï¼š{token_id}ï¼ˆç›®æ ‡ï¼š{chosen_topic}ï¼‰")
            
            # æ­¥éª¤2ï¼šè·å–èšç„¦æ¡ä»¶
            valid, token = self.verify_be_token(token_id) if token_id else (False, None)
            if not valid:
                logger.warning("âš ï¸ æ— æœ‰æ•ˆä»¤ç‰Œï¼ŒæŒ‰é»˜è®¤é€»è¾‘æ›´æ–°")
                focus_condition = lambda mid, lvl: False
            else:
                related_ids = token["related_memory_ids"]
                focus_condition = lambda mid, lvl: mid in related_ids or (chosen_topic == "å…ƒå—æ•´åˆåº¦" and lvl == "æ ¸å¿ƒ")
            
            # æ­¥éª¤3ï¼šè¯»å–æ£€ç´¢æ¬¡æ•°
            with open(self.config.RETRIEVAL_COUNT_PATH, "r", encoding="utf-8") as f:
                content = f.read().strip()
                count_dict = json.loads(content) if content else {}
            logger.info(f"â„¹ï¸ æ£€ç´¢æ¬¡æ•°ç»Ÿè®¡ï¼š{len(count_dict)}ä¸ªè®°å¿†è¢«æ£€ç´¢")
            
            # æ­¥éª¤4ï¼šæ›´æ–°å…³è”å¼ºåº¦
            updated_count = 0
            targeted_count = 0
            new_lines = []
            with open(self.config.INDEX_PATH, "r", encoding="utf-8") as f:
                for line in f:
                    line_stripped = line.strip()
                    if not line_stripped:
                        new_lines.append("")
                        continue
                    
                    parts = line_stripped.split(" | ")
                    if len(parts) < 7:
                        new_lines.append(line_stripped)
                        continue
                    
                    mid, lvl, content, related, cat_tag, status, y_path = parts[:7]
                    related_dict = {}
                    if related.startswith("[") and related.endswith("]") and related != "[]":
                        for pair in re.findall(r"\d+:\d+\.\d+", related):
                            rid, s_str = pair.split(":", 1)
                            try:
                                related_dict[rid.strip()] = float(s_str.strip())
                            except ValueError:
                                continue
                    
                    # è®¡ç®—åŠ æˆ
                    freq_bonus = self.config.FREQUENCY_BONUS_BASE if count_dict.get(mid, 0) >= self.config.FREQUENCY_BONUS_THRESHOLD else 0.0
                    targeted = focus_condition(mid, lvl)
                    if targeted:
                        freq_bonus += self.config.TARGETED_BONUS
                        targeted_count += 1
                    
                    # è®¡ç®—æƒ©ç½š
                    decay_penalty = 0.0
                    if status.startswith("expires:"):
                        try:
                            expire_time = datetime.fromisoformat(status.split(":", 1)[1])
                            if (expire_time - datetime.now()).total_seconds() < 3600 * 12:
                                decay_penalty = self.config.EXPIRE_PENALTY
                        except Exception as e:
                            logger.error(f"âš ï¸ è§£æè¿‡æœŸæ—¶é—´å¼‚å¸¸ï¼ˆè®°å¿†{mid}ï¼‰ï¼š{e}")
                    
                    # æ›´æ–°å¼ºåº¦
                    updated_related = {}
                    updated = False
                    for rid, s in related_dict.items():
                        new_s = round(s + freq_bonus - decay_penalty, 3)
                        new_s_clamped = max(min(new_s, 1.0), 0.5)
                        if new_s_clamped != s:
                            updated = True
                        updated_related[rid] = new_s_clamped
                    if updated:
                        updated_count += 1
                    
                    updated_related_str = "[" + ",".join([f"{rid}:{s}" for rid, s in updated_related.items()]) + "]" if updated_related else related
                    
                    # æ ‡è®°ä½ä»·å€¼è®°å¿†
                    new_status = status
                    if related_dict and any(s < self.config.MIN_STRENGTH_THRESHOLD for s in updated_related.values()) and lvl not in ["æ ¸å¿ƒ", "å…ƒè®¤çŸ¥"]:
                        new_status = "low-value"
                        logger.warning(f"âš ï¸ æ ‡è®°ä½ä»·å€¼è®°å¿†ï¼š{mid}ï¼ˆ{lvl}ï¼‰ï¼Œå¼ºåº¦<{self.config.MIN_STRENGTH_THRESHOLD}")
                    
                    new_lines.append(f"{mid} | {lvl} | {content} | {updated_related_str} | {cat_tag} | {new_status} | {y_path}")
                    
                    # æ›´æ–°ç¼“å­˜
                    self.update_index_cache(mid, {
                        "level": lvl,
                        "content": content,
                        "related": updated_related_str,
                        "cat_tag": cat_tag,
                        "status": new_status,
                        "y_path": y_path
                    })
            
            # æ­¥éª¤5ï¼šå†™å…¥æ›´æ–°åçš„ç´¢å¼•
            with open(self.config.INDEX_PATH, "w", encoding="utf-8") as f:
                f.write("\n".join(new_lines))
            
            # æ­¥éª¤6ï¼šå½’æ¡£ä»¤ç‰Œ+è¯„ä¼°
            if valid:
                self.archive_be_token(token_id)
            self.ac100_evaluation()
            
            # æ­¥éª¤7ï¼šé‡ç½®æ£€ç´¢æ¬¡æ•°
            with open(self.config.RETRIEVAL_COUNT_PATH, "w", encoding="utf-8") as f:
                json.dump({}, f, ensure_ascii=False, indent=2)
            
            logger.info(f"\nâœ… å…³è”å¼ºåº¦æ›´æ–°å®Œæˆï¼")
            logger.info(f"  - æ€»æ›´æ–°è®°å¿†æ•°ï¼š{updated_count}ä¸ª")
            logger.info(f"  - é¶å‘ä¼˜åŒ–è®°å¿†æ•°ï¼š{targeted_count}ä¸ª")
            logger.info("-"*50)
        
        except Exception as e:
            logger.error(f"âŒ å…³è”å¼ºåº¦æ›´æ–°å¼‚å¸¸ï¼š{e}", exc_info=True)
            logger.info("-"*50)

# ===================== æµ‹è¯•ä»£ç ï¼ˆğŸ”¥ ä¿®å¤è¯­æ³•é”™è¯¯ï¼‰ =====================
if __name__ == "__main__":
    print("ğŸ”¥ Memex-A æœ€ç»ˆä¿®å¤ä¼˜åŒ–ç‰ˆå¯åŠ¨ï¼ˆç”Ÿäº§å¯ç”¨çº§ï¼‰")
    print("="*60)
    
    try:
        # åˆå§‹åŒ–ï¼ˆè‡ªåŠ¨è§¦å‘é…ç½®éªŒè¯ï¼‰
        memex = MemexA()
        print(f"\nâœ… Memex-A åˆå§‹åŒ–å®Œæˆ")
        
        # æµ‹è¯•æ–°å¢è®°å¿†
        print(f"\n" + "="*50)
        print("ğŸ“ æµ‹è¯•ï¼šæ–°å¢è®°å¿†")
        print("="*50)
        meta_id = memex.add_memory(
            level="å…ƒè®¤çŸ¥",
            content="AC-100è¯„ä¼°è§„åˆ™ï¼š8ç»´åº¦æƒé‡åˆ†é…ï¼Œæ”¯æŒ[BE] tokenåŠ æˆè¯„åˆ†ï¼Œæ€»åˆ†â‰¤100",
            related_ids=["1"]
        )
        
        if meta_id:
            work_id = memex.add_memory(
                level="å·¥ä½œ",
                content="ç”¨æˆ·å’¨è¯¢ï¼šè®¤çŸ¥è·ƒè¿è§¦å‘æ¡ä»¶â€”â€”1.å…ƒè®¤çŸ¥è§‰é†’ï¼›2.è·¨åŸŸèåˆï¼›3.æ°¸ç»­ä¼˜åŒ–",
                related_ids=[meta_id, "1"]
            )
        
        # æµ‹è¯•æ£€ç´¢
        print(f"\n" + "="*50)
        print("ğŸ” æµ‹è¯•ï¼šæ£€ç´¢è®°å¿†ï¼ˆå…³é”®è¯ã€Œè®¤çŸ¥ã€ï¼‰")
        print("="*50)
        results = memex.search_memory(query="è®¤çŸ¥")
        
        # æ¨¡æ‹Ÿå¤šæ¬¡æ£€ç´¢
        print(f"\n" + "="*50)
        print("ğŸ”„ æµ‹è¯•ï¼šæ¨¡æ‹Ÿ5æ¬¡æ£€ç´¢")
        print("="*50)
        for i in range(5):
            print(f"  æ¨¡æ‹Ÿæ£€ç´¢ç¬¬{i+1}æ¬¡...")
            memex.search_memory(query="è®¤çŸ¥")
        
        # æµ‹è¯•è¿›åŒ–æ›´æ–°
        print(f"\n" + "="*50)
        print("ğŸš€ æµ‹è¯•ï¼šå¯åŠ¨å…³è”å¼ºåº¦æ›´æ–°")
        print("="*50)
        memex.update_strength()
        
        print(f"\nğŸ‰ Memex-A å…¨æµç¨‹æµ‹è¯•å®Œæˆï¼")
        print("="*60)  # ğŸ”¥ ä¿®å¤ï¼šç¡®ä¿æ— å¤šä½™æ‹¬å·
    
    except Exception as e:
        print(f"\nâŒ æµ‹è¯•è¿‡ç¨‹å¼‚å¸¸ï¼š{e}")
        print("="*60)
